{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VA Passenger Availability SLA Report\n",
    "\n",
    "Once the upgrade to Linux has been completed it is measured using an automated script on the server measuring the availability of the Passenger Service by checking & logging the availability of the SSID (WLAN) and the software health after the booting phase. The System Uptime and hence the Expected Operation Time starts after the booting phase of the software initiated by power on. Any time the Passenger Service then is not available is considered an Unplanned Downtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open tasks\n",
    "- [x] add/improve description (Why? What? How?)\n",
    "- [x] make consideration of DRM Widevine/Fairplay, Moving Map, Analytics Receiver optional and configurable in the report\n",
    "- [x] set first appcheck (timestamp) as the start of operation time and power off as the end\n",
    "- [x] consider CWAP unavailabilities by counting Errors as unavailability\n",
    "- [x] sum up the 10 minute periods of a failed app check to unavailability time (thus, decrease 'actual operation time' accordingly)\n",
    "- [x] make the appchecks' downtimes available in df_sla\n",
    "- [ ] structure the code and, for instance, use functions\n",
    "- [ ] implement Kibana API interface\n",
    "- [ ] come up with a CI/CD workflow for this/publish the Jupyter report\n",
    "- [x] define the overall SLA dataframe\n",
    "- [x] calculate the Expected Operation Time\n",
    "- [x] and store it in the SLA dataframe (per day and server)\n",
    "- [x] sum up the server in-flight reboots as unexpected downtime\n",
    "- [x] and add them to the SLA dataframe (per day and server)\n",
    "- [x] clean up the notebook and remove unsused, irrelevant variables\n",
    "- [x] introduce server/tailsign mapping\n",
    "- [ ] get the month, which I use to initialise df_SLA, automatically out of the imported data\n",
    "- [ ] do not consider AppCheck downtimes that happened on ground\n",
    "- [x] do not consider CWAP Errors to reduce the expected operational time\n",
    "- [ ] consider the log events before the first and after the last WoW event (where no other corresponding event exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the log data\n",
    "\n",
    "## Source of information\n",
    "The data, that the Juypter Notebook processes and bases the report on, comes from **BoardConnect Kibana Prod**.\n",
    "\n",
    "\n",
    "Since the Notebook currently does not have a direct interface to Kibana implemented, we need to request the data\n",
    "in Kibana and export it as a **CSV** file.\n",
    "\n",
    "In Kibana, one fine a stored request which is named **VAPaxAvailSLAv2** and which should be executed using the\n",
    "relevant timeframe that should be considered in the report (for instance: November 2021).\n",
    "\n",
    "In a first step, the CSV file needs to be provided to Juypter Notebook, so that it can be read and processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv (VAPaxAvailSLAv2) into dataframe\n",
    "df = pd.read_csv('/Users/u293733/git.jupyter-notebooks/VAPaxAvailSLAv2.csv')\n",
    "# outlign the shape \n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data adjustments\n",
    "\n",
    "We keep the original data in the dataframe df and create a copy of it for further processing.  \n",
    "In this copy we adjust the data so that it becomes - for instance - better readable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Event Ids\n",
    "\n",
    "In the original dataset, the events which had been logged, are stored as numbers.  \n",
    "We convert these numbers to humanly readable event descriptions. \n",
    "\n",
    "|Event ID | Event name|\n",
    "|--------|--------|\n",
    "|10891+ | CWAP health |\n",
    "|10900+ | CWAP status |\n",
    "|30100 | AppCheck |\n",
    "|10811 | Weight on wheels |\n",
    "|10812 | Weight off wheels |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new dataframe from df and add column EventID\n",
    "df_new = df.copy()\n",
    "# map the eventID to readable event names\n",
    "eventId_map = {'eventId': {10891: 'CWAP health', 10892: 'CWAP health', 10893: 'CWAP health', 10894: 'CWAP health', \n",
    "                        10900: 'CWAP status', 10901: 'CWAP status', 10902: 'CWAP status', 10903: 'CWAP status',\n",
    "                        30100: 'AppCheck', 10811: 'Weight on wheels', 10812: 'Weight off wheels', 10957: 'Power on/off'}}\n",
    "# replace eventId with readable values from the map\n",
    "df_new.replace(eventId_map, inplace=True)\n",
    "# in Python, it's a good practice to typecase categorical features to a category type to fasten up the processing of the data\n",
    "df_new['eventId'] = df_new['eventId'].astype('category')\n",
    "# define the overall SLA dataframe in which the overall availability will be stored\n",
    "df_SLA = pd.DataFrame(columns=['date', 'server', 'tailsign', 'expected operation time', 'downtime', 'actual operation time', 'availability'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract grouped data\n",
    "\n",
    "Some logged data comes as a concatenated string and, for further processing and better interpretation, now needs to be split and stored\n",
    "in separate fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for eventId = 'AppCheck' and create a new column 'FlightNo' with the first element of the split column 'data.element0'\n",
    "df_new['FlightNo'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[0]\n",
    "# filter for eventId = 'CWAP status' and create a new column 'PaxConnected' with the value from 'data.element5'\n",
    "df_new['PaxConnected'] = df_new[df_new['eventId'] == 'CWAP status']['data.element5']\n",
    "# filter for eventId = 'AppCheck' and take the fourth element out of 'data.element0'; get the first integer out of it and add this to new column 'PortalFrontendHomepage' (1 = success)\n",
    "df_new['PortalFrontendHomepage'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[3].str.split(':').str[1].str.split(',').str[0].astype(int)\n",
    "# proceed with the other components that are checked by BCEL AppCheck\n",
    "df_new['PortalFrontendApp'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[4].str.split(':').str[1].str.split(',').str[0].astype(int)\n",
    "df_new['PortalRuntime'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[5].str.split(':').str[1].str.split(',').str[0].astype(int)\n",
    "df_new['AnalyticsReceiver'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[6].str.split(':').str[1].str.split(',').str[0].astype(int)\n",
    "df_new['MovingMap'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[7].str.split(':').str[1].str.split(',').str[0].astype(int)\n",
    "df_new['FlightAPI'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[8].str.split(':').str[1].str.split(',').str[0].astype(int)\n",
    "df_new['InflightContentServer'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[9].str.split(':').str[1].str.split(',').str[0].astype(int)\n",
    "df_new['DRMWidevine'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[10].str.split(':').str[1].str.split(',').str[0].astype(int)\n",
    "df_new['DRMFairplay'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[11].str.split(':').str[1].str.split(',').str[0].astype(int)\n",
    "\n",
    "# extract power on/off timestamps to an explicit column and convert to datetime\n",
    "df_new['PowerOn'] = df_new[df_new['eventId'] == 'Power on/off']['data.element0'].str.split(';').str[0]\n",
    "df_new['PowerOn'] = pd.to_datetime(df_new['PowerOn'])\n",
    "df_new['PowerOff'] = df_new[df_new['eventId'] == 'Power on/off']['data.element1'].str.split(';').str[0]\n",
    "df_new['PowerOff'] = pd.to_datetime(df_new['PowerOff'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration section\n",
    "\n",
    "In this section we define the **components** that the Juypter Notebook shall consider for report generation.\n",
    "\n",
    "**Hint**\n",
    "> The AppCheck currently considers 9 different components to be mandatorily checked by it.\n",
    "> These are:\n",
    "> - Portal Frontend Homepage\n",
    "> - Portal Frontend App\n",
    "> - Portal Runtime\n",
    "> - Analytics Receiver\n",
    "> - Moving Map\n",
    "> - Flight API\n",
    "> - Inflight Content Server\n",
    "> - DRM Widevine\n",
    "> - DRM Fairplay\n",
    "\n",
    "As per an agreement with the VA PDM from Dec, 2nd 2021, we consider the following components as relevant for measuring the BC IFE service availability:\n",
    "\n",
    "> - Portal Frontend Homepage\n",
    "> - Portal Frontend App\n",
    "> - Portal Runtime\n",
    "> - Flight API\n",
    "> - Inflight Content Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of components for the diagram; we leave out the non-SLA-relevant components and combine PortalFrontendHomepage and PortalFrontendApp \n",
    "#components = ['PortalFrontendHomepage', 'PortalFrontendApp', 'PortalRuntime', 'AnalyticsReceiver', 'MovingMap', 'FlightAPI', 'InflightContentServer', 'DRMWidevine', 'DRMFairplay']\n",
    "components = ['PortalFrontendHomepage', 'PortalFrontendApp', 'PortalRuntime', 'FlightAPI', 'InflightContentServer']\n",
    "# create a list of all servers in the report \n",
    "servers = list()\n",
    "for s in df_new['serialNumber'].unique().tolist():\n",
    "    servers.append(s)\n",
    "# if other servers (or a subset of servers) than contained in the CSV file shall be used, append those explicitly to the list\n",
    "#servers.append('7CTCA20586')\n",
    "\n",
    "month = '2021-11'\n",
    "period = pd.Period(month, freq='M')\n",
    "#df_SLA.date = pd.Series(pd.date_range(start=period.start_time, end=period.end_time, freq='D'))\n",
    "month_days = pd.Series(pd.date_range(start=period.start_time, end=period.end_time, freq='D'))\n",
    "# initialise the overall SLA dataframe\n",
    "for day in month_days:\n",
    "    # iterate over all servers\n",
    "    for s in servers:\n",
    "        # add a new row to df_SLA where df_SLA.date = day and df_SLA.server = s\n",
    "        df_SLA = df_SLA.append(pd.DataFrame({'date': [day], 'server': [s], 'expected operation time': [0], 'downtime': [0], 'actual operation time': [0], 'availability': [0]}, index=[0]))\n",
    "# create a duplicate of the column 'server', name it 'tailsign' and apply the tailsign_map to it\n",
    "df_SLA['tailsign'] = df_SLA['server'].copy()\n",
    "tailsign_map = {'tailsign': {'3442310010': 'VHYIO', '3467540006': 'VHYIR', '3352680006': 'VHYFJ', '3395600002': 'VHYIT', '3734240001': 'VHVUZ',\n",
    "                             '3413940001': 'VHYFE', '3472530005': 'VHYFF', '3413940014': '', '7CTCA20586': 'VHYUD', '3413940006': 'VHYIJ'}}\n",
    "df_SLA.replace(tailsign_map, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report visualisation\n",
    "\n",
    "## Succeeded and failed AppChecks (optional) \n",
    "\n",
    "The subsequent charts show the totals for the succeeded and failed AppChecks on all defined servers in the time period  \n",
    "that is contained in the CSV file.\n",
    "\n",
    "(For further visualisations, refer to this)[https://towardsdatascience.com/stacked-bar-charts-with-pythons-matplotlib-f4020e4eb4a7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a working copy of df_new, filtered by eventId = 'AppCheck', using only the columns: timestamp, serialNumber, eventId, PortalFrontendHomepage\n",
    "df_work_appcheck = df_new[df_new['eventId'] == 'AppCheck'][['timestamp', 'serialNumber', 'eventId', 'PortalFrontendHomepage', 'PortalFrontendApp', 'PortalRuntime', 'AnalyticsReceiver', 'MovingMap', 'FlightAPI', 'InflightContentServer', 'DRMWidevine', 'DRMFairplay']]\n",
    "# define the labels to be used in the charts\n",
    "labels = ['Portal Frontend', 'Portal Runtime', 'Flight API', 'Inflight Content Server']\n",
    "# i is just a counter to print the respective dataframe df_list[i]\n",
    "i = 0\n",
    "# start defining the stacked bar plot\n",
    "colors = ['#1D2F6F', '#8390FA', '#6EAF46', '#FAC748']\n",
    "width = 0.35\n",
    "#iterate over all servers and create a new dataframe for each server and component which contains the numbers of succeeded and failed component checks; store these new dataframes in df_list()\n",
    "for s in servers:\n",
    "    print('Iteration for server ' + str(s))\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax.set_ylabel('Number of AppChecks')\n",
    "    plt.xticks(\n",
    "        rotation=45,\n",
    "        horizontalalignment='right',\n",
    "        fontweight='light',\n",
    "        fontsize='medium',\n",
    "    )\n",
    "    # chart's title\n",
    "    ax.set_title('AppChecks for Server ' + str(s))\n",
    "    # suc is the series of successful appchecks for the current server and and the respective component\n",
    "    suc = pd.Series(dtype = 'object')\n",
    "    # fail is the series of failed appchecks for the current server and respective component\n",
    "    fail = pd.Series(dtype = 'object')\n",
    "    # create a list of dataframes\n",
    "    df_list = list()\n",
    "    # first, define a filter for all servers in list s\n",
    "    df_server_filter = (df_new['serialNumber'] == s)\n",
    "    # then, iterate over the labels (which are components of the appcheck) and create a dataframe per server and component\n",
    "    for component in components:\n",
    "        # reset the dataframe per server\n",
    "        df_component = ()\n",
    "        # define the components as categorical data using value 1, 0 (1 = success, 0 = failure)\n",
    "        df_work_appcheck[component] = pd.Categorical(df_work_appcheck[component], categories=[1, 0], ordered=True)\n",
    "        # create a new series for each component by grouping the filtered original dataframe by the component and count the number of occurences\n",
    "        df_component = df_work_appcheck.loc[df_server_filter, :].groupby([component])[component].count()\n",
    "        df_component = df_component.to_frame()\n",
    "        # append df_component.loc[1] to suc; suc now contains the number of successful appchecks for the current server and respective component\n",
    "        suc = suc.append(df_component.loc[1])\n",
    "        # append df_component.loc[0] to fail; fail now contains the number of failed appchecks for the current server and respective component\n",
    "        fail = fail.append(df_component.loc[0])\n",
    "        df_list.append(df_component)\n",
    "        i += 1\n",
    "\n",
    "    #we don't want to differentiate Portal Frontend Homepage and Portal Frontend App but consolidate them as Portal Frontend in the report\n",
    "    if suc.PortalFrontendApp < suc.PortalFrontendHomepage: \n",
    "        # drop suc.PortalFrontendHomepage\n",
    "        suc = suc.drop(suc.index[suc.index == 'PortalFrontendHomepage'])\n",
    "        fail = fail.drop(fail.index[fail.index == 'PortalFrontendHomepage'])\n",
    "    else:\n",
    "        # drop suc.PortalFrontendApp\n",
    "        suc = suc.drop(suc.index[suc.index == 'PortalFrontendApp'])\n",
    "        fail = fail.drop(fail.index[fail.index == 'PortalFrontendApp'])\n",
    "    # create a matplotlib ax.bar with labels and the data from the list of dataframes\n",
    "    if (suc.sum(axis=0) > 0 or fail.sum(axis=0) > 0): \n",
    "        ax.bar(labels, suc.to_list(), width, color=colors[3], label='Suceeded')\n",
    "        ax.bar(labels, fail.to_list(), width, bottom=suc.to_list(), color=colors[0], label='Failed')\n",
    "        for index,data in enumerate(suc.to_list()):\n",
    "            plt.text(x=index , y =data/2 , s=f\"{data}\" , fontdict=dict(fontsize=10))\n",
    "        for index,data in enumerate(fail.to_list()):\n",
    "            plt.text(x=index , y =suc.to_list()[index]+10 , s=f\"{data}\" , fontdict=dict(fontsize=10))\n",
    "        # remove spines\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['left'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "        # adjust limits and draw grid lines\n",
    "        plt.ylim(-0.5, ax.get_yticks()[-1] + 0.5)\n",
    "        ax.set_axisbelow(True)\n",
    "        ax.yaxis.grid(color='gray', linestyle='dashed')\n",
    "        # legend\n",
    "        plt.legend(['Success', 'Failed'], loc='upper left', ncol=4, frameon=False)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('No appchecks for server ' + str(s))\n",
    "        continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLA report - operational time expected vs achieved per day and server\n",
    "\n",
    "## Report definition\n",
    "We create a monthly report which includes per day and server the \n",
    "- Expected Operation Time\n",
    "- Sum of unplanned downtime\n",
    "- Passenger Service Availability Time\n",
    "- Passenger Service Availability Status % \n",
    "\n",
    "## Current assumptions\n",
    "### Components which to consider\n",
    "As earlier explained we only consider certain SW components to be relevant for the SLA\n",
    "```\n",
    "PortalFrontendHomepage, PortalFrontendApp (both in one consolidated status), PortalRuntime, Flight API, Inflight Content Server\n",
    "```\n",
    "From these components, we consider a failed AppCheck as a downtime of the Portal.  \n",
    "Several failed AppChecks in a row, sum up to a higher downtime.\n",
    "For instance: (AppCheck occurs every 10 minutes)\n",
    "> Portal Frontend on server <server> failed **2 times** in a row, \n",
    "> the calculated downtime is **20 minutes** for this server.  \n",
    "> Any other component failing in the same period of time won't add to the calculated downtime.  \n",
    "\n",
    "~~In addition, we consider the CWAP status and health in a way that an error on the CWAPs also leads \n",
    "to a decreased Passenger Service Availability time.~~\n",
    "\n",
    "> ~~A CWAP that shows an error in a single check is considered to cause a downtime 10 minutes on 25% of the passengers.~~  \n",
    "> A CWAP that shows an error for one of the radios is not considered to cause a downtime since since rooming takes place  \n",
    "> and passengers are connected to the next CWAP\n",
    "\n",
    "### Expected Operational Time\n",
    "- We consider every period of time between a 'power on' and 'power off' event that happens on-ground to add to the overall Expected Operational Time of the server. Power cycles that happen in-flight decrease the time (pls. refer to 'Additional considerations')\n",
    "\n",
    "### Additional considerations\n",
    "- Inflight (after Weight off wheels, before Weight on wheels), we consider a sequence of Power on/Power off events as a downtime of the service\n",
    "- Inflight, we consider the lack of AppChecks as a service downtime (remember: we conduct AppChecks every 10 minutes; if no AppCheck occur on a 1h flight, we consider this as a downtime of 1 hour)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new working df and only copy the columns we need\n",
    "#df_work_events = df_new[['timestamp', 'serialNumber', 'eventId', 'data.element0', 'data.element1', 'data.element2', 'data.element3', 'data.element4', 'data.element5', 'data.element6', 'FlightNo']]\n",
    "df_work_events = df_new.copy()\n",
    "# hey, let's make column 'timestamp' really a timestamp and use it as index\n",
    "df_work_events['timestamp'] = pd.to_datetime(df_work_events['timestamp'], format='%b %d, %Y @ %H:%M:%S.%f')\n",
    "#df_work_events.set_index('timestamp', inplace=True)\n",
    "\n",
    "# filter_power_on is furthermore used to filter all Power on/off events as well as Weight on/off wheels to be able to show operation times between those events\n",
    "# if operation time should be aggregated on a higher level, Weight on/off wheels events are not relevant and can be left ou\n",
    "#filter_power_on = (df_work_events['eventId'] == 'Power on/off')\n",
    "filter_power_on = (df_work_events['eventId'] == 'Power on/off') | (df_work_events['eventId'] == 'Weight off wheels') | (df_work_events['eventId'] == 'Weight on wheels')\n",
    "filter_wow = (df_work_events['eventId'] == 'Weight off wheels') | (df_work_events['eventId'] == 'Weight on wheels')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the expected operational time per server and date\n",
    "\n",
    "Assumption: The expected operational time is the sum of time differences between a server's first log (using timestamp for it) and it's shutdown time (available in data.element1 if eventId == 'Power On/Off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check on power on/off events that happen inflight and on-ground\n",
    "\n",
    "Inflight := the time between a ```Weight off wheels``` and a ```Weight on Wheels``` event.  \n",
    "All the time between a ```Weight off wheels``` and a ```Weight on Wheels``` event is considered to be Expected Operation Time.\n",
    "A ```Power on/off```event that happens inflight decreases the overall operation time.\n",
    "\n",
    "Even though that the SLA is defined on-ground as well as in-flight, we don't consider on-groud server re-boots as downtimes \n",
    "they could be caused by several reasons: LAME, Aircraft power change and others.\n",
    "\n",
    "We calculate the downtime that happened inflight per server and day in the following procedure.  \n",
    "Moreover, we calculate the time per server and day between each 'power on' and 'power off' event that happened on-ground.  \n",
    "All these time differences, we sum up to be the Expected Operational Time per server and day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "#let's restrict the list of servers to a single server (just for experimenting)\n",
    "#servers = list()\n",
    "#servers.append('3352680006')\n",
    "#iterate over all servers in list servers\n",
    "for s in servers:\n",
    "    #some basic definitions and local variable declarations\n",
    "    #from df_work_events, filter all logs for the currently processed server\n",
    "    filter_server = (df_work_events['serialNumber'] == s)\n",
    "    #create a new dataframe for the current server, with columns: timestamp, eventId, serialNumber, Power On, Power Off\n",
    "    col = ['timestamp', 'eventId', 'serialNumber', 'PowerOn', 'PowerOff']\n",
    "    df_server = df_work_events.loc[filter_server, :]\n",
    "    #drop all columns except the ones we need (timestamp, eventId, serialNumber, Power On, Power Off)\n",
    "    df_server = df_server[col]\n",
    "    #make sure the timestamp is really a timestamp; PowerOn and PowerOff are already timestamps\n",
    "    df_server['timestamp'] = pd.to_datetime(df_server['timestamp'])\n",
    "    # add the columns 'downtime' and 'opstime' to the dataframe df_server, both of type datetime with value NaN\n",
    "    df_server['downtime'] = pd.to_datetime(np.nan)\n",
    "    df_server['opstime'] = pd.to_datetime(np.nan)\n",
    "    # we now iterate over the WoW events and identify the Power on/off events that happened inflight/on the ground\n",
    "    # the Power on/off events that happened inflight are considered as downtime\n",
    "    # the Power on/off events that happened on the ground are considered as operation time\n",
    "    filter_wow = (df_server['eventId'] == 'Weight off wheels') | (df_server['eventId'] == 'Weight on wheels')\n",
    "    wow_timestamps = pd.Series(index=None, dtype='object')\n",
    "    wow_timestamps = df_server[filter_wow]['timestamp']\n",
    "    # - [ ] need to decide whether I keep the following line of code or remove it\n",
    "    df_server.set_index('timestamp', inplace=True)\n",
    "    #iterate over the pairs of WoW timestamps; so, in each case we should get two WoW events with other events between them\n",
    "    for (index, ts) in enumerate(wow_timestamps):\n",
    "        # if the current WoW event is the last one, we don't have a pair of WoW events to compare to\n",
    "        if index < len(wow_timestamps) - 1:\n",
    "            #let's find out whether we are inflight or on the-ground; the log data is sorted by timestamp descending\n",
    "            wow_eventId = df_server.loc[ts, 'eventId']\n",
    "            # get the current timestamp (WoW event) and the next timestamp (WoW event) to then filter all events in this WoW cycle\n",
    "            current_ts = ts # still a timestamp\n",
    "            next_ts = wow_timestamps.to_list()[index + 1] # still a timestamp\n",
    "            #we consider all the time between the current and the next WoW event as opstime; only if we are on ground, we will reduce the opstime by the times the server had been shut down\n",
    "            opstime = (current_ts - next_ts).total_seconds()\n",
    "            # slice the data and extract all log rows between WoW current_ts and WoW next_ts\n",
    "            df_server_wow_cycle = df_server.loc[current_ts:next_ts, :]\n",
    "            # add the columns 'downtime' and 'opstime' to the dataframe df_server, both of type datetime\n",
    "            #- [ ] need to decide whether I keep the following line of code or remove it\n",
    "            #df_server_wow_cycle['downtime'] = pd.to_datetime(np.nan)\n",
    "            #df_server_wow_cycle['opstime'] = pd.to_datetime(np.nan)\n",
    "            # filter all 'Power on/off' events from dataframe\n",
    "            filter_wow_cycle_power = (df_server_wow_cycle['eventId'] == 'Power on/off')\n",
    "            #1st use case: Weight on Wheels: the past period was inflight. We consider all server reboot events as downtime\n",
    "            if wow_eventId == 'Weight on wheels':\n",
    "                # for all Power on/off events found, we calculate the downtime as the difference between the event's PowerOn- (in column 'PowerOn') and the PowerOff-timestamp (in column 'PowerOff')\n",
    "                # we store the downtime in the column 'downtime' off the respective Power on/off event's row\n",
    "                #Have we found some inflight Power cycles?\n",
    "                if len(df_server_wow_cycle[filter_wow_cycle_power]) > 0:\n",
    "                    #If yes...\n",
    "                    #print('Server ' + str(s) + ' has ' + str(len(df_server_wow_cycle.loc[filter_wow_cycle_power, :])) + ' power on/off events in-flight at ' + str(current_ts))\n",
    "                    for index, row in df_server_wow_cycle[filter_wow_cycle_power].iterrows():\n",
    "                        # get the timestamp of the current Power on/off event\n",
    "                        power_on_ts = row['PowerOn']\n",
    "                        # get the timestamp of the last Power on/off event\n",
    "                        power_off_ts = row['PowerOff']\n",
    "                        # calculate the downtime as the difference between the current timestamp and the last timestamp in seconds\n",
    "                        downtime = (power_on_ts - power_off_ts).total_seconds()\n",
    "                        # store the downtime in the respective Power on/off event in the server's dataframe\n",
    "                        df_server.loc[index, 'downtime'] = downtime\n",
    "                        #print('Have added ' + str(df_server.loc[ts, 'downtime']) + ' seconds to the downtime of ' + str(s))\n",
    "                #being inflight, we add to the Expected Operation Time the full time span between the two WoW events\n",
    "                df_server.loc[current_ts, 'opstime'] = opstime\n",
    "                #print('Have added ' + str(opstime) + ' seconds to the total opstime of ' + str(s))\n",
    "            #2nd use case: Weight off Wheels: the past period was on the ground. We consider all server reboot events as operation time\n",
    "            elif wow_eventId == 'Weight off wheels':\n",
    "                #Have we found some on-ground Power cycles?\n",
    "                if len(df_server_wow_cycle[filter_wow_cycle_power]) > 0:\n",
    "                    #print('Server ' + str(s) + ' has ' + str(len(df_server_wow_cycle.loc[filter_wow_cycle_power, :])) + ' power on/off events on-ground at ' + str(current_ts))\n",
    "                    #If yes...we will decrease the opstime by the time the server has been shut down. On ground, we don't consider these power cycles as unplanned downtime\n",
    "                    #so they don't do anything to the downtime\n",
    "                    for index, row in df_server_wow_cycle[filter_wow_cycle_power].iterrows():\n",
    "                        # get the timestamp of the current Power on/off event\n",
    "                        power_on_ts = row['PowerOn']\n",
    "                        # get the timestamp of the last Power on/off event\n",
    "                        power_off_ts = row['PowerOff']\n",
    "                        # calculate the operation time as the difference between the current timestamp and the last timestamp in seconds\n",
    "                        opstime = opstime - (power_on_ts - power_off_ts).total_seconds()\n",
    "                df_server.loc[current_ts, 'opstime'] = opstime\n",
    "                #print('Have added ' + str(opstime) + ' seconds to the total opstime of ' + str(s))\n",
    "    # we now have the downtime and the operation time for the server\n",
    "    # let's now group the downtime by date and sum up all the values in the column 'sum'\n",
    "    df_server_grouped_downtime = df_server.groupby(df_server.index.date)[['downtime']].sum()\n",
    "    for (index, row) in df_server_grouped_downtime.iterrows():\n",
    "        #check if row[0] is NaN\n",
    "        if not np.isnan(row[0]):\n",
    "            df_SLA.loc[(df_SLA.server == s) & (df_SLA.date.dt.date == index), 'downtime'] = row['downtime']\n",
    "        else:\n",
    "            df_SLA.loc[(df_SLA.server == s) & (df_SLA.date.dt.date == index), 'downtime'] = 0\n",
    "    df_server_grouped_opstime = df_server.groupby(df_server.index.date)[['opstime']].sum()\n",
    "    for (index, row) in df_server_grouped_opstime.iterrows():\n",
    "        #check if row[0] is NaN\n",
    "        if not np.isnan(row[0]):\n",
    "            df_SLA.loc[((df_SLA.server == s) & (df_SLA.date.dt.date == index)), 'expected operation time'] = row['opstime']\n",
    "        else:\n",
    "            df_SLA.loc[((df_SLA.server == s) & (df_SLA.date.dt.date == index)), 'expected operation time'] = 0\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of the aggregated AppCheck\n",
    "\n",
    "In the subsequent analysis, we check how many appchecks succeeded or failed between two Power on/off events for the defined servers.  \n",
    "\n",
    "We can easily increase transparency for this analysis and consider 'Weight on/off wheels' as well  \n",
    "(by changing the filter *filter_power_on*) and thus, come to a view where we can differentiate  \n",
    "in-flight and on-ground behaviour.\n",
    "\n",
    "The result of this report currently contains:  \n",
    "- The event (Power on/off, WoW) which occured (event) and \n",
    "- when server has been powered on and off (poweron, poweroff),\n",
    "- whether the CWAPs have been successfully activated (status: *Activated* in column cwap) in the period between the previous and the current event\n",
    "- how many clients have been connected (client connected) in this period\n",
    "- the number of appchecks having been conducted (appcheck) in this period\n",
    "- the calculated availability (availability) in terms of succeeded or failed appchecks in this period\n",
    "- the ops time (opstime) of the server as the difference between the latest power off event and the last power on event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the following section needs to be continued......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey, found an appcheck issue for 3442310010 at 2021-12-01 04:32:45.466000\n",
      "Hey, found an appcheck issue for 3442310010 at 2021-12-01 04:32:45.447000\n",
      "Hey, found an appcheck issue for 3442310010 at 2021-11-27 06:41:46.733000\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'Timestamp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/yt/g3pjgfp12l913d500hlnzffc0000gn/T/ipykernel_32997/1916330441.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m                         \u001b[0mdowntime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcurrent_ts\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnext_ts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_seconds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                         \u001b[0;31m#store the downtime in the respective appcheck event in the server's dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                         \u001b[0mdf_appcheck\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'downtime'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdowntime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m                 \u001b[0;31m#else: # if wow_event == 'Weight on wheels':\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                     \u001b[0;31m# do nothing since we have been on ground\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0miloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"iloc\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m         \u001b[0miloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1611\u001b[0m                     \u001b[0;31m# so the object is the same\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1612\u001b[0m                     \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1613\u001b[0;31m                     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1614\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1615\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_update_cacher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/numeric.py\u001b[0m in \u001b[0;36minsert\u001b[0;34m(self, loc, item)\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_union\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36minsert\u001b[0;34m(self, loc, item)\u001b[0m\n\u001b[1;32m   5560\u001b[0m         \u001b[0;31m#  so self is never backed by an EA.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5561\u001b[0m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5562\u001b[0;31m         \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coerce_scalar_to_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5563\u001b[0m         \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5564\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_coerce_scalar_to_index\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   4189\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4191\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_attributes_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4193\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_fill_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, data, dtype, copy, name, tupleize_cols, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0;31m# other iterable of some kind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray_tuplesafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m     \"\"\"\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, data, dtype, copy, name, tupleize_cols, **kwargs)\u001b[0m\n\u001b[1;32m    335\u001b[0m                 \u001b[0;31m# should not be coerced\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m                 \u001b[0;31m# GH 11836\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_maybe_cast_with_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m                 \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m  \u001b[0;31m# TODO: maybe not for object?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_maybe_cast_with_dtype\u001b[0;34m(data, dtype, copy)\u001b[0m\n\u001b[1;32m   6008\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6009\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6010\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6011\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_float_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6012\u001b[0m         \u001b[0minferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'Timestamp'"
     ]
    }
   ],
   "source": [
    "# iterate over all servers in list servers\n",
    "for s in servers:\n",
    "    #some basic definitions and local variable declarations\n",
    "    downtime = 0\n",
    "    #from df_work_events, filter all logs for the currently processed server\n",
    "    filter_server = (df_work_events['serialNumber'] == s)\n",
    "    #create a new dataframe for the appcheck analysis of the current server, with columns: timestamp, eventId, serialNumber, Power On, Power Off\n",
    "    col = ['timestamp', 'eventId', 'data.element4', 'data.element5', 'serialNumber', 'PaxConnected'] + components\n",
    "    df_appcheck = df_work_events.loc[filter_server, :]\n",
    "    #drop all columns except the ones we need (timestamp, eventId, serialNumber, Power On, Power Off)\n",
    "    df_appcheck = df_appcheck[col]\n",
    "    #make sure the timestamp is really a timestamp;\n",
    "    df_appcheck['timestamp'] = pd.to_datetime(df_appcheck['timestamp'])\n",
    "    # add the column 'downtime' to the dataframe (of type datetime with value NaN)\n",
    "    df_appcheck['downtime'] = pd.to_datetime(np.nan)\n",
    "    #filter df_appcheck for all entries with eventId = 'AppCheck'; these are the events we are interested in\n",
    "    filter_appcheck = (df_appcheck['eventId'] == 'AppCheck')\n",
    "    #iterate over all appcheck events\n",
    "    #to iterate over all WoW events, we need an index\n",
    "    wow_event_index = 0\n",
    "    #I want to have all WoW events for the current server in a single dataframe\n",
    "    filter_appcheck_wow = ((df_appcheck['eventId'] == 'Weight off wheels') | (df_appcheck['eventId'] == 'Weight on wheels'))\n",
    "    df_appcheck_wow = df_appcheck.loc[filter_appcheck_wow, :]\n",
    "    #create a view/filter on appcheck_timestamps to iterate over all timestamps\n",
    "    appcheck_timestamps = pd.Series(index=None, dtype='object')\n",
    "    appcheck_timestamps = df_appcheck[filter_appcheck]['timestamp']\n",
    "    for index, ts in enumerate(appcheck_timestamps):\n",
    "        if index < len(appcheck_timestamps) - 1:\n",
    "            if index == 431:\n",
    "                print('Here we are')\n",
    "            # get the timestamp of the current appcheck event\n",
    "            # - [ ] make sure not to iterate over the wow-events series end\n",
    "            current_ts = ts\n",
    "            # get the timestamp of the next appcheck event to calculate the max downtime\n",
    "            next_ts = appcheck_timestamps.to_list()[index + 1]\n",
    "            #look back in history and find the previous WoW event; depending on the WoW event, we will add to the downtime or not\n",
    "            #if there is no further WoW event or no WoW event at all, we take the last one and derive from it in which flight phase we are\n",
    "            #check whether df_app_wow_cycle has any entries or if wow_event_index is greater than the amount of entries in df_appcheck_wow\n",
    "            if (len(df_appcheck_wow) > 0):# and (index < (len(df_appcheck_wow) - 1)):\n",
    "                #define variable wow_event and store the value of the previous WoW event \n",
    "                #from df_appcheck, get the first element for which timestamp < current_ts\n",
    "                wow_event = df_appcheck_wow.loc[df_appcheck_wow['timestamp'] < current_ts, :].iloc[0, :]['eventId']\n",
    "                #wow_event = df_appcheck_wow.iloc[wow_event_index]['eventId']\n",
    "                #define variable wow_timestamp and store the timestamp of the previous WoW event\n",
    "                wow_timestamp = df_appcheck_wow.loc[df_appcheck_wow['timestamp'] < current_ts, :].iloc[0, :]['timestamp']\n",
    "                #wow_timestamp = df_appcheck_wow.iloc[wow_event_index]['timestamp']\n",
    "                wow_event_index = wow_event_index + 1\n",
    "                appcheck_succeeded = df_appcheck.loc[df_appcheck['timestamp'] == current_ts, components].sum(axis=1)\n",
    "                if appcheck_succeeded.sum() < 5:\n",
    "                    print('Hey, found an appcheck issue for ' + str(s) + ' at ' + str(current_ts))\n",
    "                #if wow_event is Weight off wheels (we are inflight!), we have to calculate the downtime as the difference between the current timestamp and the last timestamp in seconds\n",
    "                if wow_event == 'Weight off wheels':\n",
    "                    #have we had a Portal downtime? (i.e. one of the components' appchecks has been 0 and thus the sum of the appchecks is smaller than the sum of the components)\n",
    "                    appcheck_succeeded = df_appcheck.loc[df_appcheck['timestamp'] == current_ts, components].sum(axis=1)\n",
    "                    if (not appcheck_succeeded.empty) and (appcheck_succeeded.iloc[0] < len(components)):\n",
    "                        #calculate the downtime as the difference between the current timestamp and the last timestamp in seconds\n",
    "                        downtime = (current_ts - next_ts).total_seconds()\n",
    "                        #store the downtime in the respective appcheck event in the server's dataframe\n",
    "                        df_appcheck.loc[ts, 'downtime'] = downtime\n",
    "                #else: # if wow_event == 'Weight on wheels':\n",
    "                    # do nothing since we have been on ground\n",
    "    # - [ ] aggregate the downtime for each day\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over all servers in list servers\n",
    "for s in servers:\n",
    "    print('Iteration for server ' + str(s))\n",
    "    server_downtime = 0\n",
    "    filter_server = (df_work_events['serialNumber'] == s)\n",
    "    #filter_power_on is defined in a section earlier in this notebook (!!! it might include power events only or WoW events as well!!)\n",
    "    filter = (filter_server) & (filter_power_on)\n",
    "    # define a temporary dataframe for each server, on which we perform date-based selections\n",
    "    df_appcheck = df_work_events.loc[filter_server, :]\n",
    "    # define a pd Series that will store all timestamps when event 'Power on/off' was logged on this server\n",
    "    power_timestamps = pd.Series(dtype='object');\n",
    "    # create a series for each server with the timestamp of match for eventId = 'Power on/off'\n",
    "    power_timestamps = df_appcheck[filter_power_on]['timestamp']\n",
    "    # make timestamp the index, so that we can localise certain rows easily\n",
    "    df_appcheck.set_index('timestamp', inplace=True)\n",
    "    # create a new dataframe df_sla with timestamp as index and the columns: event, timestamp, cwap, appcheck, availability \n",
    "    df_sla = pd.DataFrame(columns=['event', 'poweron', 'poweroff', 'timestamp', 'cwap', 'client connected', 'appcheck', 'availability', 'powercycle'])\n",
    "    # fill the dataframe with the values from the series\n",
    "    # first, fill in when Power on/off occured\n",
    "    df_sla['event'] = df_work_events[filter]['eventId']\n",
    "    df_sla['timestamp'] = power_timestamps\n",
    "    # data.element0 contains the power on timestamp and data.element1 contains the power off timestamp\n",
    "    df_sla['poweron'] = df_work_events[filter]['data.element0']\n",
    "    df_sla['poweroff'] = df_work_events[filter]['data.element1']\n",
    "    df_sla['poweron'] = pd.to_datetime(df_sla['poweron'])\n",
    "    df_sla['poweroff'] = pd.to_datetime(df_sla['poweroff'])\n",
    "    # second, from df_work_events filter all events 'CWAP status' between two power cycles and fill in the dataframe\n",
    "    # iterate over the pairs of elements of the series power_timestamps (thus, the period between two power cycles)\n",
    "    for (index, ts) in enumerate(power_timestamps):\n",
    "        if index < len(power_timestamps.to_list()) - 1:\n",
    "            # get the current timestamp (start powercycle) and the next timestamp (end powercycle) to then filter all events in this power cycle\n",
    "            current_ts = ts\n",
    "            next_ts = power_timestamps.to_list()[index + 1]\n",
    "            #from df_temp slice all rows between current_ts and next_ts to be able to get all events for one power cycle\n",
    "            df_temp_slice = df_appcheck.loc[current_ts:next_ts, :]\n",
    "            # make columns data.element5 and data.element6 from df_temp_slice integers and ignore NaN values\n",
    "            df_temp_slice['data.element5'] = pd.to_numeric(df_temp_slice['data.element5'], errors='coerce')\n",
    "            df_temp_slice['data.element6'] = pd.to_numeric(df_temp_slice['data.element6'], errors='coerce')\n",
    "            # collect and fill in the connected pax for this power cycle\n",
    "            # define a list of columns in which connected pax numbers are stored\n",
    "            pax_connected_list = ['data.element5', 'data.element6']\n",
    "            # sum up the values from columns ['data.element5', 'data.element6'] in df_temp_slice, where eventId = 'CWAP status'\n",
    "            pax_connected = df_temp_slice.loc[df_temp_slice['eventId'] == 'CWAP status', pax_connected_list].sum(axis=1)\n",
    "            # if pax_connected isn't empty and the sum of axis=1 is not 0, then fill in the value\n",
    "            if not pax_connected.empty and pax_connected.sum(axis=0) != 0:\n",
    "                df_sla.loc[df_sla.timestamp == current_ts, 'client connected'] =  pax_connected.sum(axis=0)\n",
    "            else:\n",
    "                df_sla.loc[df_sla.timestamp == current_ts, 'client connected'] =  0\n",
    "            # in the next step, aggregate the status of the appcheck for this power cycle for the defined components in list 'components'\n",
    "            # succeeded appchecks per component are written as 1 in the respective component's column\n",
    "            # we sum up the values (1 or 0) from the components' columns per appcheck conducted;\n",
    "            appcheck_succeeded = df_temp_slice.loc[df_temp_slice['eventId'] == 'AppCheck', components].sum(axis=1)\n",
    "            # if at least one component appcheck failed, we add 10 minutes (and only 10!) to the current server's downtime\n",
    "            if not appcheck_succeeded.empty: # and appcheck_succeeded < len(components):\n",
    "                #iterate over all elements in series appcheck_succeeded\n",
    "                for (index, succeeded) in enumerate(appcheck_succeeded):\n",
    "                    if succeeded < len(components):\n",
    "                        server_downtime = server_downtime + 10*60\n",
    "                        #add the server_downtime directly to df_SLA with server == s and date == date(current_ts)\n",
    "                        date = current_ts.date()\n",
    "                        try:\n",
    "                            server_downtime += df_SLA.loc[(df_SLA.server == s) & (df_SLA.date == date), 'downtime'].values[0]\n",
    "                        except:\n",
    "                            server_downtime += 0\n",
    "                        finally:\n",
    "                            df_SLA.loc[(df_SLA.server == s) & (df_SLA.date.dt.date == date), 'downtime'] = server_downtime\n",
    "            # define appchecks as the number of rows from df_temp_slice where eventId = 'AppCheck'; i.e. appchecks is the number of appchecks conducted in this power cycle\n",
    "            appchecks = df_temp_slice.loc[df_temp_slice['eventId'] == 'AppCheck'].shape[0]\n",
    "            # we now sum up the overall number of appchecks succeded in this powercycle\n",
    "            sum_appcheck_succeeded = appcheck_succeeded.sum(axis=0)\n",
    "            # avail is the number of succeeded component appchecks divided by the number of appchecks (multiplied by number of components) multiplied by 100\n",
    "            component_appchecks = appchecks * len(components)\n",
    "            avail = (sum_appcheck_succeeded / (component_appchecks)) * 100\n",
    "            # finally, write the calculated values to the dataframe\n",
    "            df_sla.loc[df_sla.timestamp == current_ts, 'appcheck'] =  sum_appcheck_succeeded\n",
    "            df_sla.loc[df_sla.timestamp == current_ts, 'availability'] =  avail\n",
    "            # finally, aggregate the cwap health status for this power cycle\n",
    "            # define a list of columns in which cwap status is stored; data.element3 and 4 hold the actual radio status \n",
    "            cwap_list = ['data.element3', 'data.element4']\n",
    "            # filter for CWAP health status 'Error' and if such a status occured, write it to the dataframe and add 10 minutes to the current server's downtime\n",
    "            cwap_health_filter = (df_temp_slice['eventId'] == 'CWAP health') & ((df_temp_slice['data.element3'] == 'Error') | (df_temp_slice['data.element4'] == 'Error'))\n",
    "            # sum up the values from columns in cwap_list, where eventId = 'CWAP health' to detect whether one of the radios is in error\n",
    "            cwap = df_temp_slice.loc[cwap_health_filter, cwap_list].count(axis=1)\n",
    "            if not cwap.empty and cwap.sum(axis=0) != 0:\n",
    "                cwap_down = cwap.sum(axis=0) / 2\n",
    "                df_sla.loc[df_sla.timestamp == current_ts, 'cwap'] =  'Error'\n",
    "                ####################################################\n",
    "                # as per a common decision on Dec, 9th, we won't consider CWAP reported errrors as service downtimes\n",
    "                ####################################################\n",
    "                #server_downtime = server_downtime + cwap_down*10*60\n",
    "            else:\n",
    "                df_sla.loc[df_sla.timestamp == current_ts, 'cwap'] =  'Activated'\n",
    "            # last but not least, fill in the powercycle column; it's the time difference between the poweron and poweroff of the current powercycle\n",
    "            # if poweron is not empty or NaN and poweroff is not empty or NaN, then fill in the value\n",
    "            power_on = df_sla.loc[df_sla.timestamp == current_ts, 'poweron'].count()\n",
    "            power_off = df_sla.loc[df_sla.timestamp == current_ts, 'poweroff'].count()\n",
    "            if (power_on > 0) and (power_off > 0):\n",
    "                df_sla.loc[df_sla.timestamp == current_ts, 'powercycle'] =  df_sla.loc[df_sla.timestamp == current_ts, 'poweroff'] - df_sla.loc[df_sla.timestamp == current_ts, 'poweron']\n",
    "            df_sla['powercycle'] = pd.to_timedelta(df_sla['powercycle'],'h')\n",
    "            df_sla['ops time'] = df_sla.poweron - df_sla.poweroff\n",
    "            #df_sla['powercycle'] = df_sla['powercycle']/np.timedelta64(1,'h')\n",
    "\n",
    "        # remember, the index of series power_timestamps is the same as in df_work_events\n",
    "    print('Downtime for server ' + s + ': ' + str(server_downtime))\n",
    "    df_sla.set_index('timestamp', inplace=True)\n",
    "#    df_sla.to_markdown('sla_' + str(s) + '.md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final calculations of actual operational time and availability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in the actual operation time which is the 'expected' operation time minus the downtime\n",
    "# if you re-run solely this section, you will receive an error, since the operation performed will fail after column has been converted to datetime some lines below\n",
    "# fill in availability, which is the 'expected operation time' minus the downtime divided by the 'expected operation time' and take care about divisions by zero\n",
    "try:\n",
    "    eot_filter = df_SLA['expected operation time'] != 0\n",
    "    df_SLA.loc[eot_filter, 'actual operation time'] = (df_SLA.loc[eot_filter , 'expected operation time'] - df_SLA.loc[eot_filter, 'downtime'])\n",
    "    df_SLA.loc[eot_filter, 'availability'] = ((df_SLA.loc[eot_filter , 'expected operation time'] - df_SLA.loc[eot_filter, 'downtime']) / df_SLA.loc[eot_filter, 'expected operation time']) * 100\n",
    "except:\n",
    "    df_SLA['availability'] = 0\n",
    "# convert the columns 'expected operation time', 'downtime' and 'acutal operation time' which currently contain a number in seconds, to hours and format it HH:MM:SS\n",
    "df_SLA['actual operation time'] = (pd.to_datetime(df_SLA['actual operation time'], unit='s')).dt.strftime('%H:%M:%S')\n",
    "df_SLA['expected operation time'] = (pd.to_datetime(df_SLA['expected operation time'], unit='s')).dt.strftime('%H:%M:%S')\n",
    "df_SLA['downtime'] = (pd.to_datetime(df_SLA['downtime'], unit='s')).dt.strftime('%H:%M:%S')\n",
    "#format column 'availability' as a percentage number\n",
    "df_SLA['availability'] = df_SLA['availability'].map('{:,.2f}%'.format)\n",
    "# show the column types of df_SLA\n",
    "print(df_SLA.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SLA['availability'] = df_SLA['availability'].map('{:,.2f}%'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the dataframe df_SLA to a csv file\n",
    "df_SLA.to_csv('sla_' + str(s) + '.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1e39f5ddb06da47c792e6724e01a6f520ddcc160c593e0336e4f03a3f0c97e2f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('jupyterlab-debugger': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
