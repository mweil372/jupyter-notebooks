{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VA Passenger Availability SLA Report\n",
    "\n",
    "The Passenger Availability SLA report measures the availability of the service to the passengers when the MSU is switched on. It is measured using an automated script on the server checking & logging the availability of the SSID (WLAN) and the software health after the booting phase. The Operation Time starts after the final boot of the software initiated by power on.\n",
    "\n",
    "This SLA can only be applicable for software provided/developed by Lufthansa Systems. Any 3rd Party application by Air Dolomiti or respective 3rd Party vendors by Air Dolomiti is not part of the SLA measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the log data\n",
    "\n",
    "The server logs for Virgin Australia come as an CSV export from BoardConnect's Kibana.\n",
    "In this step, we import the csv file and print out the dimensions (# of rows, columns) of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv (VAPaxAvailSLAv2) into dataframe\n",
    "df = pd.read_csv('/Users/u293733/git.jupyter-notebooks/VAPaxAvailSLAv2.csv')\n",
    "# outlign the shape \n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data adjustments\n",
    "\n",
    "We keep the original data and create a copy of it for further processing.\n",
    "In this copy we adjust the data so that it becomes - for instance - better readable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer event ids\n",
    "\n",
    "In the original dataset, the events which had been logged, are stored as numbers.\n",
    "We convert these numbers to humanly readable event descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new dataframe from df and add column EventID\n",
    "df_new = df.copy()\n",
    "# map the eventID to readable event names\n",
    "eventId_map = {'eventId': {10891: 'CWAP health', 10892: 'CWAP health', 10893: 'CWAP health', 10894: 'CWAP health', \n",
    "                        10900: 'CWAP status', 10901: 'CWAP status', 10902: 'CWAP status', 10903: 'CWAP status',\n",
    "                        30100: 'AppCheck', 10811: 'Weight on wheels', 10812: 'Weight off wheels', 10957: 'Power on/off'}}\n",
    "# replace eventId with readable values from the map\n",
    "df_new.replace(eventId_map, inplace=True)\n",
    "# in Python, it's a good practice to typecase categorical features to a category type to fasten up the processing of the data\n",
    "df_new['eventId'] = df_new['eventId'].astype('category')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract grouped data\n",
    "\n",
    "Some logged data comes as a combined string and, for further processing and better interpretation, now needs to be split and stored\n",
    "in additional fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for eventId = 'AppCheck' and create a new column 'FlightNo' with the first element of the split column 'data.element0'\n",
    "df_new['FlightNo'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[0]\n",
    "# filter for eventId = 'CWAP status' and create a new column 'PaxConnected' with the value from 'data.element5'\n",
    "df_new['PaxConnected'] = df_new[df_new['eventId'] == 'CWAP status']['data.element5']\n",
    "# filter for eventId = 'AppCheck' and take the fourth element out of 'data.element0'; get the first integer out of it and add this to new column 'PortalFrontendHomepage' (1 = success)\n",
    "df_new['PortalFrontendHomepage'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[3].str.split(':').str[1].str.split(',').str[0].astype(int)\n",
    "# proceed with the other components that are checked by BCEL AppCheck\n",
    "df_new['PortalFrontendApp'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[4].str.split(':').str[1].str.split(',').str[0].astype(int)\n",
    "df_new['PortalRuntime'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[5].str.split(':').str[1].str.split(',').str[0].astype(int)\n",
    "df_new['AnalyticsReceiver'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[6].str.split(':').str[1].str.split(',').str[0].astype(int)\n",
    "df_new['MovingMap'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[7].str.split(':').str[1].str.split(',').str[0].astype(int)\n",
    "df_new['FlightAPI'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[8].str.split(':').str[1].str.split(',').str[0].astype(int)\n",
    "df_new['InflightContentServer'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[9].str.split(':').str[1].str.split(',').str[0].astype(int)\n",
    "df_new['DRMWidevine'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[10].str.split(':').str[1].str.split(',').str[0].astype(int)\n",
    "df_new['DRMFairplay'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[11].str.split(':').str[1].str.split(',').str[0].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report visualisation\n",
    "\n",
    "## Stacked bar chart of succeeded and failed appchecks per component and server (thus, aircraft)\n",
    "\n",
    "(For further visualisations, refer to this)[https://towardsdatascience.com/stacked-bar-charts-with-pythons-matplotlib-f4020e4eb4a7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a working copy of df_new, filtered by eventId = 'AppCheck', using only the columns: timestamp, serialNumber, eventId, PortalFrontendHomepage\n",
    "df_work_appcheck = df_new[df_new['eventId'] == 'AppCheck'][['timestamp', 'serialNumber', 'eventId', 'PortalFrontendHomepage', 'PortalFrontendApp', 'PortalRuntime', 'AnalyticsReceiver', 'MovingMap', 'FlightAPI', 'InflightContentServer', 'DRMWidevine', 'DRMFairplay']]\n",
    "\n",
    "# then, create a list of labels for the diagram\n",
    "labels = ['PortalFrontendHomepage', 'PortalFrontendApp', 'PortalRuntime', 'AnalyticsReceiver', 'MovingMap', 'FlightAPI', 'InflightContentServer', 'DRMWidevine', 'DRMFairplay']\n",
    "# create a list of all servers in the report \n",
    "servers = list()\n",
    "for s in df_new['serialNumber'].unique().tolist():\n",
    "    servers.append(s)\n",
    "#servers.append('7CTCA20586')\n",
    "# i is just a counter to print the respective dataframe df_list[i]\n",
    "i = 0\n",
    "# start defining the stacked bar plot\n",
    "colors = ['#1D2F6F', '#8390FA', '#6EAF46', '#FAC748']\n",
    "width = 0.35\n",
    "#iterate over all servers and create a new dataframe for each server and component which contains the numbers of succeeded and failed component checks; store these new dataframes in df_list()\n",
    "for s in servers:\n",
    "    print('Iteration for server ' + str(s))\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax.set_ylabel('Number of AppChecks')\n",
    "    plt.xticks(\n",
    "        rotation=45,\n",
    "        horizontalalignment='right',\n",
    "        fontweight='light',\n",
    "        fontsize='medium',\n",
    "    )\n",
    "    # chart's title\n",
    "    ax.set_title('AppChecks for Server ' + str(s))\n",
    "    # suc is the series of successful appchecks for the current server and and the respective component\n",
    "    suc = pd.Series(dtype = 'object')\n",
    "    # fail is the series of failed appchecks for the current server and respective component\n",
    "    fail = pd.Series(dtype = 'object')\n",
    "    # create a list of dataframes\n",
    "    df_list = list()\n",
    "    # first, define a filter for all servers in list s\n",
    "    df_server_filter = (df_new['serialNumber'] == s)\n",
    "    # then, iterate over the labels (which are components of the appcheck) and create a dataframe per server and component\n",
    "    for component in labels:\n",
    "        # reset the dataframe per server\n",
    "        df_component = ()\n",
    "        # define the components as categorical data using value 1, 0 (1 = success, 0 = failure)\n",
    "        df_work_appcheck[component] = pd.Categorical(df_work_appcheck[component], categories=[1, 0], ordered=True)\n",
    "        # create a new series for each component by grouping the filtered original dataframe by the component and count the number of occurences\n",
    "        df_component = df_work_appcheck.loc[df_server_filter, :].groupby([component])[component].count()\n",
    "        df_component = df_component.to_frame()\n",
    "        # append df_component.loc[1] to suc; suc now contains the number of successful appchecks for the current server and respective component\n",
    "        suc = suc.append(df_component.loc[1])\n",
    "        # append df_component.loc[0] to fail; fail now contains the number of failed appchecks for the current server and respective component\n",
    "        fail = fail.append(df_component.loc[0])\n",
    "        df_list.append(df_component)\n",
    "        i += 1\n",
    "\n",
    "    # create a matplotlib ax.bar with labels and the data from the list of dataframes\n",
    "    if (suc.sum(axis=0) > 0 or fail.sum(axis=0) > 0): \n",
    "        ax.bar(labels, suc.to_list(), width, color=colors[3], label='Suceeded')\n",
    "        ax.bar(labels, fail.to_list(), width, bottom=suc.to_list(), color=colors[0], label='Failed')\n",
    "        for index,data in enumerate(suc.to_list()):\n",
    "            plt.text(x=index , y =data/2 , s=f\"{data}\" , fontdict=dict(fontsize=10))\n",
    "        for index,data in enumerate(fail.to_list()):\n",
    "            plt.text(x=index , y =suc.to_list()[index]+10 , s=f\"{data}\" , fontdict=dict(fontsize=10))\n",
    "        # remove spines\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['left'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "        # adjust limits and draw grid lines\n",
    "        plt.ylim(-0.5, ax.get_yticks()[-1] + 0.5)\n",
    "        ax.set_axisbelow(True)\n",
    "        ax.yaxis.grid(color='gray', linestyle='dashed')\n",
    "        # legend\n",
    "        plt.legend(['Success', 'Failed'], loc='upper left', ncol=4, frameon=False)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('No appchecks for server ' + str(s))\n",
    "        continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular overview of the events per server\n",
    "\n",
    "What I need:\n",
    "- Event 30100 = AppCheck: ['FlightNo'], info whether (at least) one of the appchecks failed\n",
    "- Event 10900-10903: CWAP status (BITE Status:\"%1\"; Internal Power Status:\"%2\"; SSID Status:\"%3\"; Uptime:data.element3; Clients Connected:\"%5\"; Clients on 2.4 GHz: data.element5; Clients on 5 GHz:data.element6;  )\n",
    "- Event 10891-10894: CWAP health: data.element4 <Activated|Error>\n",
    "- Event 10957: Last PowerOn/Off (data.element0 = Power on, data.element1 = Power off)\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "servers = list()\n",
    "servers.append('7CTCA20586')\n",
    "#for s in df_new['serialNumber'].unique().tolist():\n",
    "#    servers.append(s)\n",
    "\n",
    "# create a new working df and only copy the columns we need\n",
    "#df_work_events = df_new[['timestamp', 'serialNumber', 'eventId', 'data.element0', 'data.element1', 'data.element2', 'data.element3', 'data.element4', 'data.element5', 'data.element6', 'FlightNo']]\n",
    "df_work_events = df_new.copy()\n",
    "# hey, let's make column 'timestamp' really a timestamp and use it as index\n",
    "df_work_events['timestamp'] = pd.to_datetime(df_work_events['timestamp'], format='%b %d, %Y @ %H:%M:%S.%f')\n",
    "#print(df_work_events.shape)\n",
    "#print(df_work_events.info())\n",
    "#print(df_work_events.head())\n",
    "#df_work_events.set_index('timestamp', inplace=True)\n",
    "\n",
    "# iterate over all servers in list servers\n",
    "for s in servers:\n",
    "    print('Iteration for server ' + str(s))\n",
    "    filter_server = (df_work_events['serialNumber'] == s)\n",
    "    #filter_power_on = (df_work_events['eventId'] == 'Power on/off')\n",
    "    filter_power_on = (df_work_events['eventId'] == 'Power on/off') | (df_work_events['eventId'] == 'Weight off wheels') | (df_work_events['eventId'] == 'Weight on wheels')\n",
    "    filter = (filter_server) & (filter_power_on)\n",
    "    # define a temporary dataframe for each server, on which we perform date-based selections\n",
    "    df_temp = df_work_events.loc[filter_server, :]\n",
    "    # define a pd Series that will store all timestamps when event 'Power on/off' was logged on this server\n",
    "    power_timestamps = pd.Series(dtype='object');\n",
    "    # create a series for each server with the timestamp of match for eventId = 'Power on/off'\n",
    "    power_timestamps = df_temp[filter_power_on]['timestamp']\n",
    "    # make timestamp the index, so that we can localise certain rows easily\n",
    "    df_temp.set_index('timestamp', inplace=True)\n",
    "    # create a new dataframe df_sla with timestamp as index and the columns: event, timestamp, cwap, appcheck, availability \n",
    "    df_sla = pd.DataFrame(columns=['event', 'poweron', 'poweroff', 'timestamp', 'cwap', 'client connected', 'appcheck', 'availability', 'powercycle'])\n",
    "    # fill the dataframe with the values from the series\n",
    "    # first, fill in when Power on/off occured\n",
    "    df_sla['event'] = df_work_events[filter]['eventId']\n",
    "    df_sla['timestamp'] = power_timestamps\n",
    "    # data.element0 contains the power on timestamp and data.element1 contains the power off timestamp\n",
    "    df_sla['poweron'] = df_work_events[filter]['data.element0']\n",
    "    df_sla['poweroff'] = df_work_events[filter]['data.element1']\n",
    "    df_sla['poweron'] = pd.to_datetime(df_sla['poweron'])\n",
    "    df_sla['poweroff'] = pd.to_datetime(df_sla['poweroff'])\n",
    "    # second, from df_work_events filter all events 'CWAP status' between two power cycles and fill in the dataframe\n",
    "    # iterate over the pairs of elements of the series power_timestamps (thus, the period between two power cycles)\n",
    "    for (index, ts) in enumerate(power_timestamps):\n",
    "        if index < len(power_timestamps.to_list()) - 1:\n",
    "            # get the current timestamp (start powercycle) and the next timestamp (end powercycle) to then filter all events in this power cycle\n",
    "            current_ts = ts\n",
    "            next_ts = power_timestamps.to_list()[index + 1]\n",
    "            #from df_temp slice all rows between current_ts and next_ts to be able to get all events for one power cycle\n",
    "            df_temp_slice = df_temp.loc[current_ts:next_ts, :]\n",
    "            # make columns data.element5 and data.element6 from df_temp_slice integers and ignore NaN values\n",
    "            df_temp_slice['data.element5'] = pd.to_numeric(df_temp_slice['data.element5'], errors='coerce')\n",
    "            df_temp_slice['data.element6'] = pd.to_numeric(df_temp_slice['data.element6'], errors='coerce')\n",
    "            # collect and fill in the connected pax for this power cycle\n",
    "            # define a list of columns in which connected pax numbers are stored\n",
    "            pax_connected_list = ['data.element5', 'data.element6']\n",
    "            # sum up the values from columns ['data.element5', 'data.element6'] in df_temp_slice, where eventId = 'CWAP status'\n",
    "            pax_connected = df_temp_slice.loc[df_temp_slice['eventId'] == 'CWAP status', pax_connected_list].sum(axis=1)\n",
    "            # if pax_connected isn't empty and the sum of axis=1 is not 0, then fill in the value\n",
    "            if not pax_connected.empty and pax_connected.sum(axis=0) != 0:\n",
    "                df_sla.loc[df_sla.timestamp == current_ts, 'client connected'] =  pax_connected.sum(axis=0)\n",
    "            else:\n",
    "                df_sla.loc[df_sla.timestamp == current_ts, 'client connected'] =  0\n",
    "            # in the next step, aggregate the status of the appcheck for this power cycle\n",
    "            # all components (frontend, runtime etc.) need to be mandatorily considered for the appcheck\n",
    "            # for an aggregated appcheck status, we sum up the values of the columns ['data.element2', 'data.element3', 'data.element4', 'data.element7', 'data.element8', 'data.element9'] in a power cycle from df_temp_slice\n",
    "            # define a list of columns in which appcheck status is stored\n",
    "            appcheck_list = ['PortalFrontendHomepage', 'PortalFrontendApp', 'PortalRuntime', 'AnalyticsReceiver', 'MovingMap', 'FlightAPI', 'InflightContentServer', 'DRMWidevine', 'DRMFairplay']\n",
    "            # sum up the values from columns in appchecklist; thus, this is the number of succeeded appchecks per appcheck in this power cycle\n",
    "            appcheck_succeeded = df_temp_slice.loc[df_temp_slice['eventId'] == 'AppCheck', appcheck_list].sum(axis=1)\n",
    "            # define appchecks as the number of rows from df_temp_slice where eventId = 'AppCheck'; i.e. appchecks is the number of appchecks in this power cycle\n",
    "            appchecks = df_temp_slice.loc[df_temp_slice['eventId'] == 'AppCheck'].shape[0]\n",
    "            # the overall number of appchecks succeded in this powercycle\n",
    "            sum_appcheck_succeeded = appcheck_succeeded.sum(axis=0)\n",
    "            # avail is the number of succeeded appchecks divided by the number of appchecks (multiplied by number of components) multiplied by 100\n",
    "            avail = (sum_appcheck_succeeded / (appchecks * 9)) * 100\n",
    "            df_sla.loc[df_sla.timestamp == current_ts, 'appcheck'] =  sum_appcheck_succeeded\n",
    "            df_sla.loc[df_sla.timestamp == current_ts, 'availability'] =  avail\n",
    "            # finally, aggregate the cwap health status for this power cycle\n",
    "            # define a list of columns in which cwap status is stored\n",
    "            cwap_list = ['data.element3', 'data.element4']\n",
    "            # sum up the values from columns in cwap_list, where eventId = 'CWAP health'\n",
    "            cwap_health_filter = (df_temp_slice['eventId'] == 'CWAP health') & (df_temp_slice['data.element3'] == 'Error')\n",
    "            # sum up the values from columns in cwap_list, where eventId = 'CWAP health'\n",
    "            cwap = df_temp_slice.loc[cwap_health_filter, cwap_list].count(axis=1)\n",
    "            if not cwap.empty and cwap.sum(axis=0) != 0:\n",
    "                df_sla.loc[df_sla.timestamp == current_ts, 'cwap'] =  'Error'\n",
    "            else:\n",
    "                df_sla.loc[df_sla.timestamp == current_ts, 'cwap'] =  'Activated'\n",
    "            # last but not least, fill in the powercycle column; it's the time difference between the poweron and poweroff of the current powercycle\n",
    "            # if poweron is not empty or NaN and poweroff is not empty or NaN, then fill in the value\n",
    "            power_on = df_sla.loc[df_sla.timestamp == current_ts, 'poweron'].count()\n",
    "            power_off = df_sla.loc[df_sla.timestamp == current_ts, 'poweroff'].count()\n",
    "            if (power_on > 0) and (power_off > 0):\n",
    "                df_sla.loc[df_sla.timestamp == current_ts, 'powercycle'] =  df_sla.loc[df_sla.timestamp == current_ts, 'poweroff'] - df_sla.loc[df_sla.timestamp == current_ts, 'poweron']\n",
    "            df_sla['powercycle'] = pd.to_timedelta(df_sla['powercycle'],'h')\n",
    "            #df_sla['powercycle'] = df_sla['powercycle']/np.timedelta64(1,'h')\n",
    "\n",
    "        # remember, the index of series power_timestamps is the same as in df_work_events\n",
    "    df_sla.set_index('timestamp', inplace=True)\n",
    "    df_sla.to_markdown('sla_' + str(s) + '.md')\n",
    "\"\"\" pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "df_sla.style\n",
    "df_sla\n",
    " \"\"\"\n",
    "    #\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1e39f5ddb06da47c792e6724e01a6f520ddcc160c593e0336e4f03a3f0c97e2f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('jupyterlab-debugger': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
