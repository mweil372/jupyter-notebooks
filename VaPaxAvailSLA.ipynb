{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VA Passenger Availability SLA Report\n",
    "\n",
    "Once the upgrade to Linux has been completed it is measured using an automated script on the server measuring the availability of the Passenger Service by checking & logging the availability of the SSID (WLAN) and the software health after the booting phase. The System Uptime and hence the Expected Operation Time starts after the booting phase of the software initiated by power on. Any time the Passenger Service then is not available is considered an Unplanned Downtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open tasks\n",
    "- [ ] add/improve description (Why? What? How?)\n",
    "- [x] make consideration of DRM Widevine/Fairplay, Moving Map, Analytics Receiver optional and configurable in the report\n",
    "- [ ] set first appcheck (timestamp) as the start of operation time and power off as the end\n",
    "- [ ] consider CWAP unavailabilities by counting Errors as unavailability\n",
    "- [ ] sum up the 10 minute periods of a failed app check to unavailability time\n",
    "- [ ] structure the code and, for instance, use functions\n",
    "- [ ] implement Kibana API interface\n",
    "- [ ] come up with a CI/CD workflow for this/publish the Jupyter report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the log data\n",
    "\n",
    "## Source of information\n",
    "The data, that the Juypter Notebook processes and bases the report on, comes from **BoardConnect Kibana Prod**.\n",
    "\n",
    "\n",
    "Since the Notebook currently does not have a direct interface to Kibana implemented, we need to request the data\n",
    "in Kibana and export it as a **CSV** file.\n",
    "\n",
    "In Kibana, one fine a stored request which is named **VAPaxAvailSLAv2** and which should be executed using the\n",
    "relevant timeframe that should be considered in the report (for instance: November 2021).\n",
    "\n",
    "In a first step, the CSV file needs to be provided to Juypter Notebook, so that it can be read and processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv (VAPaxAvailSLAv2) into dataframe\n",
    "df = pd.read_csv('/Users/u293733/git.jupyter-notebooks/VAPaxAvailSLAv2.csv')\n",
    "# outlign the shape \n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data adjustments\n",
    "\n",
    "We keep the original data in the dataframe df and create a copy of it for further processing.  \n",
    "In this copy we adjust the data so that it becomes - for instance - better readable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Event Ids\n",
    "\n",
    "In the original dataset, the events which had been logged, are stored as numbers.  \n",
    "We convert these numbers to humanly readable event descriptions. \n",
    "\n",
    "|Event ID | Event name|\n",
    "|--------|--------|\n",
    "|10891+ | CWAP health |\n",
    "|10900+ | CWAP status |\n",
    "|30100 | AppCheck |\n",
    "|10811 | Weight on wheels |\n",
    "|10812 | Weight off wheels |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new dataframe from df and add column EventID\n",
    "df_new = df.copy()\n",
    "# map the eventID to readable event names\n",
    "eventId_map = {'eventId': {10891: 'CWAP health', 10892: 'CWAP health', 10893: 'CWAP health', 10894: 'CWAP health', \n",
    "                        10900: 'CWAP status', 10901: 'CWAP status', 10902: 'CWAP status', 10903: 'CWAP status',\n",
    "                        30100: 'AppCheck', 10811: 'Weight on wheels', 10812: 'Weight off wheels', 10957: 'Power on/off'}}\n",
    "# replace eventId with readable values from the map\n",
    "df_new.replace(eventId_map, inplace=True)\n",
    "# in Python, it's a good practice to typecase categorical features to a category type to fasten up the processing of the data\n",
    "df_new['eventId'] = df_new['eventId'].astype('category')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract grouped data\n",
    "\n",
    "Some logged data comes as a concatenated string and, for further processing and better interpretation, now needs to be split and stored\n",
    "in separate fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for eventId = 'AppCheck' and create a new column 'FlightNo' with the first element of the split column 'data.element0'\n",
    "df_new['FlightNo'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[0]\n",
    "# filter for eventId = 'CWAP status' and create a new column 'PaxConnected' with the value from 'data.element5'\n",
    "df_new['PaxConnected'] = df_new[df_new['eventId'] == 'CWAP status']['data.element5']\n",
    "# filter for eventId = 'AppCheck' and take the fourth element out of 'data.element0'; get the first integer out of it and add this to new column 'PortalFrontendHomepage' (1 = success)\n",
    "df_new['PortalFrontendHomepage'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[3].str.split(':').str[1].str.split(',').str[0].astype(int)\n",
    "# proceed with the other components that are checked by BCEL AppCheck\n",
    "df_new['PortalFrontendApp'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[4].str.split(':').str[1].str.split(',').str[0].astype(int)\n",
    "df_new['PortalRuntime'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[5].str.split(':').str[1].str.split(',').str[0].astype(int)\n",
    "df_new['AnalyticsReceiver'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[6].str.split(':').str[1].str.split(',').str[0].astype(int)\n",
    "df_new['MovingMap'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[7].str.split(':').str[1].str.split(',').str[0].astype(int)\n",
    "df_new['FlightAPI'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[8].str.split(':').str[1].str.split(',').str[0].astype(int)\n",
    "df_new['InflightContentServer'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[9].str.split(':').str[1].str.split(',').str[0].astype(int)\n",
    "df_new['DRMWidevine'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[10].str.split(':').str[1].str.split(',').str[0].astype(int)\n",
    "df_new['DRMFairplay'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[11].str.split(':').str[1].str.split(',').str[0].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration section\n",
    "\n",
    "In this section we define the **components** that the Juypter Notebook shall consider for report generation.\n",
    "\n",
    "**Hint**\n",
    "> The AppCheck currently considers 9 different components to be mandatorily checked by it.\n",
    "> These are:\n",
    "> - Portal Frontend Homepage\n",
    "> - Portal Frontend App\n",
    "> - Portal Runtime\n",
    "> - Analytics Receiver\n",
    "> - Moving Map\n",
    "> - Flight API\n",
    "> - Inflight Content Server\n",
    "> - DRM Widevine\n",
    "> - DRM Fairplay\n",
    "\n",
    "As per an agreement with the VA PDM from Dec, 2nd 2021, we consider the following components as relevant for measuring the BC IFE service availability:\n",
    "\n",
    "> - Portal Frontend Homepage\n",
    "> - Portal Frontend App\n",
    "> - Portal Runtime\n",
    "> - Flight API\n",
    "> - Inflight Content Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of components for the diagram; we leave out the non-SLA-relevant components and combine PortalFrontendHomepage and PortalFrontendApp \n",
    "#components = ['PortalFrontendHomepage', 'PortalFrontendApp', 'PortalRuntime', 'AnalyticsReceiver', 'MovingMap', 'FlightAPI', 'InflightContentServer', 'DRMWidevine', 'DRMFairplay']\n",
    "components = ['PortalFrontendHomepage', 'PortalFrontendApp', 'PortalRuntime', 'FlightAPI', 'InflightContentServer']\n",
    "# create a list of all servers in the report \n",
    "servers = list()\n",
    "for s in df_new['serialNumber'].unique().tolist():\n",
    "    servers.append(s)\n",
    "# if other servers (or a subset of servers) than contained in the CSV file shall be used, append those explicitly to the list\n",
    "#servers.append('7CTCA20586')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report visualisation\n",
    "\n",
    "## Succeeded and failed AppChecks (optional) \n",
    "\n",
    "The subsequent charts show the totals for the succeeded and failed AppChecks on all defined servers in the time period  \n",
    "that is contained in the CSV file.\n",
    "\n",
    "(For further visualisations, refer to this)[https://towardsdatascience.com/stacked-bar-charts-with-pythons-matplotlib-f4020e4eb4a7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a working copy of df_new, filtered by eventId = 'AppCheck', using only the columns: timestamp, serialNumber, eventId, PortalFrontendHomepage\n",
    "df_work_appcheck = df_new[df_new['eventId'] == 'AppCheck'][['timestamp', 'serialNumber', 'eventId', 'PortalFrontendHomepage', 'PortalFrontendApp', 'PortalRuntime', 'AnalyticsReceiver', 'MovingMap', 'FlightAPI', 'InflightContentServer', 'DRMWidevine', 'DRMFairplay']]\n",
    "# define the labels to be used in the charts\n",
    "labels = ['Portal Frontend', 'Portal Runtime', 'Flight API', 'Inflight Content Server']\n",
    "# i is just a counter to print the respective dataframe df_list[i]\n",
    "i = 0\n",
    "# start defining the stacked bar plot\n",
    "colors = ['#1D2F6F', '#8390FA', '#6EAF46', '#FAC748']\n",
    "width = 0.35\n",
    "#iterate over all servers and create a new dataframe for each server and component which contains the numbers of succeeded and failed component checks; store these new dataframes in df_list()\n",
    "for s in servers:\n",
    "    print('Iteration for server ' + str(s))\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax.set_ylabel('Number of AppChecks')\n",
    "    plt.xticks(\n",
    "        rotation=45,\n",
    "        horizontalalignment='right',\n",
    "        fontweight='light',\n",
    "        fontsize='medium',\n",
    "    )\n",
    "    # chart's title\n",
    "    ax.set_title('AppChecks for Server ' + str(s))\n",
    "    # suc is the series of successful appchecks for the current server and and the respective component\n",
    "    suc = pd.Series(dtype = 'object')\n",
    "    # fail is the series of failed appchecks for the current server and respective component\n",
    "    fail = pd.Series(dtype = 'object')\n",
    "    # create a list of dataframes\n",
    "    df_list = list()\n",
    "    # first, define a filter for all servers in list s\n",
    "    df_server_filter = (df_new['serialNumber'] == s)\n",
    "    # then, iterate over the labels (which are components of the appcheck) and create a dataframe per server and component\n",
    "    for component in components:\n",
    "        # reset the dataframe per server\n",
    "        df_component = ()\n",
    "        # define the components as categorical data using value 1, 0 (1 = success, 0 = failure)\n",
    "        df_work_appcheck[component] = pd.Categorical(df_work_appcheck[component], categories=[1, 0], ordered=True)\n",
    "        # create a new series for each component by grouping the filtered original dataframe by the component and count the number of occurences\n",
    "        df_component = df_work_appcheck.loc[df_server_filter, :].groupby([component])[component].count()\n",
    "        df_component = df_component.to_frame()\n",
    "        # append df_component.loc[1] to suc; suc now contains the number of successful appchecks for the current server and respective component\n",
    "        suc = suc.append(df_component.loc[1])\n",
    "        # append df_component.loc[0] to fail; fail now contains the number of failed appchecks for the current server and respective component\n",
    "        fail = fail.append(df_component.loc[0])\n",
    "        df_list.append(df_component)\n",
    "        i += 1\n",
    "\n",
    "    #we don't want to differentiate Portal Frontend Homepage and Portal Frontend App but consolidate them as Portal Frontend in the report\n",
    "    if suc.PortalFrontendApp < suc.PortalFrontendHomepage: \n",
    "        # drop suc.PortalFrontendHomepage\n",
    "        suc = suc.drop(suc.index[suc.index == 'PortalFrontendHomepage'])\n",
    "        fail = fail.drop(fail.index[fail.index == 'PortalFrontendHomepage'])\n",
    "    else:\n",
    "        # drop suc.PortalFrontendApp\n",
    "        suc = suc.drop(suc.index[suc.index == 'PortalFrontendApp'])\n",
    "        fail = fail.drop(fail.index[fail.index == 'PortalFrontendApp'])\n",
    "    # create a matplotlib ax.bar with labels and the data from the list of dataframes\n",
    "    if (suc.sum(axis=0) > 0 or fail.sum(axis=0) > 0): \n",
    "        ax.bar(labels, suc.to_list(), width, color=colors[3], label='Suceeded')\n",
    "        ax.bar(labels, fail.to_list(), width, bottom=suc.to_list(), color=colors[0], label='Failed')\n",
    "        for index,data in enumerate(suc.to_list()):\n",
    "            plt.text(x=index , y =data/2 , s=f\"{data}\" , fontdict=dict(fontsize=10))\n",
    "        for index,data in enumerate(fail.to_list()):\n",
    "            plt.text(x=index , y =suc.to_list()[index]+10 , s=f\"{data}\" , fontdict=dict(fontsize=10))\n",
    "        # remove spines\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['left'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "        # adjust limits and draw grid lines\n",
    "        plt.ylim(-0.5, ax.get_yticks()[-1] + 0.5)\n",
    "        ax.set_axisbelow(True)\n",
    "        ax.yaxis.grid(color='gray', linestyle='dashed')\n",
    "        # legend\n",
    "        plt.legend(['Success', 'Failed'], loc='upper left', ncol=4, frameon=False)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('No appchecks for server ' + str(s))\n",
    "        continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SLA  - operational time exected vs achieved\n",
    "\n",
    "### Report definition\n",
    "We create a monthly report which includes per day and server the \n",
    "- Expected Operation Time\n",
    "- Sum of unplanned downtime\n",
    "- Passenger Service Availability Time\n",
    "- Passenger Service Availability Status % \n",
    "\n",
    "### Current assumptions\n",
    "#### Components which to consider\n",
    "As earlier explained we only consider certain SW components to be relevant for the SLA\n",
    "```\n",
    "PortalFrontendHomepage, PortalFrontendApp (both in one consolidated status), PortalRuntime, Flight API, Inflight Content Server\n",
    "```\n",
    "From these components, we consider a failed AppCheck as a downtime of the Portal.  \n",
    "Several failed AppChecks in a row, sum up to a higher downtime.\n",
    "For instance: (AppCheck occurs every 10 minutes)\n",
    "> Portal Frontend on server <server> failed **2 times** in a row, \n",
    "> the calculated downtime is **20 minutes** for this server.  \n",
    "> Any other component failing in the same period of time won't add to the calculated downtime.  \n",
    "\n",
    "In addition, we consider the CWAP status and health in a way that an error on the CWAPs also leads \n",
    "to a decreased Passenger Service Availability time.\n",
    "\n",
    "> A CWAP that shows an error in a single check is considered to cause a downtime 10 minutes on 25% of the passengers.\n",
    "\n",
    "#### Additional considerations\n",
    "- Inflight (after Weight off wheels, before Weight on wheels), we consider a sequence of Power on/Power off events as a downtime of the service\n",
    "- Inflight, we consider the lack of AppChecks as a service downtime (remember: we conduct AppChecks every 10 minutes; if no AppCheck occur on a 1h flight, we consider this as a downtime of 1 hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new working df and only copy the columns we need\n",
    "#df_work_events = df_new[['timestamp', 'serialNumber', 'eventId', 'data.element0', 'data.element1', 'data.element2', 'data.element3', 'data.element4', 'data.element5', 'data.element6', 'FlightNo']]\n",
    "df_work_events = df_new.copy()\n",
    "# hey, let's make column 'timestamp' really a timestamp and use it as index\n",
    "df_work_events['timestamp'] = pd.to_datetime(df_work_events['timestamp'], format='%b %d, %Y @ %H:%M:%S.%f')\n",
    "#df_work_events.set_index('timestamp', inplace=True)\n",
    "\n",
    "# filter_power_on is furthermore used to filter all Power on/off events as well as Weight on/off wheels to be able to show operation times between those events\n",
    "# if operation time should be aggregated on a higher level, Weight on/off wheels events are not relevant and can be left ou\n",
    "#filter_power_on = (df_work_events['eventId'] == 'Power on/off')\n",
    "filter_power_on = (df_work_events['eventId'] == 'Power on/off') | (df_work_events['eventId'] == 'Weight off wheels') | (df_work_events['eventId'] == 'Weight on wheels')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground\n",
    "\n",
    "In the playground, I play around with the data to check certain hypothesis, validate things etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check on power on/off events that happen inflight\n",
    "\n",
    "Inflight := the time between a ```Weight off wheels``` and a ```Weight on Wheels``` event.  \n",
    "A ```Power on/off```event that happens inflight decreases the overall operation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_wow = (df_work_events['eventId'] == 'Weight off wheels') | (df_work_events['eventId'] == 'Weight on wheels')\n",
    "for s in servers:\n",
    "    # counter c for power cycles per server in inflight phas\n",
    "    c = 0\n",
    "    # counter po_ground\n",
    "    po_ground = 0\n",
    "    # counter po_inflight\n",
    "    po_inflight = 0\n",
    "    filter_server = (df_work_events['serialNumber'] == s)\n",
    "    df_temp = df_work_events.loc[filter_server, :]\n",
    "    # find out whether an event \"Power on/off\" occurs between two events \"Weight off wheels\" and \"Weight on wheels\"\n",
    "    wow_timestamps = pd.Series(dtype='object');\n",
    "    # create a series for each server with the timestamp of match for eventId = 'Power on/off'\n",
    "    wow_timestamps = df_temp[filter_wow]['timestamp']\n",
    "    df_temp.set_index('timestamp', inplace=True)\n",
    "    for (index, ts) in enumerate(wow_timestamps):\n",
    "        if index < len(wow_timestamps.to_list()) - 1:\n",
    "            # from df_temp get the eventId where timestamp = ts\n",
    "            wow_eventId = df_temp.loc[ts, 'eventId']\n",
    "            # get the current timestamp (start powercycle) and the next timestamp (end powercycle) to then filter all events in this power cycle\n",
    "            current_ts = ts\n",
    "            next_ts = wow_timestamps.to_list()[index + 1]\n",
    "            df_temp_slice = df_temp.loc[current_ts:next_ts, :]\n",
    "            # in df_temp_slice count all rows where eventId = 'Power on/off'\n",
    "            power_on_count = df_temp_slice[df_temp_slice['eventId'] == 'Power on/off']['eventId'].count()\n",
    "            # if df_temp_slice is not empty, then we have a power cycle\n",
    "            if power_on_count > 0:\n",
    "                c += 1\n",
    "                # if wow_eventId = 'Weight off wheels' then the currently found power cycles happened on ground\n",
    "                # why is this: the data in the CSV is sorted from newer to older (top to bottom)\n",
    "                if wow_eventId == 'Weight off wheels':\n",
    "                   po_ground += 1\n",
    "                else:\n",
    "                    po_inflight += 1\n",
    "\n",
    "    print('Server ' + str(s) + ': ' + str(c) + ' power cycles. Ground: ' + str(po_ground) + ' inflight: ' + str(po_inflight))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SLA report per day and server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over all servers in list servers\n",
    "for s in servers:\n",
    "    print('Iteration for server ' + str(s))\n",
    "    filter_server = (df_work_events['serialNumber'] == s)\n",
    "    filter = (filter_server) & (filter_power_on)\n",
    "    # define a temporary dataframe for each server, on which we perform date-based selections\n",
    "    df_temp = df_work_events.loc[filter_server, :]\n",
    "    # define a pd Series that will store all timestamps when event 'Power on/off' was logged on this server\n",
    "    power_timestamps = pd.Series(dtype='object');\n",
    "    # create a series for each server with the timestamp of match for eventId = 'Power on/off'\n",
    "    power_timestamps = df_temp[filter_power_on]['timestamp']\n",
    "    # make timestamp the index, so that we can localise certain rows easily\n",
    "    df_temp.set_index('timestamp', inplace=True)\n",
    "    # create a new dataframe df_sla with timestamp as index and the columns: event, timestamp, cwap, appcheck, availability \n",
    "    df_sla = pd.DataFrame(columns=['event', 'poweron', 'poweroff', 'timestamp', 'cwap', 'client connected', 'appcheck', 'availability', 'powercycle'])\n",
    "    # fill the dataframe with the values from the series\n",
    "    # first, fill in when Power on/off occured\n",
    "    df_sla['event'] = df_work_events[filter]['eventId']\n",
    "    df_sla['timestamp'] = power_timestamps\n",
    "    # data.element0 contains the power on timestamp and data.element1 contains the power off timestamp\n",
    "    df_sla['poweron'] = df_work_events[filter]['data.element0']\n",
    "    df_sla['poweroff'] = df_work_events[filter]['data.element1']\n",
    "    df_sla['poweron'] = pd.to_datetime(df_sla['poweron'])\n",
    "    df_sla['poweroff'] = pd.to_datetime(df_sla['poweroff'])\n",
    "    # second, from df_work_events filter all events 'CWAP status' between two power cycles and fill in the dataframe\n",
    "    # iterate over the pairs of elements of the series power_timestamps (thus, the period between two power cycles)\n",
    "    for (index, ts) in enumerate(power_timestamps):\n",
    "        if index < len(power_timestamps.to_list()) - 1:\n",
    "            # get the current timestamp (start powercycle) and the next timestamp (end powercycle) to then filter all events in this power cycle\n",
    "            current_ts = ts\n",
    "            next_ts = power_timestamps.to_list()[index + 1]\n",
    "            #from df_temp slice all rows between current_ts and next_ts to be able to get all events for one power cycle\n",
    "            df_temp_slice = df_temp.loc[current_ts:next_ts, :]\n",
    "            # make columns data.element5 and data.element6 from df_temp_slice integers and ignore NaN values\n",
    "            df_temp_slice['data.element5'] = pd.to_numeric(df_temp_slice['data.element5'], errors='coerce')\n",
    "            df_temp_slice['data.element6'] = pd.to_numeric(df_temp_slice['data.element6'], errors='coerce')\n",
    "            # collect and fill in the connected pax for this power cycle\n",
    "            # define a list of columns in which connected pax numbers are stored\n",
    "            pax_connected_list = ['data.element5', 'data.element6']\n",
    "            # sum up the values from columns ['data.element5', 'data.element6'] in df_temp_slice, where eventId = 'CWAP status'\n",
    "            pax_connected = df_temp_slice.loc[df_temp_slice['eventId'] == 'CWAP status', pax_connected_list].sum(axis=1)\n",
    "            # if pax_connected isn't empty and the sum of axis=1 is not 0, then fill in the value\n",
    "            if not pax_connected.empty and pax_connected.sum(axis=0) != 0:\n",
    "                df_sla.loc[df_sla.timestamp == current_ts, 'client connected'] =  pax_connected.sum(axis=0)\n",
    "            else:\n",
    "                df_sla.loc[df_sla.timestamp == current_ts, 'client connected'] =  0\n",
    "            # in the next step, aggregate the status of the appcheck for this power cycle\n",
    "            # all components (frontend, runtime etc.) need to be mandatorily considered for the appcheck\n",
    "            # for an aggregated appcheck status, we sum up the values of the columns ['data.element2', 'data.element3', 'data.element4', 'data.element7', 'data.element8', 'data.element9'] in a power cycle from df_temp_slice\n",
    "            # define a list of columns in which appcheck status is stored\n",
    "            appcheck_list = ['PortalFrontendHomepage', 'PortalFrontendApp', 'PortalRuntime', 'AnalyticsReceiver', 'MovingMap', 'FlightAPI', 'InflightContentServer', 'DRMWidevine', 'DRMFairplay']\n",
    "            # sum up the values from columns in appchecklist; thus, this is the number of succeeded appchecks per appcheck in this power cycle\n",
    "            appcheck_succeeded = df_temp_slice.loc[df_temp_slice['eventId'] == 'AppCheck', appcheck_list].sum(axis=1)\n",
    "            # define appchecks as the number of rows from df_temp_slice where eventId = 'AppCheck'; i.e. appchecks is the number of appchecks in this power cycle\n",
    "            appchecks = df_temp_slice.loc[df_temp_slice['eventId'] == 'AppCheck'].shape[0]\n",
    "            # the overall number of appchecks succeded in this powercycle\n",
    "            sum_appcheck_succeeded = appcheck_succeeded.sum(axis=0)\n",
    "            # avail is the number of succeeded appchecks divided by the number of appchecks (multiplied by number of components) multiplied by 100\n",
    "            avail = (sum_appcheck_succeeded / (appchecks * 9)) * 100\n",
    "            df_sla.loc[df_sla.timestamp == current_ts, 'appcheck'] =  sum_appcheck_succeeded\n",
    "            df_sla.loc[df_sla.timestamp == current_ts, 'availability'] =  avail\n",
    "            # finally, aggregate the cwap health status for this power cycle\n",
    "            # define a list of columns in which cwap status is stored\n",
    "            cwap_list = ['data.element3', 'data.element4']\n",
    "            # sum up the values from columns in cwap_list, where eventId = 'CWAP health'\n",
    "            cwap_health_filter = (df_temp_slice['eventId'] == 'CWAP health') & (df_temp_slice['data.element3'] == 'Error')\n",
    "            # sum up the values from columns in cwap_list, where eventId = 'CWAP health'\n",
    "            cwap = df_temp_slice.loc[cwap_health_filter, cwap_list].count(axis=1)\n",
    "            if not cwap.empty and cwap.sum(axis=0) != 0:\n",
    "                df_sla.loc[df_sla.timestamp == current_ts, 'cwap'] =  'Error'\n",
    "            else:\n",
    "                df_sla.loc[df_sla.timestamp == current_ts, 'cwap'] =  'Activated'\n",
    "            # last but not least, fill in the powercycle column; it's the time difference between the poweron and poweroff of the current powercycle\n",
    "            # if poweron is not empty or NaN and poweroff is not empty or NaN, then fill in the value\n",
    "            power_on = df_sla.loc[df_sla.timestamp == current_ts, 'poweron'].count()\n",
    "            power_off = df_sla.loc[df_sla.timestamp == current_ts, 'poweroff'].count()\n",
    "            if (power_on > 0) and (power_off > 0):\n",
    "                df_sla.loc[df_sla.timestamp == current_ts, 'powercycle'] =  df_sla.loc[df_sla.timestamp == current_ts, 'poweroff'] - df_sla.loc[df_sla.timestamp == current_ts, 'poweron']\n",
    "            df_sla['powercycle'] = pd.to_timedelta(df_sla['powercycle'],'h')\n",
    "            df_sla['ops time'] = df_sla.poweron - df_sla.poweroff\n",
    "            #df_sla['powercycle'] = df_sla['powercycle']/np.timedelta64(1,'h')\n",
    "\n",
    "        # remember, the index of series power_timestamps is the same as in df_work_events\n",
    "    df_sla.set_index('timestamp', inplace=True)\n",
    "    df_sla.to_markdown('sla_' + str(s) + '.md')\n",
    "\"\"\" pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "df_sla.style\n",
    "df_sla\n",
    " \"\"\"\n",
    "    #\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1e39f5ddb06da47c792e6724e01a6f520ddcc160c593e0336e4f03a3f0c97e2f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('jupyterlab-debugger': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
