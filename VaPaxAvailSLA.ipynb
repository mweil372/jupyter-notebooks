{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VA Passenger Availability SLA Report\n",
    "\n",
    "Once the upgrade to Linux has been completed it is measured using an automated script on the server measuring the availability of the Passenger Service by checking & logging the availability of the SSID (WLAN) and the software health after the booting phase. The System Uptime and hence the Expected Operation Time starts after the booting phase of the software initiated by power on. Any time the Passenger Service then is not available is considered an Unplanned Downtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open tasks\n",
    "- [x] add/improve description (Why? What? How?)\n",
    "- [x] make consideration of DRM Widevine/Fairplay, Moving Map, Analytics Receiver optional and configurable in the report\n",
    "- [x] set first appcheck (timestamp) as the start of operation time and power off as the end\n",
    "- [x] consider CWAP unavailabilities by counting Errors as unavailability\n",
    "- [x] sum up the 10 minute periods of a failed app check to unavailability time (thus, decrease 'actual operation time' accordingly)\n",
    "- [x] make the appchecks' downtimes available in df_sla\n",
    "- [ ] structure the code and, for instance, use functions\n",
    "- [ ] implement Kibana API interface\n",
    "- [ ] come up with a CI/CD workflow for this/publish the Jupyter report\n",
    "- [x] define the overall SLA dataframe\n",
    "- [x] calculate the Expected Operation Time\n",
    "- [x] and store it in the SLA dataframe (per day and server)\n",
    "- [x] sum up the server in-flight reboots as unexpected downtime\n",
    "- [x] and add them to the SLA dataframe (per day and server)\n",
    "- [x] clean up the notebook and remove unsused, irrelevant variables\n",
    "- [x] introduce server/tailsign mapping\n",
    "- [ ] get the month, which I use to initialise df_SLA, automatically out of the imported data\n",
    "- [x] do not consider AppCheck downtimes that happened on ground\n",
    "- [x] do not consider CWAP Errors to reduce the expected operational time\n",
    "- [x] consider the log events before the first and after the last WoW event (where no other corresponding event exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the log data\n",
    "\n",
    "## Source of information\n",
    "The data, that the Juypter Notebook processes and bases the report on, comes from **BoardConnect Kibana Prod**.\n",
    "\n",
    "\n",
    "Since the Notebook currently does not have a direct interface to Kibana implemented, we need to request the data\n",
    "in Kibana and export it as a **CSV** file.\n",
    "\n",
    "In Kibana, one fine a stored request which is named **VAPaxAvailSLAv2** and which should be executed using the\n",
    "relevant timeframe that should be considered in the report (for instance: November 2021).\n",
    "\n",
    "In a first step, the CSV file needs to be provided to Juypter Notebook, so that it can be read and processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61469, 12)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read csv (VAPaxAvailSLAv2) into dataframe\n",
    "df = pd.read_csv('/Users/u293733/git.jupyter-notebooks/VAPaxAvailSLAv2.csv')\n",
    "# outlign the shape \n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data adjustments\n",
    "\n",
    "We keep the original data in the dataframe df and create a copy of it for further processing.  \n",
    "In this copy we adjust the data so that it becomes - for instance - better readable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Event Ids\n",
    "\n",
    "In the original dataset, the events which had been logged, are stored as numbers.  \n",
    "We convert these numbers to humanly readable event descriptions. \n",
    "\n",
    "|Event ID | Event name|\n",
    "|--------|--------|\n",
    "|10891+ | CWAP health |\n",
    "|10900+ | CWAP status |\n",
    "|30100 | AppCheck |\n",
    "|10811 | Weight on wheels |\n",
    "|10812 | Weight off wheels |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new dataframe from df and add column EventID\n",
    "df_new = df.copy()\n",
    "# map the eventID to readable event names\n",
    "eventId_map = {'eventId': {10891: 'CWAP health', 10892: 'CWAP health', 10893: 'CWAP health', 10894: 'CWAP health', \n",
    "                        10900: 'CWAP status', 10901: 'CWAP status', 10902: 'CWAP status', 10903: 'CWAP status',\n",
    "                        30100: 'AppCheck', 10811: 'Weight on wheels', 10812: 'Weight off wheels', 10957: 'Power on/off'}}\n",
    "# replace eventId with readable values from the map\n",
    "df_new.replace(eventId_map, inplace=True)\n",
    "# in Python, it's a good practice to typecase categorical features to a category type to fasten up the processing of the data\n",
    "df_new['eventId'] = df_new['eventId'].astype('category')\n",
    "# define the overall SLA dataframe in which the overall availability will be stored\n",
    "df_SLA = pd.DataFrame(columns=['date', 'server', 'tailsign', 'expected operation time', 'downtime', 'actual operation time', 'availability'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract grouped data\n",
    "\n",
    "Some logged data comes as a concatenated string and, for further processing and better interpretation, now needs to be split and stored\n",
    "in separate fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for eventId = 'AppCheck' and create a new column 'FlightNo' with the first element of the split column 'data.element0'\n",
    "df_new['FlightNo'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[0]\n",
    "# filter for eventId = 'CWAP status' and create a new column 'PaxConnected' with the value from 'data.element5'\n",
    "df_new['PaxConnected'] = df_new[df_new['eventId'] == 'CWAP status']['data.element5']\n",
    "# filter for eventId = 'AppCheck' and take the fourth element out of 'data.element0'; get the first integer out of it and add this to new column 'PortalFrontendHomepage' (1 = success)\n",
    "df_new['PortalFrontendHomepage'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[3].str.split(':').str[1].str.split(',').str[0].astype(int)\n",
    "# proceed with the other components that are checked by BCEL AppCheck\n",
    "df_new['PortalFrontendApp'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[4].str.split(':').str[1].str.split(',').str[0].astype(int)\n",
    "df_new['PortalRuntime'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[5].str.split(':').str[1].str.split(',').str[0].astype(int)\n",
    "df_new['AnalyticsReceiver'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[6].str.split(':').str[1].str.split(',').str[0].astype(int)\n",
    "df_new['MovingMap'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[7].str.split(':').str[1].str.split(',').str[0].astype(int)\n",
    "df_new['FlightAPI'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[8].str.split(':').str[1].str.split(',').str[0].astype(int)\n",
    "df_new['InflightContentServer'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[9].str.split(':').str[1].str.split(',').str[0].astype(int)\n",
    "df_new['DRMWidevine'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[10].str.split(':').str[1].str.split(',').str[0].astype(int)\n",
    "df_new['DRMFairplay'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[11].str.split(':').str[1].str.split(',').str[0].astype(int)\n",
    "\n",
    "# extract power on/off timestamps to an explicit column and convert to datetime\n",
    "df_new['PowerOn'] = df_new[df_new['eventId'] == 'Power on/off']['data.element0'].str.split(';').str[0]\n",
    "df_new['PowerOn'] = pd.to_datetime(df_new['PowerOn'])\n",
    "df_new['PowerOff'] = df_new[df_new['eventId'] == 'Power on/off']['data.element1'].str.split(';').str[0]\n",
    "df_new['PowerOff'] = pd.to_datetime(df_new['PowerOff'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration section\n",
    "\n",
    "In this section we define the **components** that the Juypter Notebook shall consider for report generation.\n",
    "\n",
    "**Hint**\n",
    "> The AppCheck currently considers 9 different components to be mandatorily checked by it.\n",
    "> These are:\n",
    "> - Portal Frontend Homepage\n",
    "> - Portal Frontend App\n",
    "> - Portal Runtime\n",
    "> - Analytics Receiver\n",
    "> - Moving Map\n",
    "> - Flight API\n",
    "> - Inflight Content Server\n",
    "> - DRM Widevine\n",
    "> - DRM Fairplay\n",
    "\n",
    "As per an agreement with the VA PDM from Dec, 2nd 2021, we consider the following components as relevant for measuring the BC IFE service availability:\n",
    "\n",
    "> - Portal Frontend Homepage\n",
    "> - Portal Frontend App\n",
    "> - Portal Runtime\n",
    "> - Flight API\n",
    "> - Inflight Content Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of components for the diagram; we leave out the non-SLA-relevant components and combine PortalFrontendHomepage and PortalFrontendApp \n",
    "#components = ['PortalFrontendHomepage', 'PortalFrontendApp', 'PortalRuntime', 'AnalyticsReceiver', 'MovingMap', 'FlightAPI', 'InflightContentServer', 'DRMWidevine', 'DRMFairplay']\n",
    "components = ['PortalFrontendHomepage', 'PortalFrontendApp', 'PortalRuntime', 'FlightAPI', 'InflightContentServer']\n",
    "# create a list of all servers in the report \n",
    "servers = list()\n",
    "for s in df_new['serialNumber'].unique().tolist():\n",
    "    servers.append(s)\n",
    "# if other servers (or a subset of servers) than contained in the CSV file shall be used, append those explicitly to the list\n",
    "#servers.append('7CTCA20586')\n",
    "\n",
    "month = '2021-11'\n",
    "period = pd.Period(month, freq='M')\n",
    "#df_SLA.date = pd.Series(pd.date_range(start=period.start_time, end=period.end_time, freq='D'))\n",
    "month_days = pd.Series(pd.date_range(start=period.start_time, end=period.end_time, freq='D'))\n",
    "# initialise the overall SLA dataframe\n",
    "for day in month_days:\n",
    "    # iterate over all servers\n",
    "    for s in servers:\n",
    "        # add a new row to df_SLA where df_SLA.date = day and df_SLA.server = s\n",
    "        df_SLA = df_SLA.append(pd.DataFrame({'date': [day], 'server': [s], 'expected operation time': [0], 'downtime': [0], 'actual operation time': [0], 'availability': [0]}, index=[0]))\n",
    "# create a duplicate of the column 'server', name it 'tailsign' and apply the tailsign_map to it\n",
    "df_SLA['tailsign'] = df_SLA['server'].copy()\n",
    "tailsign_map = {'tailsign': {'3442310010': 'VHYIO', '3467540006': 'VHYIR', '3352680006': 'VHYFJ', '3395600002': 'VHYIT', '3734240001': 'VHVUZ',\n",
    "                             '3413940001': 'VHYFE', '3472530005': 'VHYFF', '3413940014': '', '7CTCA20586': 'VHYUD', '3413940006': 'VHYIJ'}}\n",
    "df_SLA.replace(tailsign_map, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report visualisation\n",
    "\n",
    "## Succeeded and failed AppChecks (optional) \n",
    "\n",
    "The subsequent charts show the totals for the succeeded and failed AppChecks on all defined servers in the time period  \n",
    "that is contained in the CSV file.\n",
    "\n",
    "(For further visualisations, refer to this)[https://towardsdatascience.com/stacked-bar-charts-with-pythons-matplotlib-f4020e4eb4a7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a working copy of df_new, filtered by eventId = 'AppCheck', using only the columns: timestamp, serialNumber, eventId, PortalFrontendHomepage\n",
    "df_work_appcheck = df_new[df_new['eventId'] == 'AppCheck'][['timestamp', 'serialNumber', 'eventId', 'PortalFrontendHomepage', 'PortalFrontendApp', 'PortalRuntime', 'AnalyticsReceiver', 'MovingMap', 'FlightAPI', 'InflightContentServer', 'DRMWidevine', 'DRMFairplay']]\n",
    "# define the labels to be used in the charts\n",
    "labels = ['Portal Frontend', 'Portal Runtime', 'Flight API', 'Inflight Content Server']\n",
    "# i is just a counter to print the respective dataframe df_list[i]\n",
    "i = 0\n",
    "# start defining the stacked bar plot\n",
    "colors = ['#1D2F6F', '#8390FA', '#6EAF46', '#FAC748']\n",
    "width = 0.35\n",
    "#iterate over all servers and create a new dataframe for each server and component which contains the numbers of succeeded and failed component checks; store these new dataframes in df_list()\n",
    "for s in servers:\n",
    "    print('Iteration for server ' + str(s))\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax.set_ylabel('Number of AppChecks')\n",
    "    plt.xticks(\n",
    "        rotation=45,\n",
    "        horizontalalignment='right',\n",
    "        fontweight='light',\n",
    "        fontsize='medium',\n",
    "    )\n",
    "    # chart's title\n",
    "    ax.set_title('AppChecks for Server ' + str(s))\n",
    "    # suc is the series of successful appchecks for the current server and and the respective component\n",
    "    suc = pd.Series(dtype = 'object')\n",
    "    # fail is the series of failed appchecks for the current server and respective component\n",
    "    fail = pd.Series(dtype = 'object')\n",
    "    # create a list of dataframes\n",
    "    df_list = list()\n",
    "    # first, define a filter for all servers in list s\n",
    "    df_server_filter = (df_new['serialNumber'] == s)\n",
    "    # then, iterate over the labels (which are components of the appcheck) and create a dataframe per server and component\n",
    "    for component in components:\n",
    "        # reset the dataframe per server\n",
    "        df_component = ()\n",
    "        # define the components as categorical data using value 1, 0 (1 = success, 0 = failure)\n",
    "        df_work_appcheck[component] = pd.Categorical(df_work_appcheck[component], categories=[1, 0], ordered=True)\n",
    "        # create a new series for each component by grouping the filtered original dataframe by the component and count the number of occurences\n",
    "        df_component = df_work_appcheck.loc[df_server_filter, :].groupby([component])[component].count()\n",
    "        df_component = df_component.to_frame()\n",
    "        # append df_component.loc[1] to suc; suc now contains the number of successful appchecks for the current server and respective component\n",
    "        suc = suc.append(df_component.loc[1])\n",
    "        # append df_component.loc[0] to fail; fail now contains the number of failed appchecks for the current server and respective component\n",
    "        fail = fail.append(df_component.loc[0])\n",
    "        df_list.append(df_component)\n",
    "        i += 1\n",
    "\n",
    "    #we don't want to differentiate Portal Frontend Homepage and Portal Frontend App but consolidate them as Portal Frontend in the report\n",
    "    if suc.PortalFrontendApp < suc.PortalFrontendHomepage: \n",
    "        # drop suc.PortalFrontendHomepage\n",
    "        suc = suc.drop(suc.index[suc.index == 'PortalFrontendHomepage'])\n",
    "        fail = fail.drop(fail.index[fail.index == 'PortalFrontendHomepage'])\n",
    "    else:\n",
    "        # drop suc.PortalFrontendApp\n",
    "        suc = suc.drop(suc.index[suc.index == 'PortalFrontendApp'])\n",
    "        fail = fail.drop(fail.index[fail.index == 'PortalFrontendApp'])\n",
    "    # create a matplotlib ax.bar with labels and the data from the list of dataframes\n",
    "    if (suc.sum(axis=0) > 0 or fail.sum(axis=0) > 0): \n",
    "        ax.bar(labels, suc.to_list(), width, color=colors[3], label='Suceeded')\n",
    "        ax.bar(labels, fail.to_list(), width, bottom=suc.to_list(), color=colors[0], label='Failed')\n",
    "        for index,data in enumerate(suc.to_list()):\n",
    "            plt.text(x=index , y =data/2 , s=f\"{data}\" , fontdict=dict(fontsize=10))\n",
    "        for index,data in enumerate(fail.to_list()):\n",
    "            plt.text(x=index , y =suc.to_list()[index]+10 , s=f\"{data}\" , fontdict=dict(fontsize=10))\n",
    "        # remove spines\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['left'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "        # adjust limits and draw grid lines\n",
    "        plt.ylim(-0.5, ax.get_yticks()[-1] + 0.5)\n",
    "        ax.set_axisbelow(True)\n",
    "        ax.yaxis.grid(color='gray', linestyle='dashed')\n",
    "        # legend\n",
    "        plt.legend(['Success', 'Failed'], loc='upper left', ncol=4, frameon=False)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('No appchecks for server ' + str(s))\n",
    "        continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLA report - operational time expected vs achieved per day and server\n",
    "\n",
    "## Report definition\n",
    "We create a monthly report which includes per day and server the \n",
    "- Expected Operation Time\n",
    "- Sum of unplanned downtime\n",
    "- Passenger Service Availability Time\n",
    "- Passenger Service Availability Status % \n",
    "\n",
    "## Current assumptions\n",
    "### Components which to consider\n",
    "As earlier explained we only consider certain SW components to be relevant for the SLA\n",
    "```\n",
    "PortalFrontendHomepage, PortalFrontendApp (both in one consolidated status), PortalRuntime, Flight API, Inflight Content Server\n",
    "```\n",
    "From these components, we consider a failed AppCheck as a downtime of the Portal.  \n",
    "Several failed AppChecks in a row, sum up to a higher downtime.\n",
    "For instance: (AppCheck occurs every 10 minutes)\n",
    "> Portal Frontend on server <server> failed **2 times** in a row, \n",
    "> the calculated downtime is **20 minutes** for this server.  \n",
    "> Any other component failing in the same period of time won't add to the calculated downtime.  \n",
    "\n",
    "~~In addition, we consider the CWAP status and health in a way that an error on the CWAPs also leads \n",
    "to a decreased Passenger Service Availability time.~~\n",
    "\n",
    "> ~~A CWAP that shows an error in a single check is considered to cause a downtime 10 minutes on 25% of the passengers.~~  \n",
    "> A CWAP that shows an error for one of the radios is not considered to cause a downtime since since rooming takes place  \n",
    "> and passengers are connected to the next CWAP\n",
    "\n",
    "### Expected Operational Time\n",
    "- We consider every period of time between a 'power on' and 'power off' event that happens on-ground to add to the overall Expected Operational Time of the server. Power cycles that happen in-flight decrease the time (pls. refer to 'Additional considerations')\n",
    "\n",
    "### Additional considerations\n",
    "- Inflight (after Weight off wheels, before Weight on wheels), we consider a sequence of Power on/Power off events as a downtime of the service\n",
    "- Inflight, we consider the lack of AppChecks as a service downtime (remember: we conduct AppChecks every 10 minutes; if no AppCheck occur on a 1h flight, we consider this as a downtime of 1 hour)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new working df and only copy the columns we need\n",
    "#df_work_events = df_new[['timestamp', 'serialNumber', 'eventId', 'data.element0', 'data.element1', 'data.element2', 'data.element3', 'data.element4', 'data.element5', 'data.element6', 'FlightNo']]\n",
    "df_work_events = df_new.copy()\n",
    "# hey, let's make column 'timestamp' really a timestamp and use it as index\n",
    "df_work_events['timestamp'] = pd.to_datetime(df_work_events['timestamp'], format='%b %d, %Y @ %H:%M:%S.%f')\n",
    "#df_work_events.set_index('timestamp', inplace=True)\n",
    "\n",
    "# filter_power_on is furthermore used to filter all Power on/off events as well as Weight on/off wheels to be able to show operation times between those events\n",
    "# if operation time should be aggregated on a higher level, Weight on/off wheels events are not relevant and can be left ou\n",
    "#filter_power_on = (df_work_events['eventId'] == 'Power on/off')\n",
    "filter_power_on = (df_work_events['eventId'] == 'Power on/off') | (df_work_events['eventId'] == 'Weight off wheels') | (df_work_events['eventId'] == 'Weight on wheels')\n",
    "filter_wow = (df_work_events['eventId'] == 'Weight off wheels') | (df_work_events['eventId'] == 'Weight on wheels')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the expected operational time per server and date\n",
    "\n",
    "Assumption: The expected operational time is the sum of time differences between a server's first log (using timestamp for it) and it's shutdown time (available in data.element1 if eventId == 'Power On/Off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check on power on/off events that happen inflight and on-ground\n",
    "\n",
    "Inflight := the time between a ```Weight off wheels``` and a ```Weight on Wheels``` event.  \n",
    "All the time between a ```Weight off wheels``` and a ```Weight on Wheels``` event is considered to be Expected Operation Time.\n",
    "A ```Power on/off```event that happens inflight decreases the overall operation time.\n",
    "\n",
    "Even though that the SLA is defined on-ground as well as in-flight, we don't consider on-groud server re-boots as downtimes \n",
    "they could be caused by several reasons: LAME, Aircraft power change and others.\n",
    "\n",
    "We calculate the downtime that happened inflight per server and day in the following procedure.  \n",
    "Moreover, we calculate the time per server and day between each 'power on' and 'power off' event that happened on-ground.  \n",
    "All these time differences, we sum up to be the Expected Operational Time per server and day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "#let's restrict the list of servers to a single server (just for experimenting)\n",
    "#servers = list()\n",
    "#servers.append('3352680006')\n",
    "#iterate over all servers in list servers\n",
    "for s in servers:\n",
    "    #some basic definitions and local variable declarations\n",
    "    #from df_work_events, filter all logs for the currently processed server\n",
    "    filter_server = (df_work_events['serialNumber'] == s)\n",
    "    #create a new dataframe for the current server, with columns: timestamp, eventId, serialNumber, Power On, Power Off\n",
    "    col = ['timestamp', 'eventId', 'serialNumber', 'PowerOn', 'PowerOff']\n",
    "    df_server = df_work_events.loc[filter_server, :]\n",
    "    #drop all columns except the ones we need (timestamp, eventId, serialNumber, Power On, Power Off)\n",
    "    df_server = df_server[col]\n",
    "    #make sure the timestamp is really a timestamp; PowerOn and PowerOff are already timestamps\n",
    "    df_server['timestamp'] = pd.to_datetime(df_server['timestamp'])\n",
    "    # add the columns 'downtime' and 'opstime' to the dataframe df_server, both of type datetime with value NaN\n",
    "    df_server['downtime'] = pd.to_datetime(np.nan)\n",
    "    df_server['opstime'] = pd.to_datetime(np.nan)\n",
    "    # we now iterate over the WoW events and identify the Power on/off events that happened inflight/on the ground\n",
    "    # the Power on/off events that happened inflight are considered as downtime\n",
    "    # the Power on/off events that happened on the ground are considered as operation time\n",
    "    filter_wow = (df_server['eventId'] == 'Weight off wheels') | (df_server['eventId'] == 'Weight on wheels')\n",
    "    wow_timestamps = pd.Series(index=None, dtype='object')\n",
    "    wow_timestamps = df_server[filter_wow]['timestamp']\n",
    "    # - [ ] need to decide whether I keep the following line of code or remove it\n",
    "    df_server.set_index('timestamp', inplace=True)\n",
    "    #iterate over the pairs of WoW timestamps; so, in each case we should get two WoW events with other events between them\n",
    "    for (index, ts) in enumerate(wow_timestamps):\n",
    "        # if the current WoW event is the last one, we don't have a pair of WoW events to compare to\n",
    "        if index < len(wow_timestamps) - 1:\n",
    "            #let's find out whether we are inflight or on the-ground; the log data is sorted by timestamp descending\n",
    "            wow_eventId = df_server.loc[ts, 'eventId']\n",
    "            # get the current timestamp (WoW event) and the next timestamp (WoW event) to then filter all events in this WoW cycle\n",
    "            current_ts = ts # still a timestamp\n",
    "            next_ts = wow_timestamps.to_list()[index + 1] # still a timestamp\n",
    "            #we consider all the time between the current and the next WoW event as opstime; only if we are on ground, we will reduce the opstime by the times the server had been shut down\n",
    "            opstime = (current_ts - next_ts).total_seconds()\n",
    "            # slice the data and extract all log rows between WoW current_ts and WoW next_ts\n",
    "            df_server_wow_cycle = df_server.loc[current_ts:next_ts, :]\n",
    "            # add the columns 'downtime' and 'opstime' to the dataframe df_server, both of type datetime\n",
    "            #- [ ] need to decide whether I keep the following line of code or remove it\n",
    "            #df_server_wow_cycle['downtime'] = pd.to_datetime(np.nan)\n",
    "            #df_server_wow_cycle['opstime'] = pd.to_datetime(np.nan)\n",
    "            # filter all 'Power on/off' events from dataframe\n",
    "            filter_wow_cycle_power = (df_server_wow_cycle['eventId'] == 'Power on/off')\n",
    "            #1st use case: Weight on Wheels: the past period was inflight. We consider all server reboot events as downtime\n",
    "            if wow_eventId == 'Weight on wheels':\n",
    "                # for all Power on/off events found, we calculate the downtime as the difference between the event's PowerOn- (in column 'PowerOn') and the PowerOff-timestamp (in column 'PowerOff')\n",
    "                # we store the downtime in the column 'downtime' off the respective Power on/off event's row\n",
    "                #Have we found some inflight Power cycles?\n",
    "                if len(df_server_wow_cycle[filter_wow_cycle_power]) > 0:\n",
    "                    #If yes...\n",
    "                    #print('Server ' + str(s) + ' has ' + str(len(df_server_wow_cycle.loc[filter_wow_cycle_power, :])) + ' power on/off events in-flight at ' + str(current_ts))\n",
    "                    for index, row in df_server_wow_cycle[filter_wow_cycle_power].iterrows():\n",
    "                        # get the timestamp of the current Power on/off event\n",
    "                        power_on_ts = row['PowerOn']\n",
    "                        # get the timestamp of the last Power on/off event\n",
    "                        power_off_ts = row['PowerOff']\n",
    "                        # calculate the downtime as the difference between the current timestamp and the last timestamp in seconds\n",
    "                        downtime = (power_on_ts - power_off_ts).total_seconds()\n",
    "                        # store the downtime in the respective Power on/off event in the server's dataframe\n",
    "                        df_server.loc[index, 'downtime'] = downtime\n",
    "                        #print('Have added ' + str(df_server.loc[ts, 'downtime']) + ' seconds to the downtime of ' + str(s))\n",
    "                #being inflight, we add to the Expected Operation Time the full time span between the two WoW events\n",
    "                df_server.loc[current_ts, 'opstime'] = opstime\n",
    "                #print('Have added ' + str(opstime) + ' seconds to the total opstime of ' + str(s))\n",
    "            #2nd use case: Weight off Wheels: the past period was on the ground. We consider all server reboot events as operation time\n",
    "            elif wow_eventId == 'Weight off wheels':\n",
    "                #Have we found some on-ground Power cycles?\n",
    "                if len(df_server_wow_cycle[filter_wow_cycle_power]) > 0:\n",
    "                    #print('Server ' + str(s) + ' has ' + str(len(df_server_wow_cycle.loc[filter_wow_cycle_power, :])) + ' power on/off events on-ground at ' + str(current_ts))\n",
    "                    #If yes...we will decrease the opstime by the time the server has been shut down. On ground, we don't consider these power cycles as unplanned downtime\n",
    "                    #so they don't do anything to the downtime\n",
    "                    for index, row in df_server_wow_cycle[filter_wow_cycle_power].iterrows():\n",
    "                        # get the timestamp of the current Power on/off event\n",
    "                        power_on_ts = row['PowerOn']\n",
    "                        # get the timestamp of the last Power on/off event\n",
    "                        power_off_ts = row['PowerOff']\n",
    "                        # calculate the operation time as the difference between the current timestamp and the last timestamp in seconds\n",
    "                        opstime = opstime - (power_on_ts - power_off_ts).total_seconds()\n",
    "                df_server.loc[current_ts, 'opstime'] = opstime\n",
    "                #print('Have added ' + str(opstime) + ' seconds to the total opstime of ' + str(s))\n",
    "    # we now have the downtime and the operation time for the server\n",
    "    # let's now group the downtime by date and sum up all the values in the column 'sum'\n",
    "    df_server_grouped_downtime = df_server.groupby(df_server.index.date)[['downtime']].sum()\n",
    "    for (index, row) in df_server_grouped_downtime.iterrows():\n",
    "        #check if row[0] is NaN\n",
    "        if not np.isnan(row[0]):\n",
    "            df_SLA.loc[(df_SLA.server == s) & (df_SLA.date.dt.date == index), 'downtime'] = row['downtime']\n",
    "        else:\n",
    "            df_SLA.loc[(df_SLA.server == s) & (df_SLA.date.dt.date == index), 'downtime'] = 0\n",
    "    df_server_grouped_opstime = df_server.groupby(df_server.index.date)[['opstime']].sum()\n",
    "    for (index, row) in df_server_grouped_opstime.iterrows():\n",
    "        #check if row[0] is NaN\n",
    "        if not np.isnan(row[0]):\n",
    "            df_SLA.loc[((df_SLA.server == s) & (df_SLA.date.dt.date == index)), 'expected operation time'] = row['opstime']\n",
    "        else:\n",
    "            df_SLA.loc[((df_SLA.server == s) & (df_SLA.date.dt.date == index)), 'expected operation time'] = 0\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of the aggregated AppCheck\n",
    "\n",
    "In the subsequent analysis, we check how many appchecks succeeded or failed between two Power on/off events for the defined servers.  \n",
    "\n",
    "We can easily increase transparency for this analysis and consider 'Weight on/off wheels' as well  \n",
    "(by changing the filter *filter_power_on*) and thus, come to a view where we can differentiate  \n",
    "in-flight and on-ground behaviour.\n",
    "\n",
    "The result of this report currently contains:  \n",
    "- The event (Power on/off, WoW) which occured (event) and \n",
    "- when server has been powered on and off (poweron, poweroff),\n",
    "- whether the CWAPs have been successfully activated (status: *Activated* in column cwap) in the period between the previous and the current event\n",
    "- how many clients have been connected (client connected) in this period\n",
    "- the number of appchecks having been conducted (appcheck) in this period\n",
    "- the calculated availability (availability) in terms of succeeded or failed appchecks in this period\n",
    "- the ops time (opstime) of the server as the difference between the latest power off event and the last power on event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "# iterate over all servers in list servers\n",
    "for s in servers:\n",
    "    #some basic definitions and local variable declarations\n",
    "    downtime = 0\n",
    "    #from df_work_events, filter all logs for the currently processed server\n",
    "    filter_server = (df_work_events['serialNumber'] == s)\n",
    "    #create a new dataframe for the appcheck analysis of the current server, with columns: timestamp, eventId, serialNumber, Power On, Power Off\n",
    "    col = ['timestamp', 'eventId', 'data.element4', 'data.element5', 'serialNumber', 'PaxConnected'] + components\n",
    "    df_appcheck = df_work_events.loc[filter_server, :]\n",
    "    #drop all columns except the ones we need (timestamp, eventId, serialNumber, Power On, Power Off)\n",
    "    df_appcheck = df_appcheck[col]\n",
    "    #make sure the timestamp is really a timestamp;\n",
    "    df_appcheck['timestamp'] = pd.to_datetime(df_appcheck['timestamp'])\n",
    "    # add the column 'downtime' to the dataframe (of type datetime with value NaN)\n",
    "    df_appcheck['downtime'] = pd.to_datetime(np.nan)\n",
    "    #filter df_appcheck for all entries with eventId = 'AppCheck'; these are the events we are interested in\n",
    "    filter_appcheck = (df_appcheck['eventId'] == 'AppCheck')\n",
    "    #iterate over all appcheck events\n",
    "    #I want to have all WoW events for the current server in a single dataframe\n",
    "    filter_appcheck_wow = ((df_appcheck['eventId'] == 'Weight off wheels') | (df_appcheck['eventId'] == 'Weight on wheels'))\n",
    "    df_appcheck_wow = df_appcheck.loc[filter_appcheck_wow, :]\n",
    "    #create a view/filter on appcheck_timestamps to iterate over all timestamps\n",
    "    appcheck_timestamps = pd.Series(index=None, dtype='object')\n",
    "    appcheck_timestamps = df_appcheck[filter_appcheck]['timestamp']\n",
    "    for index, ts in enumerate(appcheck_timestamps):\n",
    "        if index < len(appcheck_timestamps) - 1:\n",
    "            # get the timestamp of the current appcheck event\n",
    "            current_ts = ts\n",
    "            # get the timestamp of the next appcheck event to calculate the max downtime\n",
    "            next_ts = appcheck_timestamps.to_list()[index + 1]\n",
    "            #look back in history and find the previous WoW event; depending on the WoW event, we will add to the downtime or not\n",
    "            #if there is no further WoW event or no WoW event at all, we take the last one and derive from it in which flight phase we are\n",
    "            #check whether df_app_wow_cycle has any entries\n",
    "            if (len(df_appcheck_wow) > 0):# and (index < (len(df_appcheck_wow) - 1)):\n",
    "                #define variable wow_event and store the value of the previous WoW event \n",
    "                #from df_appcheck, get the first element for which timestamp < current_ts\n",
    "                #catch an IndexError if there is no previous WoW event\n",
    "                try:\n",
    "                    wow_event = df_appcheck_wow.loc[df_appcheck_wow['timestamp'] < current_ts, :].iloc[0, :]['eventId']\n",
    "                    #- [ ] can I remove wow_timestamp since it seems that I don't need it?\n",
    "                    #define variable wow_timestamp and store the timestamp of the previous WoW event\n",
    "                    wow_timestamp = df_appcheck_wow.loc[df_appcheck_wow['timestamp'] < current_ts, :].iloc[0, :]['timestamp']\n",
    "                except IndexError:\n",
    "                    if wow_event == 'Weight on wheels':\n",
    "                        wow_event = 'Weight off wheels'\n",
    "                    else:\n",
    "                        wow_event = 'Weight on wheels'\n",
    "                #if wow_event is Weight off wheels (we are inflight!), we have to calculate the downtime as the difference between the current timestamp and the last timestamp in seconds\n",
    "                if wow_event == 'Weight off wheels':\n",
    "                    #have we had a Portal downtime? (i.e. one of the components' appchecks has been 0 and thus the sum of the appchecks is smaller than the sum of the components)\n",
    "                    appcheck_succeeded = df_appcheck.loc[df_appcheck['timestamp'] == current_ts, components].sum(axis=1)\n",
    "                    if (not appcheck_succeeded.empty) and (appcheck_succeeded.iloc[0] < len(components)):\n",
    "                        #calculate the downtime as the difference between the current timestamp and the last timestamp in seconds\n",
    "                        #print column types of dataframe df_appcheck\n",
    "                        downtime = (current_ts - next_ts).total_seconds()\n",
    "                        #store the downtime in the respective appcheck event in the server's dataframe\n",
    "                        df_appcheck.loc[df_appcheck.timestamp == ts, 'downtime'] = downtime\n",
    "                else: # if wow_event == 'Weight on wheels':\n",
    "                    # do nothing since we have been on ground\n",
    "                    downtime = 0\n",
    "    # - [ ] aggregate the downtime for each day\n",
    "    # we now have the downtime and the operation time for the server\n",
    "    # let's now group the downtime by date and sum up all the values in the column 'sum'\n",
    "    df_appcheck_grouped_downtime = df_appcheck.groupby(df_appcheck.timestamp.dt.date)[['downtime']].sum()\n",
    "    #iterate over alls rows in df_appcheck_grouped_downtime, where the values of column 'downtime' are not NaN and not 0\n",
    "    for (index, row) in df_appcheck_grouped_downtime.iterrows():\n",
    "        #check if row[0] is NaN\n",
    "        if (not np.isnan(row[0]) and (row[0] > 0)):\n",
    "            #store the scalar value of df_SLA.downtime, where df_SLA.server == s and df_SLA.date.dt.date  == index in variable srv_downtime\n",
    "            srv_downtime = df_SLA.loc[((df_SLA.server == s) & (df_SLA.date.dt.date == index)), 'downtime']\n",
    "            downtime = srv_downtime[0] + row['downtime']\n",
    "            df_SLA.loc[(df_SLA.server == s) & (df_SLA.date.dt.date == index), 'downtime'] = downtime\n",
    "        #else:\n",
    "        #    df_SLA.loc[(df_SLA.server == s) & (df_SLA.date.dt.date == index), 'downtime'] = 0\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some figures and charts about connected passengers (optional)\n",
    "In the subsequent section we show the number of passengers per aircraft and date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in servers:\n",
    "    # create a new dataframe df_cwap from df_work_events, where the eventId is 'AppCheck'\n",
    "    df_cwap = df_work_events.loc[((df_work_events.eventId == 'CWAP status') & (df_work_events.serialNumber == s)), :]\n",
    "    #drop all columns except the ones we need (timestamp, eventId, serialNumber, Power On, Power Off)\n",
    "    df_cwap = df_cwap[['timestamp', 'eventId', 'serialNumber', 'data.element5', 'data.element6']]\n",
    "    #make columns data.element5 and data.element6 of type int and if a ValueError occurs, replace the value with NaN\n",
    "    df_cwap[['data.element5', 'data.element6']] = df_cwap[['data.element5', 'data.element6']].apply(pd.to_numeric, errors='coerce')\n",
    "    #now, group df_cwap by serialNumber and timestamp (dt.date) and sum up the values in the column 'data.element5' and 'data.element6'\n",
    "    try:\n",
    "        df_cwap_grouped = df_cwap.groupby(df_cwap.timestamp.dt.date)[['data.element5', 'data.element6']].mean ()\n",
    "        df_cwap_grouped.connectedpax = df_cwap_grouped['data.element5'] + df_cwap_grouped['data.element6']\n",
    "        #plot a chart with df_cwap_grouped.connectedpax as data over time\n",
    "        df_cwap_grouped.connectedpax.plot(figsize=(20,10))\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print('Error for server ' + s)\n",
    "        print(e)\n",
    "        pass\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final calculations of actual operational time and availability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'grouped'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/yt/g3pjgfp12l913d500hlnzffc0000gn/T/ipykernel_39207/2526032118.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mdf_SLA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'availability'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_SLA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'availability'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{:,.2f}%'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mdf_SLA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrouped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'actual operation time'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_SLA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrouped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'actual operation time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m's'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%H:%M:%S'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mdf_SLA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrouped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'expected operation time'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_SLA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrouped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'expected operation time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m's'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%H:%M:%S'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mdf_SLA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrouped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'downtime'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_SLA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrouped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'downtime'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m's'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%H:%M:%S'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5460\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5461\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5462\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5464\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'grouped'"
     ]
    }
   ],
   "source": [
    "# fill in the actual operation time which is the 'expected' operation time minus the downtime\n",
    "# if you re-run solely this section, you will receive an error, since the operation performed will fail after column has been converted to datetime some lines below\n",
    "# fill in availability, which is the 'expected operation time' minus the downtime divided by the 'expected operation time' and take care about divisions by zero\n",
    "try:\n",
    "    eot_filter = df_SLA['expected operation time'] != 0\n",
    "    df_SLA.loc[eot_filter, 'actual operation time'] = (df_SLA.loc[eot_filter , 'expected operation time'] - df_SLA.loc[eot_filter, 'downtime'])\n",
    "    df_SLA.loc[eot_filter, 'availability'] = ((df_SLA.loc[eot_filter , 'expected operation time'] - df_SLA.loc[eot_filter, 'downtime']) / df_SLA.loc[eot_filter, 'expected operation time']) * 100\n",
    "except:\n",
    "    df_SLA['availability'] = 0\n",
    "#the figures in df_SLA are per date and tailSign/server; thus, let's group them by tailSign/server and date\n",
    "#define a new dataframe from df_SLA, grouped by server and date, where the columns 'actual operation time' and 'expected operation time' and 'downtime' are summed up and for column 'availability' the mean is calculated\n",
    "df_SLA_grouped = df_SLA.groupby(['date'])[['actual operation time', 'expected operation time', 'downtime']].sum()\n",
    "df_SLA_grouped['availability'] = df_SLA_grouped['actual operation time'] / df_SLA_grouped['expected operation time'] * 100\n",
    "\n",
    "# convert the columns 'expected operation time', 'downtime' and 'acutal operation time' which currently contain a number in seconds, to hours and format it HH:MM:SS\n",
    "df_SLA['actual operation time'] = (pd.to_datetime(df_SLA['actual operation time'], unit='s')).dt.strftime('%H:%M:%S')\n",
    "df_SLA['expected operation time'] = (pd.to_datetime(df_SLA['expected operation time'], unit='s')).dt.strftime('%H:%M:%S')\n",
    "df_SLA['downtime'] = (pd.to_datetime(df_SLA['downtime'], unit='s')).dt.strftime('%H:%M:%S')\n",
    "#format column 'availability' as a percentage number\n",
    "df_SLA['availability'] = df_SLA['availability'].map('{:,.2f}%'.format)\n",
    "\n",
    "df_SLA_grouped['actual operation time'] = (pd.to_datetime(df_SLA_grouped['actual operation time'], unit='s')).dt.strftime('%H:%M:%S')\n",
    "df_SLA_grouped['expected operation time'] = (pd.to_datetime(df_SLA_grouped['expected operation time'], unit='s')).dt.strftime('%H:%M:%S')\n",
    "df_SLA_grouped['downtime'] = (pd.to_datetime(df_SLA_grouped['downtime'], unit='s')).dt.strftime('%H:%M:%S')\n",
    "#format column 'availability' as a percentage number\n",
    "df_SLA_grouped['availability'] = df_SLA_grouped['availability'].map('{:,.2f}%'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the dataframe df_SLA to a csv file\n",
    "df_SLA.to_csv('sla_' + 'VAPaxAvail' '.csv')\n",
    "df_SLA_grouped.to_csv('sla_grouped_' + 'VAPaxAvail' + '.csv')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1e39f5ddb06da47c792e6724e01a6f520ddcc160c593e0336e4f03a3f0c97e2f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('jupyterlab-debugger': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
