{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VA Passenger Availability SLA Report\n",
    "\n",
    "Once the upgrade to Linux has been completed it is measured using an automated script on the server measuring the availability of the Passenger Service by checking & logging the availability of the SSID (WLAN) and the software health after the booting phase. The System Uptime and hence the Expected Operation Time starts after the booting phase of the software initiated by power on. Any time the Passenger Service then is not available is considered an Unplanned Downtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open tasks\n",
    "- [ ] add/improve description (Why? What? How?)\n",
    "- [x] make consideration of DRM Widevine/Fairplay, Moving Map, Analytics Receiver optional and configurable in the report\n",
    "- [ ] set first appcheck (timestamp) as the start of operation time and power off as the end\n",
    "- [ ] consider CWAP unavailabilities by counting Errors as unavailability\n",
    "- [ ] sum up the 10 minute periods of a failed app check to unavailability time\n",
    "- [ ] structure the code and, for instance, use functions\n",
    "- [ ] implement Kibana API interface\n",
    "- [ ] come up with a CI/CD workflow for this/publish the Jupyter report\n",
    "- [x] define the overall SLA dataframe\n",
    "- [ ] calculate the Expected Operation Time and store it in the SLA dataframe (per day and server)\n",
    "- [ ] add the server in-flight reboots as unexpected downtime to the SLA dataframe (per day and server)\n",
    "- [x] clean up the notebook and remove unsused, irrelevant variables\n",
    "- [ ] introduce server/tailsign mapping\n",
    "- [ ] get the month, which I use to initialise df_SLA, automatically out of the imported data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the log data\n",
    "\n",
    "## Source of information\n",
    "The data, that the Juypter Notebook processes and bases the report on, comes from **BoardConnect Kibana Prod**.\n",
    "\n",
    "\n",
    "Since the Notebook currently does not have a direct interface to Kibana implemented, we need to request the data\n",
    "in Kibana and export it as a **CSV** file.\n",
    "\n",
    "In Kibana, one fine a stored request which is named **VAPaxAvailSLAv2** and which should be executed using the\n",
    "relevant timeframe that should be considered in the report (for instance: November 2021).\n",
    "\n",
    "In a first step, the CSV file needs to be provided to Juypter Notebook, so that it can be read and processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42774, 12)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read csv (VAPaxAvailSLAv2) into dataframe\n",
    "df = pd.read_csv('/Users/u293733/git.jupyter-notebooks/VAPaxAvailSLAv2.csv')\n",
    "# outlign the shape \n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data adjustments\n",
    "\n",
    "We keep the original data in the dataframe df and create a copy of it for further processing.  \n",
    "In this copy we adjust the data so that it becomes - for instance - better readable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Event Ids\n",
    "\n",
    "In the original dataset, the events which had been logged, are stored as numbers.  \n",
    "We convert these numbers to humanly readable event descriptions. \n",
    "\n",
    "|Event ID | Event name|\n",
    "|--------|--------|\n",
    "|10891+ | CWAP health |\n",
    "|10900+ | CWAP status |\n",
    "|30100 | AppCheck |\n",
    "|10811 | Weight on wheels |\n",
    "|10812 | Weight off wheels |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new dataframe from df and add column EventID\n",
    "df_new = df.copy()\n",
    "# map the eventID to readable event names\n",
    "eventId_map = {'eventId': {10891: 'CWAP health', 10892: 'CWAP health', 10893: 'CWAP health', 10894: 'CWAP health', \n",
    "                        10900: 'CWAP status', 10901: 'CWAP status', 10902: 'CWAP status', 10903: 'CWAP status',\n",
    "                        30100: 'AppCheck', 10811: 'Weight on wheels', 10812: 'Weight off wheels', 10957: 'Power on/off'}}\n",
    "# replace eventId with readable values from the map\n",
    "df_new.replace(eventId_map, inplace=True)\n",
    "# in Python, it's a good practice to typecase categorical features to a category type to fasten up the processing of the data\n",
    "df_new['eventId'] = df_new['eventId'].astype('category')\n",
    "\n",
    "# define the overall SLA dataframe in which the overall availability will be stored\n",
    "df_SLA = pd.DataFrame(columns=['date', 'server', 'expected operation time', 'downtime', 'actual operation time', 'availability'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract grouped data\n",
    "\n",
    "Some logged data comes as a concatenated string and, for further processing and better interpretation, now needs to be split and stored\n",
    "in separate fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for eventId = 'AppCheck' and create a new column 'FlightNo' with the first element of the split column 'data.element0'\n",
    "df_new['FlightNo'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[0]\n",
    "# filter for eventId = 'CWAP status' and create a new column 'PaxConnected' with the value from 'data.element5'\n",
    "df_new['PaxConnected'] = df_new[df_new['eventId'] == 'CWAP status']['data.element5']\n",
    "# filter for eventId = 'AppCheck' and take the fourth element out of 'data.element0'; get the first integer out of it and add this to new column 'PortalFrontendHomepage' (1 = success)\n",
    "df_new['PortalFrontendHomepage'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[3].str.split(':').str[1].str.split(',').str[0].astype(int)\n",
    "# proceed with the other components that are checked by BCEL AppCheck\n",
    "df_new['PortalFrontendApp'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[4].str.split(':').str[1].str.split(',').str[0].astype(int)\n",
    "df_new['PortalRuntime'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[5].str.split(':').str[1].str.split(',').str[0].astype(int)\n",
    "df_new['AnalyticsReceiver'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[6].str.split(':').str[1].str.split(',').str[0].astype(int)\n",
    "df_new['MovingMap'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[7].str.split(':').str[1].str.split(',').str[0].astype(int)\n",
    "df_new['FlightAPI'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[8].str.split(':').str[1].str.split(',').str[0].astype(int)\n",
    "df_new['InflightContentServer'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[9].str.split(':').str[1].str.split(',').str[0].astype(int)\n",
    "df_new['DRMWidevine'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[10].str.split(':').str[1].str.split(',').str[0].astype(int)\n",
    "df_new['DRMFairplay'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[11].str.split(':').str[1].str.split(',').str[0].astype(int)\n",
    "\n",
    "# extract power on/off timestamps to an explicit column and convert to datetime\n",
    "df_new['PowerOn'] = df_new[df_new['eventId'] == 'Power on/off']['data.element0'].str.split(';').str[0]\n",
    "df_new['PowerOn'] = pd.to_datetime(df_new['PowerOn'])\n",
    "df_new['PowerOff'] = df_new[df_new['eventId'] == 'Power on/off']['data.element1'].str.split(';').str[0]\n",
    "df_new['PowerOff'] = pd.to_datetime(df_new['PowerOff'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration section\n",
    "\n",
    "In this section we define the **components** that the Juypter Notebook shall consider for report generation.\n",
    "\n",
    "**Hint**\n",
    "> The AppCheck currently considers 9 different components to be mandatorily checked by it.\n",
    "> These are:\n",
    "> - Portal Frontend Homepage\n",
    "> - Portal Frontend App\n",
    "> - Portal Runtime\n",
    "> - Analytics Receiver\n",
    "> - Moving Map\n",
    "> - Flight API\n",
    "> - Inflight Content Server\n",
    "> - DRM Widevine\n",
    "> - DRM Fairplay\n",
    "\n",
    "As per an agreement with the VA PDM from Dec, 2nd 2021, we consider the following components as relevant for measuring the BC IFE service availability:\n",
    "\n",
    "> - Portal Frontend Homepage\n",
    "> - Portal Frontend App\n",
    "> - Portal Runtime\n",
    "> - Flight API\n",
    "> - Inflight Content Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of components for the diagram; we leave out the non-SLA-relevant components and combine PortalFrontendHomepage and PortalFrontendApp \n",
    "#components = ['PortalFrontendHomepage', 'PortalFrontendApp', 'PortalRuntime', 'AnalyticsReceiver', 'MovingMap', 'FlightAPI', 'InflightContentServer', 'DRMWidevine', 'DRMFairplay']\n",
    "components = ['PortalFrontendHomepage', 'PortalFrontendApp', 'PortalRuntime', 'FlightAPI', 'InflightContentServer']\n",
    "# create a list of all servers in the report \n",
    "servers = list()\n",
    "for s in df_new['serialNumber'].unique().tolist():\n",
    "    servers.append(s)\n",
    "# if other servers (or a subset of servers) than contained in the CSV file shall be used, append those explicitly to the list\n",
    "#servers.append('7CTCA20586')\n",
    "\n",
    "month = '2021-11'\n",
    "period = pd.Period(month, freq='M')\n",
    "#df_SLA.date = pd.Series(pd.date_range(start=period.start_time, end=period.end_time, freq='D'))\n",
    "month_days = pd.Series(pd.date_range(start=period.start_time, end=period.end_time, freq='D'))\n",
    "# initialise the overall SLA dataframe\n",
    "for day in month_days:\n",
    "    # iterate over all servers\n",
    "    for s in servers:\n",
    "        # add a new row to df_SLA where df_SLA.date = day and df_SLA.server = s\n",
    "        df_SLA = df_SLA.append(pd.DataFrame({'date': [day], 'server': [s], 'expected operation time': ['00:00:00'], 'downtime': ['00:00:00'], 'actual operation time': ['00:00:00'], 'availability': ['0.0']}, index=[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report visualisation\n",
    "\n",
    "## Succeeded and failed AppChecks (optional) \n",
    "\n",
    "The subsequent charts show the totals for the succeeded and failed AppChecks on all defined servers in the time period  \n",
    "that is contained in the CSV file.\n",
    "\n",
    "(For further visualisations, refer to this)[https://towardsdatascience.com/stacked-bar-charts-with-pythons-matplotlib-f4020e4eb4a7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a working copy of df_new, filtered by eventId = 'AppCheck', using only the columns: timestamp, serialNumber, eventId, PortalFrontendHomepage\n",
    "df_work_appcheck = df_new[df_new['eventId'] == 'AppCheck'][['timestamp', 'serialNumber', 'eventId', 'PortalFrontendHomepage', 'PortalFrontendApp', 'PortalRuntime', 'AnalyticsReceiver', 'MovingMap', 'FlightAPI', 'InflightContentServer', 'DRMWidevine', 'DRMFairplay']]\n",
    "# define the labels to be used in the charts\n",
    "labels = ['Portal Frontend', 'Portal Runtime', 'Flight API', 'Inflight Content Server']\n",
    "# i is just a counter to print the respective dataframe df_list[i]\n",
    "i = 0\n",
    "# start defining the stacked bar plot\n",
    "colors = ['#1D2F6F', '#8390FA', '#6EAF46', '#FAC748']\n",
    "width = 0.35\n",
    "#iterate over all servers and create a new dataframe for each server and component which contains the numbers of succeeded and failed component checks; store these new dataframes in df_list()\n",
    "for s in servers:\n",
    "    print('Iteration for server ' + str(s))\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax.set_ylabel('Number of AppChecks')\n",
    "    plt.xticks(\n",
    "        rotation=45,\n",
    "        horizontalalignment='right',\n",
    "        fontweight='light',\n",
    "        fontsize='medium',\n",
    "    )\n",
    "    # chart's title\n",
    "    ax.set_title('AppChecks for Server ' + str(s))\n",
    "    # suc is the series of successful appchecks for the current server and and the respective component\n",
    "    suc = pd.Series(dtype = 'object')\n",
    "    # fail is the series of failed appchecks for the current server and respective component\n",
    "    fail = pd.Series(dtype = 'object')\n",
    "    # create a list of dataframes\n",
    "    df_list = list()\n",
    "    # first, define a filter for all servers in list s\n",
    "    df_server_filter = (df_new['serialNumber'] == s)\n",
    "    # then, iterate over the labels (which are components of the appcheck) and create a dataframe per server and component\n",
    "    for component in components:\n",
    "        # reset the dataframe per server\n",
    "        df_component = ()\n",
    "        # define the components as categorical data using value 1, 0 (1 = success, 0 = failure)\n",
    "        df_work_appcheck[component] = pd.Categorical(df_work_appcheck[component], categories=[1, 0], ordered=True)\n",
    "        # create a new series for each component by grouping the filtered original dataframe by the component and count the number of occurences\n",
    "        df_component = df_work_appcheck.loc[df_server_filter, :].groupby([component])[component].count()\n",
    "        df_component = df_component.to_frame()\n",
    "        # append df_component.loc[1] to suc; suc now contains the number of successful appchecks for the current server and respective component\n",
    "        suc = suc.append(df_component.loc[1])\n",
    "        # append df_component.loc[0] to fail; fail now contains the number of failed appchecks for the current server and respective component\n",
    "        fail = fail.append(df_component.loc[0])\n",
    "        df_list.append(df_component)\n",
    "        i += 1\n",
    "\n",
    "    #we don't want to differentiate Portal Frontend Homepage and Portal Frontend App but consolidate them as Portal Frontend in the report\n",
    "    if suc.PortalFrontendApp < suc.PortalFrontendHomepage: \n",
    "        # drop suc.PortalFrontendHomepage\n",
    "        suc = suc.drop(suc.index[suc.index == 'PortalFrontendHomepage'])\n",
    "        fail = fail.drop(fail.index[fail.index == 'PortalFrontendHomepage'])\n",
    "    else:\n",
    "        # drop suc.PortalFrontendApp\n",
    "        suc = suc.drop(suc.index[suc.index == 'PortalFrontendApp'])\n",
    "        fail = fail.drop(fail.index[fail.index == 'PortalFrontendApp'])\n",
    "    # create a matplotlib ax.bar with labels and the data from the list of dataframes\n",
    "    if (suc.sum(axis=0) > 0 or fail.sum(axis=0) > 0): \n",
    "        ax.bar(labels, suc.to_list(), width, color=colors[3], label='Suceeded')\n",
    "        ax.bar(labels, fail.to_list(), width, bottom=suc.to_list(), color=colors[0], label='Failed')\n",
    "        for index,data in enumerate(suc.to_list()):\n",
    "            plt.text(x=index , y =data/2 , s=f\"{data}\" , fontdict=dict(fontsize=10))\n",
    "        for index,data in enumerate(fail.to_list()):\n",
    "            plt.text(x=index , y =suc.to_list()[index]+10 , s=f\"{data}\" , fontdict=dict(fontsize=10))\n",
    "        # remove spines\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['left'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "        # adjust limits and draw grid lines\n",
    "        plt.ylim(-0.5, ax.get_yticks()[-1] + 0.5)\n",
    "        ax.set_axisbelow(True)\n",
    "        ax.yaxis.grid(color='gray', linestyle='dashed')\n",
    "        # legend\n",
    "        plt.legend(['Success', 'Failed'], loc='upper left', ncol=4, frameon=False)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('No appchecks for server ' + str(s))\n",
    "        continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SLA  - operational time exected vs achieved\n",
    "\n",
    "### Report definition\n",
    "We create a monthly report which includes per day and server the \n",
    "- Expected Operation Time\n",
    "- Sum of unplanned downtime\n",
    "- Passenger Service Availability Time\n",
    "- Passenger Service Availability Status % \n",
    "\n",
    "### Current assumptions\n",
    "#### Components which to consider\n",
    "As earlier explained we only consider certain SW components to be relevant for the SLA\n",
    "```\n",
    "PortalFrontendHomepage, PortalFrontendApp (both in one consolidated status), PortalRuntime, Flight API, Inflight Content Server\n",
    "```\n",
    "From these components, we consider a failed AppCheck as a downtime of the Portal.  \n",
    "Several failed AppChecks in a row, sum up to a higher downtime.\n",
    "For instance: (AppCheck occurs every 10 minutes)\n",
    "> Portal Frontend on server <server> failed **2 times** in a row, \n",
    "> the calculated downtime is **20 minutes** for this server.  \n",
    "> Any other component failing in the same period of time won't add to the calculated downtime.  \n",
    "\n",
    "In addition, we consider the CWAP status and health in a way that an error on the CWAPs also leads \n",
    "to a decreased Passenger Service Availability time.\n",
    "\n",
    "> A CWAP that shows an error in a single check is considered to cause a downtime 10 minutes on 25% of the passengers.\n",
    "\n",
    "#### Additional considerations\n",
    "- Inflight (after Weight off wheels, before Weight on wheels), we consider a sequence of Power on/Power off events as a downtime of the service\n",
    "- Inflight, we consider the lack of AppChecks as a service downtime (remember: we conduct AppChecks every 10 minutes; if no AppCheck occur on a 1h flight, we consider this as a downtime of 1 hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new working df and only copy the columns we need\n",
    "#df_work_events = df_new[['timestamp', 'serialNumber', 'eventId', 'data.element0', 'data.element1', 'data.element2', 'data.element3', 'data.element4', 'data.element5', 'data.element6', 'FlightNo']]\n",
    "df_work_events = df_new.copy()\n",
    "# hey, let's make column 'timestamp' really a timestamp and use it as index\n",
    "df_work_events['timestamp'] = pd.to_datetime(df_work_events['timestamp'], format='%b %d, %Y @ %H:%M:%S.%f')\n",
    "#df_work_events.set_index('timestamp', inplace=True)\n",
    "\n",
    "# filter_power_on is furthermore used to filter all Power on/off events as well as Weight on/off wheels to be able to show operation times between those events\n",
    "# if operation time should be aggregated on a higher level, Weight on/off wheels events are not relevant and can be left ou\n",
    "#filter_power_on = (df_work_events['eventId'] == 'Power on/off')\n",
    "filter_power_on = (df_work_events['eventId'] == 'Power on/off') | (df_work_events['eventId'] == 'Weight off wheels') | (df_work_events['eventId'] == 'Weight on wheels')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground\n",
    "\n",
    "In the playground, I play around with the data to check certain hypothesis, validate things etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the expected operational time per server and date\n",
    "\n",
    "Assumption: The expected operational time is the sum of time differences between a server's first log (using timestamp for it) and it's shutdown time (available in data.element1 if eventId == 'Power On/Off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumption: The expected operational time is the sum of time differences between a server's first log (using timestamp for it) and it's shutdown time (available in data.element1 if eventId == 'Power On/Off')\n",
    "# from df_new, grouped by the servers in column 'serialNumber', build the time differences between data.element0 and data.element1 for rows where eventId == 'Power On/Off' and sum all time differences\n",
    "#df_work_events['expected_operational_time'] = df_work_events.loc[filter_power_on, :].groupby(['serialNumber'])['data.element0', 'data.element1'].apply(lambda x: x['data.element1'] - x['data.element0'])\n",
    "df_work_events['expected_operational_time'] = df_work_events.loc[filter_power_on, :].groupby(['serialNumber'])['data.element0', 'data.element1'].apply(lambda x: x['data.element1'] - x['data.element0'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check on power on/off events that happen inflight\n",
    "\n",
    "Inflight := the time between a ```Weight off wheels``` and a ```Weight on Wheels``` event.  \n",
    "A ```Power on/off```event that happens inflight decreases the overall operation time.\n",
    "\n",
    "Even though that the SLA is defined on-ground as well as in-flight, we don't measure on-groud server re-boots  \n",
    "they could be caused by several reasons: LAME, Aircraft power change and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server 3467540006 has 1 power on/off events in-flight at 2021-11-19 10:10:33.658000 with a downtime of 392.0\n",
      "Have added 392.0 seconds to the total downtime of 3467540006\n",
      "Server 3467540006 has 1 power on/off events in-flight at 2021-11-19 07:07:49.646000 with a downtime of 397.0\n",
      "Have added 397.0 seconds to the total downtime of 3467540006\n",
      "Server 3467540006 has 1 power on/off events in-flight at 2021-11-18 03:26:29.386000 with a downtime of 404.0\n",
      "Have added 404.0 seconds to the total downtime of 3467540006\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reindex from a duplicate axis",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/yt/g3pjgfp12l913d500hlnzffc0000gn/T/ipykernel_11357/2880309925.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mdf_server_grouped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_server\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGrouper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'1D'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sum'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# add the values of column sum in dataframe df_server_grouped to the corresponding column in dataframe df_SLA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mdf_SLA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'downtime'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_server_grouped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sum'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'2021-11-25 00:07:01.307000'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%Y-%m-%d %H:%M:%S.%f'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m#df_SLA_tmp = df_SLA.loc[df_SLA['Server'] == s, :]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0miloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"iloc\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m         \u001b[0miloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m                     \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m                     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1614\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1615\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_update_cacher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1616\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_copy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0mkind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPOSITIONAL_OR_KEYWORD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mreindex\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   4171\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"axis\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4172\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4173\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4175\u001b[0m     def drop(\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mreindex\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   4806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4807\u001b[0m         \u001b[0;31m# perform the reindex on the axes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4808\u001b[0;31m         return self._reindex_axes(\n\u001b[0m\u001b[1;32m   4809\u001b[0m             \u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4810\u001b[0m         ).__finalize__(self, method=\"reindex\")\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_reindex_axes\u001b[0;34m(self, axes, level, limit, tolerance, method, fill_value, copy)\u001b[0m\n\u001b[1;32m   4017\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"index\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4018\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4019\u001b[0;31m             frame = frame._reindex_index(\n\u001b[0m\u001b[1;32m   4020\u001b[0m                 \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4021\u001b[0m             )\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_reindex_index\u001b[0;34m(self, new_index, method, copy, level, fill_value, limit, tolerance)\u001b[0m\n\u001b[1;32m   4036\u001b[0m             \u001b[0mnew_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4037\u001b[0m         )\n\u001b[0;32m-> 4038\u001b[0;31m         return self._reindex_with_indexers(\n\u001b[0m\u001b[1;32m   4039\u001b[0m             \u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnew_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4040\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_reindex_with_indexers\u001b[0;34m(self, reindexers, fill_value, copy, allow_dups)\u001b[0m\n\u001b[1;32m   4872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4873\u001b[0m             \u001b[0;31m# TODO: speed up on homogeneous DataFrame objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4874\u001b[0;31m             new_data = new_data.reindex_indexer(\n\u001b[0m\u001b[1;32m   4875\u001b[0m                 \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4876\u001b[0m                 \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mreindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice)\u001b[0m\n\u001b[1;32m   1299\u001b[0m         \u001b[0;31m# some axes don't allow reindexing with dups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_dups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1301\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_reindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_can_reindex\u001b[0;34m(self, indexer)\u001b[0m\n\u001b[1;32m   3474\u001b[0m         \u001b[0;31m# trying to reindex on an axis with duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_as_unique\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3476\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cannot reindex from a duplicate axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reindex from a duplicate axis"
     ]
    }
   ],
   "source": [
    "\n",
    "#filter_wow = (df_work_events['eventId'] == 'Weight off wheels') | (df_work_events['eventId'] == 'Weight on wheels')\n",
    "warnings.filterwarnings('ignore')\n",
    "for s in servers:\n",
    "    unexpected_downtime = 0\n",
    "    filter_server = (df_work_events['serialNumber'] == s)\n",
    "    df_server = df_work_events.loc[filter_server, :]\n",
    "    filter_wow = (df_server['eventId'] == 'Weight off wheels') | (df_server['eventId'] == 'Weight on wheels')\n",
    "    # find out whether an event \"Power on/off\" occurs between two events \"Weight off wheels\" and \"Weight on wheels\"\n",
    "    wow_timestamps = pd.Series(index=None, dtype='object');\n",
    "    # create a series for each server with the timestamp of match for eventId = 'Power on/off'\n",
    "    wow_timestamps = df_server[filter_wow]['timestamp']\n",
    "    df_server.set_index('timestamp', inplace=True)\n",
    "    for (index, ts) in enumerate(wow_timestamps):\n",
    "        if index < len(wow_timestamps.to_list()) - 1:\n",
    "            # from df_server get the eventId where timestamp = ts\n",
    "            wow_eventId = df_server.loc[ts, 'eventId']\n",
    "            # get the current timestamp (WoW event) and the next timestamp (WoW event) to then filter all events in this power cycle\n",
    "            current_ts = ts\n",
    "            next_ts = wow_timestamps.to_list()[index + 1]\n",
    "            # slice the data and extract all log rows between current_ts and next_ts\n",
    "            df_server_wow_cycle = df_server.loc[current_ts:next_ts, :]\n",
    "            df_server_wow_cycle['sum'] = 0\n",
    "            # if current wow_eventID equals 'Weight off wheels, then we are on ground (rember: the dataframe is sorted by timestamp desc)\n",
    "            filter_wow_cycle_power = (df_server_wow_cycle['eventId'] == 'Power on/off')\n",
    "            if wow_eventId == 'Weight on wheels':\n",
    "                # we only consider power cycles as unexpected downtime if we are in-flight and we store the time difference between power off and power on in a new column 'sum'\n",
    "                df_server_wow_cycle.loc[filter_wow_cycle_power, 'sum'] = (df_server_wow_cycle[filter_wow_cycle_power].index - df_server_wow_cycle[filter_wow_cycle_power]['PowerOff']).astype('timedelta64[s]')\n",
    "                if len(df_server_wow_cycle.loc[filter_wow_cycle_power, :]) > 0:\n",
    "                    print('Server ' + str(s) + ' has ' + str(len(df_server_wow_cycle.loc[filter_wow_cycle_power, :])) + ' power on/off events in-flight at ' + str(current_ts) + ' with a downtime of ' + str(df_server_wow_cycle.loc[filter_wow_cycle_power, 'sum'].sum(axis=0)))\n",
    "                    try:\n",
    "                        df_server.loc[ts, 'sum'] = df_server_wow_cycle.loc[:, 'sum'].sum(axis=0)\n",
    "                    except:\n",
    "                        df_server.loc[ts, 'sum'] = 0\n",
    "                    print('Have added ' + str(df_server.loc[ts, 'sum']) + ' seconds to the total downtime of ' + str(s))\n",
    "    if 'sum' in df_server.columns:\n",
    "        # group by timestamp and sum up all the values in the column 'sum'\n",
    "        df_server_grouped = df_server.reset_index().groupby(pd.Grouper(freq='1D', key='timestamp'))['timestamp', 'sum'].sum()\n",
    "        # add the values of column sum in dataframe df_server_grouped to the corresponding column in dataframe df_SLA\n",
    "        df_SLA.loc[s, 'downtime'] = df_server_grouped['sum']\n",
    "        dt.datetime.strptime('2021-11-25 00:07:01.307000', '%Y-%m-%d %H:%M:%S.%f').date()\n",
    "        #df_SLA_tmp = df_SLA.loc[df_SLA['Server'] == s, :]\n",
    "        #copy values from df_server_grouped to df_SLA by matching the timestamp from df_server_grouped to the date from df_SLA\n",
    "        #df_SLA.loc[((df_SLA['date'] == df_server_grouped.reset_index()['timestamp'].dt.date) & (df_SLA.server == s)), 'downtime'] = df_server_grouped['sum']\n",
    "        #df_SLA.loc[(df_SLA.server == s), 'downtime'] = df_server['sum'].sum(axis=0)\n",
    "        print('Unexpected server downtime on server: ' + str(s) + '...' + str(df_server['sum'].sum(axis=0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SLA report per day and server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over all servers in list servers\n",
    "for s in servers:\n",
    "    print('Iteration for server ' + str(s))\n",
    "    filter_server = (df_work_events['serialNumber'] == s)\n",
    "    filter = (filter_server) & (filter_power_on)\n",
    "    # define a temporary dataframe for each server, on which we perform date-based selections\n",
    "    df_temp = df_work_events.loc[filter_server, :]\n",
    "    # define a pd Series that will store all timestamps when event 'Power on/off' was logged on this server\n",
    "    power_timestamps = pd.Series(dtype='object');\n",
    "    # create a series for each server with the timestamp of match for eventId = 'Power on/off'\n",
    "    power_timestamps = df_temp[filter_power_on]['timestamp']\n",
    "    # make timestamp the index, so that we can localise certain rows easily\n",
    "    df_temp.set_index('timestamp', inplace=True)\n",
    "    # create a new dataframe df_sla with timestamp as index and the columns: event, timestamp, cwap, appcheck, availability \n",
    "    df_sla = pd.DataFrame(columns=['event', 'poweron', 'poweroff', 'timestamp', 'cwap', 'client connected', 'appcheck', 'availability', 'powercycle'])\n",
    "    # fill the dataframe with the values from the series\n",
    "    # first, fill in when Power on/off occured\n",
    "    df_sla['event'] = df_work_events[filter]['eventId']\n",
    "    df_sla['timestamp'] = power_timestamps\n",
    "    # data.element0 contains the power on timestamp and data.element1 contains the power off timestamp\n",
    "    df_sla['poweron'] = df_work_events[filter]['data.element0']\n",
    "    df_sla['poweroff'] = df_work_events[filter]['data.element1']\n",
    "    df_sla['poweron'] = pd.to_datetime(df_sla['poweron'])\n",
    "    df_sla['poweroff'] = pd.to_datetime(df_sla['poweroff'])\n",
    "    # second, from df_work_events filter all events 'CWAP status' between two power cycles and fill in the dataframe\n",
    "    # iterate over the pairs of elements of the series power_timestamps (thus, the period between two power cycles)\n",
    "    for (index, ts) in enumerate(power_timestamps):\n",
    "        if index < len(power_timestamps.to_list()) - 1:\n",
    "            # get the current timestamp (start powercycle) and the next timestamp (end powercycle) to then filter all events in this power cycle\n",
    "            current_ts = ts\n",
    "            next_ts = power_timestamps.to_list()[index + 1]\n",
    "            #from df_temp slice all rows between current_ts and next_ts to be able to get all events for one power cycle\n",
    "            df_temp_slice = df_temp.loc[current_ts:next_ts, :]\n",
    "            # make columns data.element5 and data.element6 from df_temp_slice integers and ignore NaN values\n",
    "            df_temp_slice['data.element5'] = pd.to_numeric(df_temp_slice['data.element5'], errors='coerce')\n",
    "            df_temp_slice['data.element6'] = pd.to_numeric(df_temp_slice['data.element6'], errors='coerce')\n",
    "            # collect and fill in the connected pax for this power cycle\n",
    "            # define a list of columns in which connected pax numbers are stored\n",
    "            pax_connected_list = ['data.element5', 'data.element6']\n",
    "            # sum up the values from columns ['data.element5', 'data.element6'] in df_temp_slice, where eventId = 'CWAP status'\n",
    "            pax_connected = df_temp_slice.loc[df_temp_slice['eventId'] == 'CWAP status', pax_connected_list].sum(axis=1)\n",
    "            # if pax_connected isn't empty and the sum of axis=1 is not 0, then fill in the value\n",
    "            if not pax_connected.empty and pax_connected.sum(axis=0) != 0:\n",
    "                df_sla.loc[df_sla.timestamp == current_ts, 'client connected'] =  pax_connected.sum(axis=0)\n",
    "            else:\n",
    "                df_sla.loc[df_sla.timestamp == current_ts, 'client connected'] =  0\n",
    "            # in the next step, aggregate the status of the appcheck for this power cycle\n",
    "            # all components (frontend, runtime etc.) need to be mandatorily considered for the appcheck\n",
    "            # for an aggregated appcheck status, we sum up the values of the columns ['data.element2', 'data.element3', 'data.element4', 'data.element7', 'data.element8', 'data.element9'] in a power cycle from df_temp_slice\n",
    "            # define a list of columns in which appcheck status is stored\n",
    "            appcheck_list = ['PortalFrontendHomepage', 'PortalFrontendApp', 'PortalRuntime', 'AnalyticsReceiver', 'MovingMap', 'FlightAPI', 'InflightContentServer', 'DRMWidevine', 'DRMFairplay']\n",
    "            # sum up the values from columns in appchecklist; thus, this is the number of succeeded appchecks per appcheck in this power cycle\n",
    "            appcheck_succeeded = df_temp_slice.loc[df_temp_slice['eventId'] == 'AppCheck', appcheck_list].sum(axis=1)\n",
    "            # define appchecks as the number of rows from df_temp_slice where eventId = 'AppCheck'; i.e. appchecks is the number of appchecks in this power cycle\n",
    "            appchecks = df_temp_slice.loc[df_temp_slice['eventId'] == 'AppCheck'].shape[0]\n",
    "            # the overall number of appchecks succeded in this powercycle\n",
    "            sum_appcheck_succeeded = appcheck_succeeded.sum(axis=0)\n",
    "            # avail is the number of succeeded appchecks divided by the number of appchecks (multiplied by number of components) multiplied by 100\n",
    "            avail = (sum_appcheck_succeeded / (appchecks * 9)) * 100\n",
    "            df_sla.loc[df_sla.timestamp == current_ts, 'appcheck'] =  sum_appcheck_succeeded\n",
    "            df_sla.loc[df_sla.timestamp == current_ts, 'availability'] =  avail\n",
    "            # finally, aggregate the cwap health status for this power cycle\n",
    "            # define a list of columns in which cwap status is stored\n",
    "            cwap_list = ['data.element3', 'data.element4']\n",
    "            # sum up the values from columns in cwap_list, where eventId = 'CWAP health'\n",
    "            cwap_health_filter = (df_temp_slice['eventId'] == 'CWAP health') & (df_temp_slice['data.element3'] == 'Error')\n",
    "            # sum up the values from columns in cwap_list, where eventId = 'CWAP health'\n",
    "            cwap = df_temp_slice.loc[cwap_health_filter, cwap_list].count(axis=1)\n",
    "            if not cwap.empty and cwap.sum(axis=0) != 0:\n",
    "                df_sla.loc[df_sla.timestamp == current_ts, 'cwap'] =  'Error'\n",
    "            else:\n",
    "                df_sla.loc[df_sla.timestamp == current_ts, 'cwap'] =  'Activated'\n",
    "            # last but not least, fill in the powercycle column; it's the time difference between the poweron and poweroff of the current powercycle\n",
    "            # if poweron is not empty or NaN and poweroff is not empty or NaN, then fill in the value\n",
    "            power_on = df_sla.loc[df_sla.timestamp == current_ts, 'poweron'].count()\n",
    "            power_off = df_sla.loc[df_sla.timestamp == current_ts, 'poweroff'].count()\n",
    "            if (power_on > 0) and (power_off > 0):\n",
    "                df_sla.loc[df_sla.timestamp == current_ts, 'powercycle'] =  df_sla.loc[df_sla.timestamp == current_ts, 'poweroff'] - df_sla.loc[df_sla.timestamp == current_ts, 'poweron']\n",
    "            df_sla['powercycle'] = pd.to_timedelta(df_sla['powercycle'],'h')\n",
    "            df_sla['ops time'] = df_sla.poweron - df_sla.poweroff\n",
    "            #df_sla['powercycle'] = df_sla['powercycle']/np.timedelta64(1,'h')\n",
    "\n",
    "        # remember, the index of series power_timestamps is the same as in df_work_events\n",
    "    df_sla.set_index('timestamp', inplace=True)\n",
    "    df_sla.to_markdown('sla_' + str(s) + '.md')\n",
    "\"\"\" pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "df_sla.style\n",
    "df_sla\n",
    " \"\"\"\n",
    "    #\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1e39f5ddb06da47c792e6724e01a6f520ddcc160c593e0336e4f03a3f0c97e2f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('jupyterlab-debugger': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
