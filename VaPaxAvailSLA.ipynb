{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VA Passenger Availability SLA Report\n",
    "\n",
    "Once the upgrade to Linux has been completed it is measured using an automated script on the server measuring the availability of the Passenger Service by checking & logging the availability of the SSID (WLAN) and the software health after the booting phase. The System Uptime and hence the Expected Operation Time starts after the booting phase of the software initiated by power on. Any time the Passenger Service then is not available is considered an Unplanned Downtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open tasks\n",
    "- [x] add/improve description (Why? What? How?)\n",
    "- [x] make consideration of DRM Widevine/Fairplay, Moving Map, Analytics Receiver optional and configurable in the report\n",
    "- [x] set first appcheck (timestamp) as the start of operation time and power off as the end\n",
    "- [x] consider CWAP unavailabilities by counting Errors as unavailability\n",
    "- [x] sum up the 10 minute periods of a failed app check to unavailability time (thus, decrease 'actual operation time' accordingly)\n",
    "- [ ] make the appchecks' downtimes available in df_sla\n",
    "- [ ] structure the code and, for instance, use functions\n",
    "- [ ] implement Kibana API interface\n",
    "- [ ] come up with a CI/CD workflow for this/publish the Jupyter report\n",
    "- [x] define the overall SLA dataframe\n",
    "- [x] calculate the Expected Operation Time\n",
    "- [ ] and store it in the SLA dataframe (per day and server)\n",
    "- [x] sum up the server in-flight reboots as unexpected downtime\n",
    "- [ ] and add them to the SLA dataframe (per day and server)\n",
    "- [x] clean up the notebook and remove unsused, irrelevant variables\n",
    "- [ ] introduce server/tailsign mapping\n",
    "- [ ] get the month, which I use to initialise df_SLA, automatically out of the imported data\n",
    "- [ ] do not consider AppCheck downtimes that happened on ground\n",
    "- [x] do not consider CWAP Errors to reduce the expected operational time\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the log data\n",
    "\n",
    "## Source of information\n",
    "The data, that the Juypter Notebook processes and bases the report on, comes from **BoardConnect Kibana Prod**.\n",
    "\n",
    "\n",
    "Since the Notebook currently does not have a direct interface to Kibana implemented, we need to request the data\n",
    "in Kibana and export it as a **CSV** file.\n",
    "\n",
    "In Kibana, one fine a stored request which is named **VAPaxAvailSLAv2** and which should be executed using the\n",
    "relevant timeframe that should be considered in the report (for instance: November 2021).\n",
    "\n",
    "In a first step, the CSV file needs to be provided to Juypter Notebook, so that it can be read and processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42774, 12)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read csv (VAPaxAvailSLAv2) into dataframe\n",
    "df = pd.read_csv('/Users/u293733/git.jupyter-notebooks/VAPaxAvailSLAv2.csv')\n",
    "# outlign the shape \n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data adjustments\n",
    "\n",
    "We keep the original data in the dataframe df and create a copy of it for further processing.  \n",
    "In this copy we adjust the data so that it becomes - for instance - better readable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Event Ids\n",
    "\n",
    "In the original dataset, the events which had been logged, are stored as numbers.  \n",
    "We convert these numbers to humanly readable event descriptions. \n",
    "\n",
    "|Event ID | Event name|\n",
    "|--------|--------|\n",
    "|10891+ | CWAP health |\n",
    "|10900+ | CWAP status |\n",
    "|30100 | AppCheck |\n",
    "|10811 | Weight on wheels |\n",
    "|10812 | Weight off wheels |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new dataframe from df and add column EventID\n",
    "df_new = df.copy()\n",
    "# map the eventID to readable event names\n",
    "eventId_map = {'eventId': {10891: 'CWAP health', 10892: 'CWAP health', 10893: 'CWAP health', 10894: 'CWAP health', \n",
    "                        10900: 'CWAP status', 10901: 'CWAP status', 10902: 'CWAP status', 10903: 'CWAP status',\n",
    "                        30100: 'AppCheck', 10811: 'Weight on wheels', 10812: 'Weight off wheels', 10957: 'Power on/off'}}\n",
    "# replace eventId with readable values from the map\n",
    "df_new.replace(eventId_map, inplace=True)\n",
    "# in Python, it's a good practice to typecase categorical features to a category type to fasten up the processing of the data\n",
    "df_new['eventId'] = df_new['eventId'].astype('category')\n",
    "\n",
    "# define the overall SLA dataframe in which the overall availability will be stored\n",
    "df_SLA = pd.DataFrame(columns=['date', 'server', 'expected operation time', 'downtime', 'actual operation time', 'availability'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract grouped data\n",
    "\n",
    "Some logged data comes as a concatenated string and, for further processing and better interpretation, now needs to be split and stored\n",
    "in separate fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for eventId = 'AppCheck' and create a new column 'FlightNo' with the first element of the split column 'data.element0'\n",
    "df_new['FlightNo'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[0]\n",
    "# filter for eventId = 'CWAP status' and create a new column 'PaxConnected' with the value from 'data.element5'\n",
    "df_new['PaxConnected'] = df_new[df_new['eventId'] == 'CWAP status']['data.element5']\n",
    "# filter for eventId = 'AppCheck' and take the fourth element out of 'data.element0'; get the first integer out of it and add this to new column 'PortalFrontendHomepage' (1 = success)\n",
    "df_new['PortalFrontendHomepage'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[3].str.split(':').str[1].str.split(',').str[0].astype(int)\n",
    "# proceed with the other components that are checked by BCEL AppCheck\n",
    "df_new['PortalFrontendApp'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[4].str.split(':').str[1].str.split(',').str[0].astype(int)\n",
    "df_new['PortalRuntime'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[5].str.split(':').str[1].str.split(',').str[0].astype(int)\n",
    "df_new['AnalyticsReceiver'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[6].str.split(':').str[1].str.split(',').str[0].astype(int)\n",
    "df_new['MovingMap'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[7].str.split(':').str[1].str.split(',').str[0].astype(int)\n",
    "df_new['FlightAPI'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[8].str.split(':').str[1].str.split(',').str[0].astype(int)\n",
    "df_new['InflightContentServer'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[9].str.split(':').str[1].str.split(',').str[0].astype(int)\n",
    "df_new['DRMWidevine'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[10].str.split(':').str[1].str.split(',').str[0].astype(int)\n",
    "df_new['DRMFairplay'] = df_new[df_new['eventId'] == 'AppCheck']['data.element0'].str.split(';').str[11].str.split(':').str[1].str.split(',').str[0].astype(int)\n",
    "\n",
    "# extract power on/off timestamps to an explicit column and convert to datetime\n",
    "df_new['PowerOn'] = df_new[df_new['eventId'] == 'Power on/off']['data.element0'].str.split(';').str[0]\n",
    "df_new['PowerOn'] = pd.to_datetime(df_new['PowerOn'])\n",
    "df_new['PowerOff'] = df_new[df_new['eventId'] == 'Power on/off']['data.element1'].str.split(';').str[0]\n",
    "df_new['PowerOff'] = pd.to_datetime(df_new['PowerOff'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration section\n",
    "\n",
    "In this section we define the **components** that the Juypter Notebook shall consider for report generation.\n",
    "\n",
    "**Hint**\n",
    "> The AppCheck currently considers 9 different components to be mandatorily checked by it.\n",
    "> These are:\n",
    "> - Portal Frontend Homepage\n",
    "> - Portal Frontend App\n",
    "> - Portal Runtime\n",
    "> - Analytics Receiver\n",
    "> - Moving Map\n",
    "> - Flight API\n",
    "> - Inflight Content Server\n",
    "> - DRM Widevine\n",
    "> - DRM Fairplay\n",
    "\n",
    "As per an agreement with the VA PDM from Dec, 2nd 2021, we consider the following components as relevant for measuring the BC IFE service availability:\n",
    "\n",
    "> - Portal Frontend Homepage\n",
    "> - Portal Frontend App\n",
    "> - Portal Runtime\n",
    "> - Flight API\n",
    "> - Inflight Content Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of components for the diagram; we leave out the non-SLA-relevant components and combine PortalFrontendHomepage and PortalFrontendApp \n",
    "#components = ['PortalFrontendHomepage', 'PortalFrontendApp', 'PortalRuntime', 'AnalyticsReceiver', 'MovingMap', 'FlightAPI', 'InflightContentServer', 'DRMWidevine', 'DRMFairplay']\n",
    "components = ['PortalFrontendHomepage', 'PortalFrontendApp', 'PortalRuntime', 'FlightAPI', 'InflightContentServer']\n",
    "# create a list of all servers in the report \n",
    "servers = list()\n",
    "for s in df_new['serialNumber'].unique().tolist():\n",
    "    servers.append(s)\n",
    "# if other servers (or a subset of servers) than contained in the CSV file shall be used, append those explicitly to the list\n",
    "#servers.append('7CTCA20586')\n",
    "\n",
    "month = '2021-11'\n",
    "period = pd.Period(month, freq='M')\n",
    "#df_SLA.date = pd.Series(pd.date_range(start=period.start_time, end=period.end_time, freq='D'))\n",
    "month_days = pd.Series(pd.date_range(start=period.start_time, end=period.end_time, freq='D'))\n",
    "# initialise the overall SLA dataframe\n",
    "for day in month_days:\n",
    "    # iterate over all servers\n",
    "    for s in servers:\n",
    "        # add a new row to df_SLA where df_SLA.date = day and df_SLA.server = s\n",
    "        df_SLA = df_SLA.append(pd.DataFrame({'date': [day], 'server': [s], 'expected operation time': [0], 'downtime': [0], 'actual operation time': [0], 'availability': [0]}, index=[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report visualisation\n",
    "\n",
    "## Succeeded and failed AppChecks (optional) \n",
    "\n",
    "The subsequent charts show the totals for the succeeded and failed AppChecks on all defined servers in the time period  \n",
    "that is contained in the CSV file.\n",
    "\n",
    "(For further visualisations, refer to this)[https://towardsdatascience.com/stacked-bar-charts-with-pythons-matplotlib-f4020e4eb4a7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a working copy of df_new, filtered by eventId = 'AppCheck', using only the columns: timestamp, serialNumber, eventId, PortalFrontendHomepage\n",
    "df_work_appcheck = df_new[df_new['eventId'] == 'AppCheck'][['timestamp', 'serialNumber', 'eventId', 'PortalFrontendHomepage', 'PortalFrontendApp', 'PortalRuntime', 'AnalyticsReceiver', 'MovingMap', 'FlightAPI', 'InflightContentServer', 'DRMWidevine', 'DRMFairplay']]\n",
    "# define the labels to be used in the charts\n",
    "labels = ['Portal Frontend', 'Portal Runtime', 'Flight API', 'Inflight Content Server']\n",
    "# i is just a counter to print the respective dataframe df_list[i]\n",
    "i = 0\n",
    "# start defining the stacked bar plot\n",
    "colors = ['#1D2F6F', '#8390FA', '#6EAF46', '#FAC748']\n",
    "width = 0.35\n",
    "#iterate over all servers and create a new dataframe for each server and component which contains the numbers of succeeded and failed component checks; store these new dataframes in df_list()\n",
    "for s in servers:\n",
    "    print('Iteration for server ' + str(s))\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax.set_ylabel('Number of AppChecks')\n",
    "    plt.xticks(\n",
    "        rotation=45,\n",
    "        horizontalalignment='right',\n",
    "        fontweight='light',\n",
    "        fontsize='medium',\n",
    "    )\n",
    "    # chart's title\n",
    "    ax.set_title('AppChecks for Server ' + str(s))\n",
    "    # suc is the series of successful appchecks for the current server and and the respective component\n",
    "    suc = pd.Series(dtype = 'object')\n",
    "    # fail is the series of failed appchecks for the current server and respective component\n",
    "    fail = pd.Series(dtype = 'object')\n",
    "    # create a list of dataframes\n",
    "    df_list = list()\n",
    "    # first, define a filter for all servers in list s\n",
    "    df_server_filter = (df_new['serialNumber'] == s)\n",
    "    # then, iterate over the labels (which are components of the appcheck) and create a dataframe per server and component\n",
    "    for component in components:\n",
    "        # reset the dataframe per server\n",
    "        df_component = ()\n",
    "        # define the components as categorical data using value 1, 0 (1 = success, 0 = failure)\n",
    "        df_work_appcheck[component] = pd.Categorical(df_work_appcheck[component], categories=[1, 0], ordered=True)\n",
    "        # create a new series for each component by grouping the filtered original dataframe by the component and count the number of occurences\n",
    "        df_component = df_work_appcheck.loc[df_server_filter, :].groupby([component])[component].count()\n",
    "        df_component = df_component.to_frame()\n",
    "        # append df_component.loc[1] to suc; suc now contains the number of successful appchecks for the current server and respective component\n",
    "        suc = suc.append(df_component.loc[1])\n",
    "        # append df_component.loc[0] to fail; fail now contains the number of failed appchecks for the current server and respective component\n",
    "        fail = fail.append(df_component.loc[0])\n",
    "        df_list.append(df_component)\n",
    "        i += 1\n",
    "\n",
    "    #we don't want to differentiate Portal Frontend Homepage and Portal Frontend App but consolidate them as Portal Frontend in the report\n",
    "    if suc.PortalFrontendApp < suc.PortalFrontendHomepage: \n",
    "        # drop suc.PortalFrontendHomepage\n",
    "        suc = suc.drop(suc.index[suc.index == 'PortalFrontendHomepage'])\n",
    "        fail = fail.drop(fail.index[fail.index == 'PortalFrontendHomepage'])\n",
    "    else:\n",
    "        # drop suc.PortalFrontendApp\n",
    "        suc = suc.drop(suc.index[suc.index == 'PortalFrontendApp'])\n",
    "        fail = fail.drop(fail.index[fail.index == 'PortalFrontendApp'])\n",
    "    # create a matplotlib ax.bar with labels and the data from the list of dataframes\n",
    "    if (suc.sum(axis=0) > 0 or fail.sum(axis=0) > 0): \n",
    "        ax.bar(labels, suc.to_list(), width, color=colors[3], label='Suceeded')\n",
    "        ax.bar(labels, fail.to_list(), width, bottom=suc.to_list(), color=colors[0], label='Failed')\n",
    "        for index,data in enumerate(suc.to_list()):\n",
    "            plt.text(x=index , y =data/2 , s=f\"{data}\" , fontdict=dict(fontsize=10))\n",
    "        for index,data in enumerate(fail.to_list()):\n",
    "            plt.text(x=index , y =suc.to_list()[index]+10 , s=f\"{data}\" , fontdict=dict(fontsize=10))\n",
    "        # remove spines\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['left'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "        # adjust limits and draw grid lines\n",
    "        plt.ylim(-0.5, ax.get_yticks()[-1] + 0.5)\n",
    "        ax.set_axisbelow(True)\n",
    "        ax.yaxis.grid(color='gray', linestyle='dashed')\n",
    "        # legend\n",
    "        plt.legend(['Success', 'Failed'], loc='upper left', ncol=4, frameon=False)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('No appchecks for server ' + str(s))\n",
    "        continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLA report - operational time expected vs achieved per day and server\n",
    "\n",
    "## Report definition\n",
    "We create a monthly report which includes per day and server the \n",
    "- Expected Operation Time\n",
    "- Sum of unplanned downtime\n",
    "- Passenger Service Availability Time\n",
    "- Passenger Service Availability Status % \n",
    "\n",
    "## Current assumptions\n",
    "### Components which to consider\n",
    "As earlier explained we only consider certain SW components to be relevant for the SLA\n",
    "```\n",
    "PortalFrontendHomepage, PortalFrontendApp (both in one consolidated status), PortalRuntime, Flight API, Inflight Content Server\n",
    "```\n",
    "From these components, we consider a failed AppCheck as a downtime of the Portal.  \n",
    "Several failed AppChecks in a row, sum up to a higher downtime.\n",
    "For instance: (AppCheck occurs every 10 minutes)\n",
    "> Portal Frontend on server <server> failed **2 times** in a row, \n",
    "> the calculated downtime is **20 minutes** for this server.  \n",
    "> Any other component failing in the same period of time won't add to the calculated downtime.  \n",
    "\n",
    "~~In addition, we consider the CWAP status and health in a way that an error on the CWAPs also leads \n",
    "to a decreased Passenger Service Availability time.~~\n",
    "\n",
    "> ~~A CWAP that shows an error in a single check is considered to cause a downtime 10 minutes on 25% of the passengers.~~  \n",
    "> A CWAP that shows an error for one of the radios is not considered to cause a downtime since since rooming takes place  \n",
    "> and passengers are connected to the next CWAP\n",
    "\n",
    "### Expected Operational Time\n",
    "- We consider every period of time between a 'power on' and 'power off' event that happens on-ground to add to the overall Expected Operational Time of the server. Power cycles that happen in-flight decrease the time (pls. refer to 'Additional considerations')\n",
    "\n",
    "### Additional considerations\n",
    "- Inflight (after Weight off wheels, before Weight on wheels), we consider a sequence of Power on/Power off events as a downtime of the service\n",
    "- Inflight, we consider the lack of AppChecks as a service downtime (remember: we conduct AppChecks every 10 minutes; if no AppCheck occur on a 1h flight, we consider this as a downtime of 1 hour)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new working df and only copy the columns we need\n",
    "#df_work_events = df_new[['timestamp', 'serialNumber', 'eventId', 'data.element0', 'data.element1', 'data.element2', 'data.element3', 'data.element4', 'data.element5', 'data.element6', 'FlightNo']]\n",
    "df_work_events = df_new.copy()\n",
    "# hey, let's make column 'timestamp' really a timestamp and use it as index\n",
    "df_work_events['timestamp'] = pd.to_datetime(df_work_events['timestamp'], format='%b %d, %Y @ %H:%M:%S.%f')\n",
    "#df_work_events.set_index('timestamp', inplace=True)\n",
    "\n",
    "# filter_power_on is furthermore used to filter all Power on/off events as well as Weight on/off wheels to be able to show operation times between those events\n",
    "# if operation time should be aggregated on a higher level, Weight on/off wheels events are not relevant and can be left ou\n",
    "#filter_power_on = (df_work_events['eventId'] == 'Power on/off')\n",
    "filter_power_on = (df_work_events['eventId'] == 'Power on/off') | (df_work_events['eventId'] == 'Weight off wheels') | (df_work_events['eventId'] == 'Weight on wheels')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground\n",
    "\n",
    "In the playground, I play around with the data to check certain hypothesis, validate things etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the expected operational time per server and date\n",
    "\n",
    "Assumption: The expected operational time is the sum of time differences between a server's first log (using timestamp for it) and it's shutdown time (available in data.element1 if eventId == 'Power On/Off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumption: The expected operational time is the sum of time differences between a server's first log (using timestamp for it) and it's shutdown time (available in data.element1 if eventId == 'Power On/Off')\n",
    "# from df_new, grouped by the servers in column 'serialNumber', build the time differences between data.element0 and data.element1 for rows where eventId == 'Power On/Off' and sum all time differences\n",
    "#df_work_events['expected_operational_time'] = df_work_events.loc[filter_power_on, :].groupby(['serialNumber'])['data.element0', 'data.element1'].apply(lambda x: x['data.element1'] - x['data.element0'])\n",
    "df_work_events['expected_operational_time'] = df_work_events.loc[filter_power_on, :].groupby(['serialNumber'])['data.element0', 'data.element1'].apply(lambda x: x['data.element1'] - x['data.element0'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check on power on/off events that happen inflight and on-ground\n",
    "\n",
    "Inflight := the time between a ```Weight off wheels``` and a ```Weight on Wheels``` event.  \n",
    "A ```Power on/off```event that happens inflight decreases the overall operation time.\n",
    "\n",
    "Even though that the SLA is defined on-ground as well as in-flight, we don't consider on-groud server re-boots as downtimes \n",
    "they could be caused by several reasons: LAME, Aircraft power change and others.\n",
    "\n",
    "We calculate the downtime that happened inflight per server and day in the following procedure.  \n",
    "Moreover, we calculate the time per server and day between each 'power on' and 'power off' event that happened on-ground.  \n",
    "All these time differences, we sum up to be the Expected Operational Time per server and day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-18 11:06:51.493000\n",
      "Opstime on-ground and inflight on server: 3442310010...73.24666666666667\n",
      "2021-11-26 08:22:01.495000\n",
      "Unexpected server downtime inflight on server: 3467540006...0.3313888888888889\n",
      "2021-11-01 08:27:18.361000\n",
      "Opstime on-ground and inflight on server: 3467540006...366.21194444444444\n",
      "2021-11-26 07:59:26.243000\n",
      "2021-11-09 01:09:38.943000\n",
      "Opstime on-ground and inflight on server: 3352680006...109.51777777777778\n",
      "2021-11-26 07:48:40.345000\n",
      "Unexpected server downtime inflight on server: 3395600002...0.11083333333333334\n",
      "2021-11-19 12:06:01.307000\n",
      "Opstime on-ground and inflight on server: 3395600002...63.7675\n",
      "2021-11-26 07:38:13.479000\n",
      "Unexpected server downtime inflight on server: 3734240001...0.11\n",
      "2021-11-07 06:17:17.394000\n",
      "Opstime on-ground and inflight on server: 3734240001...201.4325\n",
      "2021-11-25 11:01:23.072000\n",
      "2021-11-15 11:06:33.774000\n",
      "Opstime on-ground and inflight on server: 3413940001...113.11277777777778\n",
      "2021-11-25 09:39:14.513000\n",
      "2021-11-04 07:16:28.218000\n",
      "Opstime on-ground and inflight on server: 3472530005...400.23333333333335\n",
      "2021-11-25 06:23:09.607000\n",
      "2021-11-10 08:47:45.754000\n",
      "Opstime on-ground and inflight on server: 3413940006...72.99861111111112\n",
      "2021-11-21 11:33:42.684000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#filter_wow = (df_work_events['eventId'] == 'Weight off wheels') | (df_work_events['eventId'] == 'Weight on wheels')\n",
    "warnings.filterwarnings('ignore')\n",
    "for s in servers:\n",
    "    unexpected_downtime = 0\n",
    "    filter_server = (df_work_events['serialNumber'] == s)\n",
    "    # df_server contains all events for the current server s\n",
    "    df_server = df_work_events.loc[filter_server, :]\n",
    "    filter_wow = (df_server['eventId'] == 'Weight off wheels') | (df_server['eventId'] == 'Weight on wheels')\n",
    "    # find out whether an event \"Power on/off\" occurs between two events \"Weight off wheels\" and \"Weight on wheels\"\n",
    "    wow_timestamps = pd.Series(index=None, dtype='object');\n",
    "    # create a series for each server with the timestamp of match for eventId = 'Power on/off'\n",
    "    wow_timestamps = df_server[filter_wow]['timestamp']\n",
    "    df_server.set_index('timestamp', inplace=True)\n",
    "    for (index, ts) in enumerate(wow_timestamps):\n",
    "        if index < len(wow_timestamps.to_list()) - 1:\n",
    "            # from df_server get the eventId where timestamp = ts\n",
    "            wow_eventId = df_server.loc[ts, 'eventId']\n",
    "            # get the current timestamp (WoW event) and the next timestamp (WoW event) to then filter all events in this power cycle\n",
    "            current_ts = ts\n",
    "            next_ts = wow_timestamps.to_list()[index + 1]\n",
    "            # slice the data and extract all log rows between current_ts and next_ts\n",
    "            df_server_wow_cycle = df_server.loc[current_ts:next_ts, :]\n",
    "            df_server_wow_cycle['downtime'] = 0\n",
    "            df_server_wow_cycle['opstime'] = 0\n",
    "            # if current wow_eventID equals 'Weight on wheels, then the currently analysed period was 'inflight' (rember: the dataframe is sorted by timestamp desc)\n",
    "            filter_wow_cycle_power = (df_server_wow_cycle['eventId'] == 'Power on/off')\n",
    "            if wow_eventId == 'Weight on wheels':\n",
    "                # we only consider power cycles as unexpected downtime if we are in-flight and we store the time difference between power off and power on in a new column 'sum'\n",
    "                df_server_wow_cycle.loc[filter_wow_cycle_power, 'downtime'] = (df_server_wow_cycle[filter_wow_cycle_power].index - df_server_wow_cycle[filter_wow_cycle_power]['PowerOff']).astype('timedelta64[s]')\n",
    "                if len(df_server_wow_cycle.loc[filter_wow_cycle_power, :]) > 0:\n",
    "                    #print('Server ' + str(s) + ' has ' + str(len(df_server_wow_cycle.loc[filter_wow_cycle_power, :])) + ' power on/off events in-flight at ' + str(current_ts) + ' with a downtime of ' + str(df_server_wow_cycle.loc[filter_wow_cycle_power, 'downtime'].sum(axis=0)))\n",
    "                    try:\n",
    "                        df_server.loc[ts, 'downtime'] = df_server_wow_cycle.loc[:, 'downtime'].sum(axis=0)\n",
    "                    except:\n",
    "                        df_server.loc[ts, 'downtime'] = 0\n",
    "                    #print('Have added ' + str(df_server.loc[ts, 'downtime']) + ' seconds to the total downtime of ' + str(s))\n",
    "            #else: if we were on ground, sum up the time difference between power on and power off and add those differences to the Expected Operational Time\n",
    "            else:\n",
    "                df_server_wow_cycle.loc[filter_wow_cycle_power, 'opstime'] = (df_server_wow_cycle[filter_wow_cycle_power].index - df_server_wow_cycle[filter_wow_cycle_power]['PowerOff']).astype('timedelta64[s]')\n",
    "                if len(df_server_wow_cycle.loc[filter_wow_cycle_power, :]) > 0:\n",
    "                    #print('Server ' + str(s) + ' has ' + str(len(df_server_wow_cycle.loc[filter_wow_cycle_power, :])) + ' power on/off events on-ground at ' + str(current_ts) + ' with an opstime of ' + str(df_server_wow_cycle.loc[filter_wow_cycle_power, 'opstime'].sum(axis=0)))\n",
    "                    try:\n",
    "                        df_server.loc[ts, 'opstime'] = df_server_wow_cycle.loc[:, 'opstime'].sum(axis=0)\n",
    "                    except:\n",
    "                        df_server.loc[ts, 'opstime'] = 0\n",
    "                    #print('Have added ' + str(df_server.loc[ts, 'opstime']) + ' seconds to the total opstime of ' + str(s))\n",
    "    if ('downtime' in df_server.columns):\n",
    "        # group by timestamp and sum up all the values in the column 'sum'\n",
    "        df_server_grouped_downtime = df_server.reset_index().groupby(pd.Grouper(freq='1D', key='timestamp'))['timestamp', 'downtime'].sum()\n",
    "        # add the values of column sum in dataframe df_server_grouped to the corresponding column in dataframe df_SLA\n",
    "        #df_SLA.loc[s, 'downtime'] = df_server_grouped['sum']\n",
    "        #dt.datetime.strptime('2021-11-25 00:07:01.307000', '%Y-%m-%d %H:%M:%S.%f').date()\n",
    "        #df_SLA_tmp = df_SLA.loc[df_SLA['Server'] == s, :]\n",
    "        #copy values from df_server_grouped to df_SLA by matching the timestamp from df_server_grouped to the date from df_SLA\n",
    "        #df_SLA.loc[((df_SLA['date'] == df_server_grouped.reset_index()['timestamp'].dt.date) & (df_SLA.server == s)), 'downtime'] = df_server_grouped['sum']\n",
    "        #df_SLA.loc[(df_SLA.server == s), 'downtime'] = df_server['sum'].sum(axis=0)\n",
    "        print('Unexpected server downtime inflight on server: ' + str(s) + '...' + str((df_server['downtime'].sum(axis=0))/3600))\n",
    "        #iterate over all elements in df_server_grouped_downtime and add the values to the corresponding column in df_SLA\n",
    "        for (index, row) in df_server_grouped_downtime.iterrows():\n",
    "            server_downtime = 0\n",
    "            server_downtime = df_SLA.loc[(df_SLA.server == s) & (df_SLA.date == index), 'downtime']\n",
    "            server_downtime += row['downtime']\n",
    "            df_SLA.loc[(df_SLA.server == s) & (df_SLA.date == index), 'downtime'] = server_downtime\n",
    "\n",
    "    if ('opstime' in df_server.columns):\n",
    "        df_server_grouped_opstime = df_server.reset_index().groupby(pd.Grouper(freq='1D', key='timestamp'))['timestamp', 'opstime'].sum()\n",
    "        #iterate over all elements in df_server_grouped_opstime and add the values to the corresponding column in df_SLA\n",
    "        for (index, row) in df_server_grouped_opstime.iterrows():\n",
    "            server_opstime = 0\n",
    "            server_opstime = df_SLA.loc[(df_SLA.server == s) & (df_SLA.date == index), 'actual operation time']\n",
    "            server_opstime += row['opstime']\n",
    "            df_SLA.loc[(df_SLA.server == s) & (df_SLA.date == index), 'actual operation time'] = server_opstime\n",
    "        print(str(df_server.index.min()))\n",
    "        print('Opstime on-ground and inflight on server: ' + str(s) + '...' + str((df_server['opstime'].sum(axis=0))/3600))\n",
    "        print(str(df_server.index.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of the aggregated AppCheck\n",
    "\n",
    "In the subsequent analysis, we check how many appchecks succeeded or failed between two Power on/off events for the defined servers.  \n",
    "\n",
    "We can easily increase transparency for this analysis and consider 'Weight on/off wheels' as well  \n",
    "(by changing the filter *filter_power_on*) and thus, come to a view where we can differentiate  \n",
    "in-flight and on-ground behaviour.\n",
    "\n",
    "The result of this report currently contains:  \n",
    "- The event (Power on/off, WoW) which occured (event) and \n",
    "- when server has been powered on and off (poweron, poweroff),\n",
    "- whether the CWAPs have been successfully activated (status: *Activated* in column cwap) in the period between the previous and the current event\n",
    "- how many clients have been connected (client connected) in this period\n",
    "- the number of appchecks having been conducted (appcheck) in this period\n",
    "- the calculated availability (availability) in terms of succeeded or failed appchecks in this period\n",
    "- the ops time (opstime) of the server as the difference between the latest power off event and the last power on event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration for server 3442310010\n",
      "Downtime for server 3442310010: 0\n",
      "Iteration for server 3467540006\n",
      "Downtime for server 3467540006: 0\n",
      "Iteration for server 3352680006\n",
      "Downtime for server 3352680006: 0\n",
      "Iteration for server 3395600002\n",
      "Downtime for server 3395600002: 1200\n",
      "Iteration for server 3734240001\n",
      "Downtime for server 3734240001: 0\n",
      "Iteration for server 3413940001\n",
      "Downtime for server 3413940001: 0\n",
      "Iteration for server 3472530005\n",
      "Downtime for server 3472530005: 0\n",
      "Iteration for server 3413940014\n",
      "Downtime for server 3413940014: 0\n",
      "Iteration for server 7CTCA20586\n",
      "Downtime for server 7CTCA20586: 0\n",
      "Iteration for server 3413940006\n",
      "Downtime for server 3413940006: 600\n"
     ]
    }
   ],
   "source": [
    "# iterate over all servers in list servers\n",
    "for s in servers:\n",
    "    print('Iteration for server ' + str(s))\n",
    "    server_downtime = 0\n",
    "    filter_server = (df_work_events['serialNumber'] == s)\n",
    "    #filter_power_on is defined in a section earlier in this notebook (!!! it might include power events only or WoW events as well!!)\n",
    "    filter = (filter_server) & (filter_power_on)\n",
    "    # define a temporary dataframe for each server, on which we perform date-based selections\n",
    "    df_appcheck = df_work_events.loc[filter_server, :]\n",
    "    # define a pd Series that will store all timestamps when event 'Power on/off' was logged on this server\n",
    "    power_timestamps = pd.Series(dtype='object');\n",
    "    # create a series for each server with the timestamp of match for eventId = 'Power on/off'\n",
    "    power_timestamps = df_appcheck[filter_power_on]['timestamp']\n",
    "    # make timestamp the index, so that we can localise certain rows easily\n",
    "    df_appcheck.set_index('timestamp', inplace=True)\n",
    "    # create a new dataframe df_sla with timestamp as index and the columns: event, timestamp, cwap, appcheck, availability \n",
    "    df_sla = pd.DataFrame(columns=['event', 'poweron', 'poweroff', 'timestamp', 'cwap', 'client connected', 'appcheck', 'availability', 'powercycle'])\n",
    "    # fill the dataframe with the values from the series\n",
    "    # first, fill in when Power on/off occured\n",
    "    df_sla['event'] = df_work_events[filter]['eventId']\n",
    "    df_sla['timestamp'] = power_timestamps\n",
    "    # data.element0 contains the power on timestamp and data.element1 contains the power off timestamp\n",
    "    df_sla['poweron'] = df_work_events[filter]['data.element0']\n",
    "    df_sla['poweroff'] = df_work_events[filter]['data.element1']\n",
    "    df_sla['poweron'] = pd.to_datetime(df_sla['poweron'])\n",
    "    df_sla['poweroff'] = pd.to_datetime(df_sla['poweroff'])\n",
    "    # second, from df_work_events filter all events 'CWAP status' between two power cycles and fill in the dataframe\n",
    "    # iterate over the pairs of elements of the series power_timestamps (thus, the period between two power cycles)\n",
    "    for (index, ts) in enumerate(power_timestamps):\n",
    "        if index < len(power_timestamps.to_list()) - 1:\n",
    "            # get the current timestamp (start powercycle) and the next timestamp (end powercycle) to then filter all events in this power cycle\n",
    "            current_ts = ts\n",
    "            next_ts = power_timestamps.to_list()[index + 1]\n",
    "            #from df_temp slice all rows between current_ts and next_ts to be able to get all events for one power cycle\n",
    "            df_temp_slice = df_appcheck.loc[current_ts:next_ts, :]\n",
    "            # make columns data.element5 and data.element6 from df_temp_slice integers and ignore NaN values\n",
    "            df_temp_slice['data.element5'] = pd.to_numeric(df_temp_slice['data.element5'], errors='coerce')\n",
    "            df_temp_slice['data.element6'] = pd.to_numeric(df_temp_slice['data.element6'], errors='coerce')\n",
    "            # collect and fill in the connected pax for this power cycle\n",
    "            # define a list of columns in which connected pax numbers are stored\n",
    "            pax_connected_list = ['data.element5', 'data.element6']\n",
    "            # sum up the values from columns ['data.element5', 'data.element6'] in df_temp_slice, where eventId = 'CWAP status'\n",
    "            pax_connected = df_temp_slice.loc[df_temp_slice['eventId'] == 'CWAP status', pax_connected_list].sum(axis=1)\n",
    "            # if pax_connected isn't empty and the sum of axis=1 is not 0, then fill in the value\n",
    "            if not pax_connected.empty and pax_connected.sum(axis=0) != 0:\n",
    "                df_sla.loc[df_sla.timestamp == current_ts, 'client connected'] =  pax_connected.sum(axis=0)\n",
    "            else:\n",
    "                df_sla.loc[df_sla.timestamp == current_ts, 'client connected'] =  0\n",
    "            # in the next step, aggregate the status of the appcheck for this power cycle for the defined components in list 'components'\n",
    "            # succeeded appchecks per component are written as 1 in the respective component's column\n",
    "            # we sum up the values (1 or 0) from the components' columns per appcheck conducted;\n",
    "            appcheck_succeeded = df_temp_slice.loc[df_temp_slice['eventId'] == 'AppCheck', components].sum(axis=1)\n",
    "            # if at least one component appcheck failed, we add 10 minutes (and only 10!) to the current server's downtime\n",
    "            if not appcheck_succeeded.empty: # and appcheck_succeeded < len(components):\n",
    "                #iterate over all elements in series appcheck_succeeded\n",
    "                for (index, succeeded) in enumerate(appcheck_succeeded):\n",
    "                    if succeeded < len(components):\n",
    "                        server_downtime = server_downtime + 10*60\n",
    "                        #add the server_downtime directly to df_SLA with server == s and date == date(current_ts)\n",
    "                        date = current_ts.date()\n",
    "                        try:\n",
    "                            server_downtime += df_SLA.loc[(df_SLA.server == s) & (df_SLA.date == date), 'downtime'].values[0]\n",
    "                        except:\n",
    "                            server_downtime += 0\n",
    "                        finally:\n",
    "                            df_SLA.loc[(df_SLA.server == s) & (df_SLA.date == date), 'downtime'] = server_downtime\n",
    "            # define appchecks as the number of rows from df_temp_slice where eventId = 'AppCheck'; i.e. appchecks is the number of appchecks conducted in this power cycle\n",
    "            appchecks = df_temp_slice.loc[df_temp_slice['eventId'] == 'AppCheck'].shape[0]\n",
    "            # we now sum up the overall number of appchecks succeded in this powercycle\n",
    "            sum_appcheck_succeeded = appcheck_succeeded.sum(axis=0)\n",
    "            # avail is the number of succeeded component appchecks divided by the number of appchecks (multiplied by number of components) multiplied by 100\n",
    "            component_appchecks = appchecks * len(components)\n",
    "            avail = (sum_appcheck_succeeded / (component_appchecks)) * 100\n",
    "            # finally, write the calculated values to the dataframe\n",
    "            df_sla.loc[df_sla.timestamp == current_ts, 'appcheck'] =  sum_appcheck_succeeded\n",
    "            df_sla.loc[df_sla.timestamp == current_ts, 'availability'] =  avail\n",
    "            # finally, aggregate the cwap health status for this power cycle\n",
    "            # define a list of columns in which cwap status is stored; data.element3 and 4 hold the actual radio status \n",
    "            cwap_list = ['data.element3', 'data.element4']\n",
    "            # filter for CWAP health status 'Error' and if such a status occured, write it to the dataframe and add 10 minutes to the current server's downtime\n",
    "            cwap_health_filter = (df_temp_slice['eventId'] == 'CWAP health') & ((df_temp_slice['data.element3'] == 'Error') | (df_temp_slice['data.element4'] == 'Error'))\n",
    "            # sum up the values from columns in cwap_list, where eventId = 'CWAP health' to detect whether one of the radios is in error\n",
    "            cwap = df_temp_slice.loc[cwap_health_filter, cwap_list].count(axis=1)\n",
    "            if not cwap.empty and cwap.sum(axis=0) != 0:\n",
    "                cwap_down = cwap.sum(axis=0) / 2\n",
    "                df_sla.loc[df_sla.timestamp == current_ts, 'cwap'] =  'Error'\n",
    "                ####################################################\n",
    "                # as per a common decision on Dec, 9th, we won't consider CWAP reported errrors as service downtimes\n",
    "                ####################################################\n",
    "                #server_downtime = server_downtime + cwap_down*10*60\n",
    "            else:\n",
    "                df_sla.loc[df_sla.timestamp == current_ts, 'cwap'] =  'Activated'\n",
    "            # last but not least, fill in the powercycle column; it's the time difference between the poweron and poweroff of the current powercycle\n",
    "            # if poweron is not empty or NaN and poweroff is not empty or NaN, then fill in the value\n",
    "            power_on = df_sla.loc[df_sla.timestamp == current_ts, 'poweron'].count()\n",
    "            power_off = df_sla.loc[df_sla.timestamp == current_ts, 'poweroff'].count()\n",
    "            if (power_on > 0) and (power_off > 0):\n",
    "                df_sla.loc[df_sla.timestamp == current_ts, 'powercycle'] =  df_sla.loc[df_sla.timestamp == current_ts, 'poweroff'] - df_sla.loc[df_sla.timestamp == current_ts, 'poweron']\n",
    "            df_sla['powercycle'] = pd.to_timedelta(df_sla['powercycle'],'h')\n",
    "            df_sla['ops time'] = df_sla.poweron - df_sla.poweroff\n",
    "            #df_sla['powercycle'] = df_sla['powercycle']/np.timedelta64(1,'h')\n",
    "\n",
    "        # remember, the index of series power_timestamps is the same as in df_work_events\n",
    "    print('Downtime for server ' + s + ': ' + str(server_downtime))\n",
    "    df_sla.set_index('timestamp', inplace=True)\n",
    "#    df_sla.to_markdown('sla_' + str(s) + '.md')\n",
    "\n",
    "    #\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1e39f5ddb06da47c792e6724e01a6f520ddcc160c593e0336e4f03a3f0c97e2f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('jupyterlab-debugger': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
